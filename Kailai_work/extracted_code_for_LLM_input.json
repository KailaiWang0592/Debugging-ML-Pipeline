{
    "simple-house-prices-prediction-with-explanation.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__aryangupta30__simple-house-prices-prediction-with-explanation/simple-house-prices-prediction-with-explanation.ipynb",
        "code": [
            "# This Python 3 environment comes with many helpful analytics libraries installed",
            "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
            "# For example, here's several helpful packages to load",
            "",
            "import numpy as np # linear algebra",
            "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
            "import matplotlib.pyplot as plt",
            "",
            "# Input data files are available in the read-only \"/share/dutta/eyao/dataset/kaggle/\" directory",
            "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
            "",
            "import os",
            "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):",
            "for filename in filenames:",
            "print(os.path.join(dirname, filename))",
            "",
            "# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using \"Save & Run All\"",
            "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv')",
            "test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv')",
            "train.head()",
            "train.info()",
            "train.shape",
            "train.isnull().sum()",
            "test.isnull().sum()",
            "def fill_missing_values(df):",
            "for column in df.columns:",
            "if df[column].dtype == np.float64:",
            "df[column].fillna(df[column].mean(), inplace=True)",
            "elif (df[column] == 'O').any():",
            "df[column].fillna(df[column].mode().iloc[0], inplace=True)",
            "fill_missing_values(train)",
            "train.isnull().sum()",
            "fill_missing_values(test)",
            "# columns_to_drop = ['Id']",
            "# train = train.drop(columns=columns_to_drop)",
            "from sklearn.preprocessing import LabelEncoder",
            "",
            "def encode_categorical_columns(df):",
            "label_encoder = LabelEncoder()",
            "",
            "for column in df.columns:",
            "if df[column].dtype == 'object':",
            "df[column] = label_encoder.fit_transform(df[column])",
            "elif df[column].dtype == 'float64':",
            "df[column] = df[column].fillna(df[column].mean())",
            "",
            "return df",
            "train = encode_categorical_columns(train)",
            "test = encode_categorical_columns(test)",
            "# display_column_data_types(train)",
            "X = train.drop(['SalePrice','Id'], axis=1)",
            "y = train['SalePrice']",
            "X_val = test.drop(['Id'],axis = 1)",
            "from sklearn.preprocessing import StandardScaler",
            "sc = StandardScaler()",
            "X = sc.fit_transform(X)",
            "X_val = sc.fit_transform(X_val)",
            "from sklearn.model_selection import train_test_split",
            "",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
            "# def encode_labels(y):",
            "#     label_encoder = LabelEncoder()",
            "#     y_encoded = label_encoder.fit_transform(y)",
            "#     return y_encoded",
            "# y_train_encoded = encode_labels(y_train)",
            "# y_test_encoded = encode_labels(y_test)",
            "from sklearn.metrics import mean_squared_error",
            "from sklearn.metrics import  accuracy_score",
            "# seed = np.random.seed(0)",
            "# from sklearn.ensemble import RandomForestRegressor",
            "# rfmodel = RandomForestRegressor(n_estimators = 1000,random_state = 0,criterion=\"absolute_error\")",
            "# rfmodel.fit(X_train,y_train)",
            "# print(\"MSE of Random Forest Regressor: \",mean_square_error(y_test,rfmodel.predict(X_test)))",
            "from xgboost import XGBRegressor",
            "",
            "xgb_regressor = XGBRegressor(random_state=0,learning_rate=0.5, max_depth=5, n_estimators=400)",
            "xgb_regressor.fit(X_train, y_train)",
            "y_test.head()",
            "print(f\"Mean Squared Error: {mean_squared_error(y_test, xgb_regressor.predict(X_test))}\")",
            "from lightgbm import LGBMRegressor",
            "from sklearn.metrics import mean_absolute_error",
            "lgbmodel = LGBMRegressor(random_state=0,max_depth= 8,n_estimators= 5000)",
            "lgbmodel.fit(X_train,y_train)",
            "print(\"MSE of Light Gradient Boosted Machine Regressor: \",mean_squared_error(y_test,lgbmodel.predict(X_test)))",
            "from catboost import CatBoostRegressor",
            "catmodel = CatBoostRegressor(random_state = 0,loss_function=\"MAE\",verbose=False)",
            "catmodel.fit(X_train,y_train)",
            "print(\"MSE of Category Boosting Regressor: \",mean_squared_error(y_test,catmodel.predict(X_test)))",
            "",
            "from sklearn.ensemble import RandomForestRegressor",
            "from sklearn.model_selection import RandomizedSearchCV",
            "from scipy.stats import randint",
            "random_forest = RandomForestRegressor(random_state=0)",
            "distributions = {",
            "'n_estimators': randint(low=10, high=200),",
            "'max_depth': randint(low=1, high=20),",
            "'min_samples_split': randint(low=2, high=20),",
            "'min_samples_leaf': randint(low=1, high=20),",
            "}",
            "rf_search = RandomizedSearchCV(random_forest, distributions, random_state=0)",
            "rf_search.fit(X_train, y_train)",
            "rf_search.best_params_",
            "best_params = rf_search.best_params_",
            "best_rf_model = RandomForestRegressor(random_state=0, **best_params)",
            "best_rf_model.fit(X_train, y_train)",
            "y_pred = best_rf_model.predict(X_test)",
            "mse = mean_squared_error(y_test, y_pred)",
            "print(f\"Mean Squared Error: {mse}\")",
            "submission = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/sample_submission.csv')",
            "submission.head()",
            "# test_without_id = test.drop('Id', axis=1)",
            "predictions = xgb_regressor.predict(X_val)",
            "# submission['SalePrice'] = predictions",
            "",
            "submission = pd.DataFrame({'Id': test.Id ,'SalePrice': predictions})",
            "",
            "submission.to_csv('submission.csv', index=False)",
            "",
            "print(submission.head())"
        ]
    },
    "reduce-complexity-for-house-prices-predictions.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__ctrnngtrung__reduce-complexity-for-house-prices-predictions/reduce-complexity-for-house-prices-predictions.ipynb",
        "code": [
            "import warnings",
            "warnings.filterwarnings(\"ignore\")",
            "",
            "import tensorflow as tf",
            "import pandas as pd",
            "import numpy as np",
            "import copy",
            "import seaborn as sns",
            "import matplotlib.pyplot as plt",
            "",
            "",
            "from xgboost import XGBRegressor",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score",
            "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score",
            "from sklearn.model_selection import GridSearchCV",
            "from sklearn.metrics import mean_squared_error",
            "from sklearn.decomposition import PCA",
            "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor",
            "from sklearn.neighbors import KNeighborsRegressor",
            "from sklearn.tree import DecisionTreeRegressor",
            "%matplotlib inline",
            "data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv')",
            "print(\"Table Shape: {}\".format(data.shape))",
            "data.head(7) # See 7 information row at the top of dataset",
            "null_class = data.isnull().sum()",
            "print(null_class[null_class != 0])",
            "data.describe()",
            "def preprocess(df, train_data=True):",
            "drop_rows_list = [\"MasVnrType\", \"MasVnrArea\", \"Electrical\"]",
            "if train_data:",
            "# Drop NaN rows",
            "for sample_row in drop_rows_list:",
            "df.drop(df[df[sample_row].isnull()].index, inplace=True)",
            "",
            "# Replace NaN with the most common value",
            "most_common_list = [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\",",
            "\"GarageType\", \"GarageYrBlt\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]",
            "for sample_row in most_common_list:",
            "df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)",
            "",
            "# Replace NaN with Mean",
            "df.LotFrontage.fillna(df.LotFrontage.mean(), inplace=True)",
            "",
            "# Remove some unecessary columns",
            "removed_features_list = [\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\", \"Id\"]",
            "for feature in removed_features_list:",
            "del df[feature]",
            "",
            "if not train_data:",
            "# Replace NaN with the  most common value for test set",
            "for sample_row in test_set.isnull().columns:",
            "df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)",
            "",
            "return df",
            "",
            "def preprocess_no_obj_feature(df, train_data=True):",
            "drop_rows_list = [\"MasVnrType\", \"MasVnrArea\", \"Electrical\"]",
            "if train_data:",
            "# Drop NaN rows",
            "for sample_row in drop_rows_list:",
            "df.drop(df[df[sample_row].isnull()].index, inplace=True)",
            "",
            "# Replace NaN with the most common value",
            "most_common_list = [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\",",
            "\"GarageType\", \"GarageYrBlt\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]",
            "for sample_row in  data.select_dtypes(include=['object']).columns:",
            "df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)",
            "",
            "# Replace NaN with Mean",
            "df.LotFrontage.fillna(df.LotFrontage.mean(), inplace=True)",
            "",
            "# Remove some unecessary columns",
            "removed_features_list = [\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\", \"Id\"]",
            "for feature in removed_features_list:",
            "del df[feature]",
            "",
            "# Delete all remaining object features",
            "for feature in df.select_dtypes(include=['object']).columns:",
            "del df[feature]",
            "",
            "if not train_data:",
            "# Replace NaN with the  most common value for test set",
            "for sample_row in test_set.isnull().columns:",
            "df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)",
            "",
            "return df",
            "",
            "# We can choose 1 of 2 preprocess methods.",
            "# data = preprocess(data, train_data=True) # This preprocess for using object features",
            "data = preprocess_no_obj_feature(data, train_data=True) # Do not use object features",
            "data.head(3)",
            "correlation_matrix = pd.DataFrame.corr(data)",
            "correlation_matrix",
            "upper_bound_threshold, lower_bound_threshold = 0.8, -0.3",
            "# Find features with high correlation scores",
            "high_corr_features = np.where(correlation_matrix > upper_bound_threshold)",
            "neg_corr_features = np.where(correlation_matrix < lower_bound_threshold)",
            "high_corr_features = [(correlation_matrix.columns[i], correlation_matrix.columns[j])",
            "for i, j in zip(*high_corr_features) if i != j]",
            "neg_corr_features = [(correlation_matrix.columns[i], correlation_matrix.columns[j])",
            "for i, j in zip(*neg_corr_features) if i != j]",
            "# Convert to a set of unique features",
            "high_corr_features = set(feature for pair in high_corr_features for feature in pair)",
            "neg_corr_features = set(feature for pair in neg_corr_features for feature in pair)",
            "# Remove high correlated features from the dataset",
            "data_without_high_corr = data.drop(columns=high_corr_features)",
            "# data_without_high_corr = data_without_high_corr.drop(columns=neg_corr_features.difference(high_corr_features))",
            "# # SKIP this cell if do not use object features. Uncomment if use object features",
            "# # Visualize the boxplot to find the outlier for all object features",
            "# feats_for_find_outlier = (data_without_high_corr.dtypes[data_without_high_corr.dtypes == object]).keys().values.reshape(-1, 2)",
            "# num_row, num_col = feats_for_find_outlier.shape[0], feats_for_find_outlier.shape[1]",
            "# fig, ax = plt.subplots(num_row, num_col, figsize=(12, 60))",
            "# for row in range(num_row):",
            "#     for col in range(num_col):",
            "#         sns.boxplot(data=data_without_high_corr, x=feats_for_find_outlier[row,col], y='SalePrice', ax = ax[row,col], dodge=False)",
            "# plt.tight_layout()",
            "# plt.show()",
            "# # SKIP this cell if do not use object features. Uncomment if use object features",
            "# col_feats = [\"RoofStyle\", \"BsmtCond\", \"SaleCondition\"]",
            "# categorical_of_col = [\"Gable\",\"TA\",\"Abnorml\"]",
            "",
            "# def find_combination(categorical_of_col, num_items_to_select):",
            "#     combinations_of_cat = list(combinations(categorical_of_col, num_items_to_select))",
            "#     return combinations_of_cat",
            "",
            "# def find_outlier_threshold(df_in, target_col, in_col, in_category):",
            "#     price_by_cat = df_in[target_col][df_in[in_col]==in_category]",
            "#     q1 = price_by_cat.quantile(0.25)",
            "#     q3 = price_by_cat.quantile(0.75)",
            "#     iqr = q3-q1",
            "#     f_low  = q1 - 1.5 * iqr",
            "#     f_high = q3 + 1.5 * iqr",
            "#     return f_low, f_high",
            "",
            "# def remove_outlier(df_in, target_col, col_feats, categorical_of_col, fence_low, fence_high):",
            "#     remove_ind = []",
            "#     for ind in df_in.index:",
            "#         if df_in[col_feats][ind] == categorical_of_col:",
            "#             if (df_in[target_col][ind] > fence_low) and (df_in[target_col][ind] < fence_high):",
            "#                 remove_ind.append(ind)",
            "#     for ind in remove_ind:",
            "#         df_in = df_in.drop(ind)",
            "#     return df_in",
            "",
            "# # Find threshold for each feature",
            "# fence_low, fence_high = [0]*len(categorical_of_col), [0]*len(categorical_of_col)",
            "# for i in range(len(categorical_of_col)):",
            "#     fence_low[i], fence_high[i] = find_outlier_threshold(data_without_high_corr, \"SalePrice\", col_feats[i], categorical_of_col[i])",
            "# for i in range(len(fence_low)):",
            "#     removed_outlier_data = remove_outlier(data_without_high_corr, \"SalePrice\", col_feats[i], categorical_of_col[i], fence_low[i], fence_high[i])",
            "# # SKIP this cell if do not use object features.",
            "# # For sure test and train have the same dummies",
            "# test_set = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv')",
            "# ids = test_set.Id",
            "# test_set = preprocess(test_set, train_data=False)",
            "# test_set = test_set.drop(columns=high_corr_features)",
            "# test_set = test_set.drop(columns=neg_corr_features.difference(high_corr_features))",
            "# target = removed_outlier_data.SalePrice",
            "# removed_outlier_data = removed_outlier_data.drop(columns=[\"SalePrice\"])",
            "",
            "# # Create dummies and remove object features",
            "# data_type = removed_outlier_data.dtypes",
            "# object_features = data_type[data_type==object]",
            "# non_object_features = data_type[data_type!=object]",
            "# object_data = removed_outlier_data[object_features.keys()]",
            "# # Categorical data",
            "# len_train = len(removed_outlier_data)",
            "# dataset = pd.concat(objs=[removed_outlier_data, test_set], axis=0)",
            "# dataset = pd.get_dummies(dataset)",
            "# test_set = copy.copy(dataset[len_train:])",
            "# source = copy.copy(dataset[:len_train])",
            "# Function to remove outlier without object features",
            "def remove_outlier(df_in, col_name):",
            "q1 = df_in[col_name].quantile(0.25) # Q1",
            "q3 = df_in[col_name].quantile(0.75) # Q3",
            "iqr = q3-q1 # Interquartile range",
            "fence_low  = q1-1.5*iqr",
            "fence_high = q3+1.5*iqr",
            "df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]",
            "return df_out",
            "",
            "removed_outlier_data = remove_outlier(data_without_high_corr, 'SalePrice')",
            "target = removed_outlier_data.SalePrice",
            "test_set = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv')",
            "ids = test_set.Id",
            "test_set = preprocess_no_obj_feature(test_set, train_data=False)",
            "test_set = test_set.drop(columns=high_corr_features)",
            "# test_set = test_set.drop(columns=neg_corr_features.difference(high_corr_features))",
            "removed_outlier_data = removed_outlier_data.drop(columns=[\"SalePrice\"])",
            "source = removed_outlier_data",
            "X, y = source, target",
            "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)",
            "# Initialization",
            "LR = LinearRegression()",
            "LR.fit(X_train, y_train)",
            "y_pred = LR.predict(X_val)",
            "plt.scatter(y_val,y_pred)",
            "plt.plot([y_val.min(), y_val.max()], [y_pred.min(), y_pred.max()], 'k--', lw=3)",
            "plt.xlabel('y_predicted')",
            "plt.ylabel('y_val')",
            "plt.title('Linear Regression')",
            "plt.show()",
            "print(\"Train R2 Score: {}\".format(LR.score(X_train,y_train)))",
            "print(\"Test R2 Score: {}\".format(LR.score(X_val,y_val)))",
            "y_pred_kFolds = cross_val_predict(LR, X.values, y.values, cv = 5)",
            "plt.scatter(y, y_pred_kFolds)",
            "plt.plot([y_val.min(), y_val.max()], [y_pred_kFolds.min(), y_pred_kFolds.max()], 'k--', lw=3)",
            "plt.xlabel('y_Predicted')",
            "plt.ylabel('y_Test')",
            "plt.title('Linear Regression with K-Folds')",
            "plt.show()",
            "cv_r2_scores = cross_val_score(LR, source, target, scoring='r2')",
            "print(\"Mean 5-Folds R Squared: {}\".format(np.mean(cv_r2_scores)))",
            "pca = PCA(n_components=15)",
            "pca_fit = pca.fit_transform(source)",
            "pca_df = pd.DataFrame(data = pca_fit, columns = ['pca1','pca2','pca3','pca4','pca5',",
            "'pca6','pca7','pca8','pca9','pca10','pca11',",
            "'pca12','pca13','pca14','pca15'])",
            "X_pca_train, X_pca_val, y_train_pca, y_val_pca = train_test_split(pca_df, y, test_size=0.2)",
            "LR_pca = LinearRegression()",
            "LR_pca.fit(X_pca_train, y_train_pca)",
            "y_pred_pca = LR_pca.predict(X_pca_val)",
            "plt.scatter(y_val_pca,y_pred_pca)",
            "plt.plot([y_val_pca.min(), y_val_pca.max()], [y_pred_pca.min(), y_pred_pca.max()], 'k--', lw=3)",
            "plt.xlabel('y_predicted')",
            "plt.ylabel('y_val')",
            "plt.title('Linear Regression with PCA')",
            "plt.show()",
            "print(\"Train R2 Score: {}\".format(LR_pca.score(X_pca_train,y_train_pca)))",
            "print(\"Test R2 Score: {}\".format(LR_pca.score(X_pca_val,y_val_pca)))",
            "# Calculate residuals",
            "residuals = y_val - y_pred",
            "# Create a residual plot",
            "plt.figure(figsize=(8, 6))",
            "plt.scatter(y_pred, residuals, color='blue')",
            "plt.axhline(y=0, color='red', linestyle='--')",
            "plt.title('Residual Plot')",
            "plt.xlabel('Predicted Values')",
            "plt.ylabel('Residuals')",
            "plt.grid(True)",
            "plt.show()",
            "plt.hist(residuals, bins=200)",
            "plt.title('Distribution of Residuals')",
            "plt.ylabel('Residuals')",
            "plt.show()",
            "X, y = source, target",
            "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)",
            "learning_rate = [0.01, 0.02, 0.05, 0.1]",
            "subsample = [0.5, 0.2, 0.1]",
            "n_estimators = [100, 500, 1000, 1500]",
            "max_depth = [None, 3, 6, 9, 12]",
            "",
            "param_grid = {'learning_rate':learning_rate,",
            "'subsample':subsample,",
            "'n_estimators':n_estimators,",
            "'max_depth':max_depth}",
            "",
            "GBR = GradientBoostingRegressor()",
            "GBR = GridSearchCV(estimator=GBR, param_grid=param_grid, cv=2,n_jobs=-1)",
            "GBR.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", GBR.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", GBR.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", GBR.best_params_)",
            "GBR_best = GradientBoostingRegressor(**GBR.best_params_) # train with best parameter",
            "GBR_best.fit(X_train, y_train)",
            "y_pred = GBR_best.predict(X_val)",
            "print('\\n\\nR-squared val set: ')",
            "print(GBR_best.score(X_val, y_val))",
            "print('\\nMAE val set: ')",
            "print(mean_absolute_error(y_val, y_pred))",
            "print('\\nMSE val set: ')",
            "print(mean_squared_error(y_val, y_pred))",
            "learning_rate = [0.01, 0.02, 0.05, 0.1]",
            "loss = ['linear', 'square', 'exponential']",
            "n_estimators = [25, 50, 100, 120]",
            "",
            "param_grid = {'learning_rate':learning_rate,",
            "'loss': loss,",
            "'n_estimators': n_estimators}",
            "",
            "ABR = AdaBoostRegressor()",
            "ABR = GridSearchCV(estimator=ABR, param_grid=param_grid, cv=2,n_jobs=-1)",
            "ABR.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", ABR.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", ABR.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", ABR.best_params_)",
            "ABR_best = AdaBoostRegressor(**ABR.best_params_) # train with best parameter",
            "ABR_best.fit(X_train, y_train)",
            "y_pred = ABR_best.predict(X_val)",
            "print('\\n\\nR-squared val set: ')",
            "print(ABR_best.score(X_val, y_val))",
            "print('\\nMAE val set: ')",
            "print(mean_absolute_error(y_val, y_pred))",
            "print('\\nMSE val set: ')",
            "print(mean_squared_error(y_val, y_pred))",
            "n_estimators = [50, 100, 150, 300]",
            "max_depth = [None, 4, 6, 8, 10]",
            "max_features = ['sqrt', 'log2', None, int, float]",
            "param_grid = {'n_estimators': n_estimators,",
            "'max_depth': max_depth,",
            "'max_features': max_features}",
            "",
            "RFR = RandomForestRegressor()",
            "RFR = GridSearchCV(estimator=RFR, param_grid=param_grid, cv=2,n_jobs=-1)",
            "RFR.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", RFR.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", RFR.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", RFR.best_params_)",
            "RFR_best = RandomForestRegressor(**RFR.best_params_) # train with best parameter",
            "RFR_best.fit(X_train, y_train)",
            "y_pred = RFR_best.predict(X_val)",
            "print('\\n\\nR-squared val set: ')",
            "print(RFR_best.score(X_val, y_val))",
            "print('\\nMAE val set: ')",
            "print(mean_absolute_error(y_val, y_pred))",
            "print('\\nMSE val set: ')",
            "print(mean_squared_error(y_val, y_pred))",
            "n_neighbors = [3, 5, 7, 10, 15]",
            "weights = ['uniform', 'distance']",
            "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']",
            "leaf_size = [20, 30, 50, 70]",
            "p = [1, 2, 3]",
            "",
            "param_grid = {'n_neighbors':n_neighbors,",
            "'weights':weights,",
            "'algorithm':algorithm,",
            "'leaf_size':leaf_size,",
            "'p': p}",
            "",
            "KNR = KNeighborsRegressor()",
            "KNR = GridSearchCV(estimator=KNR, param_grid=param_grid, cv=2,n_jobs=-1)",
            "KNR.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", KNR.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", KNR.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", KNR.best_params_)",
            "KNR_best = KNeighborsRegressor(**KNR.best_params_) # train with best parameter",
            "KNR_best.fit(X_train, y_train)",
            "y_pred = KNR_best.predict(X_val)",
            "print('\\n\\nR-squared val set: ')",
            "print(KNR_best.score(X_val, y_val))",
            "print('\\nMAE val set: ')",
            "print(mean_absolute_error(y_val, y_pred))",
            "print('\\nMSE val set: ')",
            "print(mean_squared_error(y_val, y_pred))",
            "n_estimators = [5, 10, 20, 25]",
            "max_features  = [0.1, 0.2, 0.3, 0.5, 1.0]",
            "max_samples = [0.1, 0.2, 0.3, 0.5, 1.0]",
            "",
            "param_grid = {'n_estimators': n_estimators,",
            "'max_features': max_features,",
            "'max_samples': max_samples}",
            "",
            "BR = BaggingRegressor()",
            "BR = GridSearchCV(estimator=BR, param_grid=param_grid, cv=2,n_jobs=-1)",
            "BR.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", BR.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", BR.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", BR.best_params_)",
            "BR_best = BaggingRegressor(**BR.best_params_) # train with best parameter",
            "BR_best.fit(X_train, y_train)",
            "y_pred = BR_best.predict(X_val)",
            "print('\\n\\nR-squared val set: ')",
            "print(BR_best.score(X_val, y_val))",
            "print('\\nMAE val set: ')",
            "print(mean_absolute_error(y_val, y_pred))",
            "print('\\nMSE val set: ')",
            "print(mean_squared_error(y_val, y_pred))",
            "criterion = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']",
            "splitter  = ['best', 'random']",
            "max_depth = [None, 4, 6, 8, 10]",
            "",
            "param_grid = {'criterion': criterion,",
            "'splitter': splitter,",
            "'max_depth': max_depth}",
            "",
            "DTR = DecisionTreeRegressor()",
            "DTR = GridSearchCV(estimator=DTR, param_grid=param_grid, cv=2,n_jobs=-1)",
            "DTR.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", DTR.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", DTR.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", DTR.best_params_)",
            "DTR_best = DecisionTreeRegressor(**DTR.best_params_) # train with best parameter",
            "DTR_best.fit(X_train, y_train)",
            "y_pred = DTR_best.predict(X_val)",
            "print('\\n\\nR-squared val set: ')",
            "print(DTR_best.score(X_val, y_val))",
            "print('\\nMAE val set: ')",
            "print(mean_absolute_error(y_val, y_pred))",
            "print('\\nMSE val set: ')",
            "print(mean_squared_error(y_val, y_pred))",
            "learning_rate = [0.01, 0.03, 0.05, 0.07]",
            "max_depth = [5, 6, 7]",
            "subsample = [0.1, 0.2, 0.5, 0.7]",
            "",
            "param_grid = {'learning_rate': learning_rate,",
            "'max_depth': max_depth,",
            "'subsample':subsample}",
            "",
            "XGB = XGBRegressor()",
            "XGB = GridSearchCV(estimator=XGB, param_grid=param_grid, cv=2,n_jobs=-1)",
            "XGB.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", XGB.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", XGB.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", XGB.best_params_)",
            "XGB_best = XGBRegressor(**XGB.best_params_) # train with best parameter",
            "XGB_best.fit(X_train, y_train)",
            "y_pred = XGB_best.predict(X_val)",
            "print('\\n\\nR-squared val set: ')",
            "print(DTR_best.score(X_val, y_val))",
            "print('\\nMAE val set: ')",
            "print(mean_absolute_error(y_val, y_pred))",
            "print('\\nMSE val set: ')",
            "print(mean_squared_error(y_val, y_pred))",
            "X, y = source, target",
            "X_train, y_train = X, y",
            "X_train.shape",
            "learning_rate = [0.01, 0.02, 0.05, 0.1]",
            "subsample = [0.5, 0.2, 0.1]",
            "n_estimators = [100, 500, 1000, 1500]",
            "max_depth = [None, 3, 6, 9, 12]",
            "",
            "param_grid = {'learning_rate':learning_rate,",
            "'subsample':subsample,",
            "'n_estimators':n_estimators,",
            "'max_depth':max_depth}",
            "",
            "GBR = GradientBoostingRegressor()",
            "GBR = GridSearchCV(estimator=GBR, param_grid=param_grid, cv=2,n_jobs=-1)",
            "GBR.fit(X_train, y_train)",
            "print(\"Results from Grid Search\")",
            "print(\"\\n The best estimator across ALL searched params:\\n\", GBR.best_estimator_)",
            "print(\"\\n The best score across ALL searched params:\\n\", GBR.best_score_)",
            "print(\"\\n The best parameters across ALL searched params:\\n\", GBR.best_params_)",
            "GBR_best = GradientBoostingRegressor(**GBR.best_params_) # train with best parameter",
            "GBR_best.fit(X_train, y_train)",
            "y_pred = GBR_best.predict(test_set)",
            "output = pd.DataFrame({'Id': ids, 'SalePrice': y_pred.squeeze()})",
            "output.head()",
            "output.to_csv('submission_GBR.csv', index=False)",
            "y_pred_LR = LR.predict(test_set)",
            "output_LR = pd.DataFrame({'Id': ids, 'SalePrice': y_pred.squeeze()})",
            "output_LR.head()",
            "output_LR.to_csv('submission_LR.csv', index=False)"
        ]
    },
    "spaceship-titanic-eda-predictions.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__4_abraamsaid__spaceship-titanic-eda-predictions/spaceship-titanic-eda-predictions.ipynb",
        "code": [
            "pip install datasist",
            "# EDA Libraries",
            "import pandas as pd",
            "import numpy as np",
            "import plotly.express as px",
            "import plotly.figure_factory as ff",
            "from plotly.subplots import make_subplots",
            "import plotly.subplots as sp",
            "import plotly.graph_objects as go",
            "import seaborn as sns",
            "import matplotlib.pyplot as plt",
            "sns.set_style(\"whitegrid\")",
            "",
            "# Data Preprocessing Libraries",
            "from datasist.structdata import detect_outliers",
            "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, LabelEncoder",
            "from category_encoders import BinaryEncoder",
            "from sklearn.impute import SimpleImputer, KNNImputer",
            "from imblearn.under_sampling import RandomUnderSampler",
            "from imblearn.over_sampling import SMOTE",
            "",
            "# Machine Learing (classification models) Libraries",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.neighbors import KNeighborsClassifier",
            "from sklearn.svm import SVC",
            "from sklearn.naive_bayes import GaussianNB",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier",
            "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_regression, RFE, SelectFromModel",
            "from imblearn.pipeline import Pipeline",
            "from sklearn.compose import ColumnTransformer",
            "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_curve, roc_auc_score",
            "import lightgbm as lgb",
            "import xgboost as xgb",
            "from catboost import CatBoostClassifier",
            "from sklearn.model_selection import GridSearchCV",
            "# Reading Train csv file",
            "df_train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')",
            "df_train.sample(10)",
            "# check the train dataset shape",
            "print(\"Number of Columns in Train data\",df_train.shape[1])",
            "print(\"---------------------------------------\")",
            "print(\"Number of Rows in Train data\",df_train.shape[0])",
            "# Reading Test csv file",
            "df_test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')",
            "df_test.sample(10)",
            "# check the test dataset shape",
            "print(\"Number of Columns in Test data\",df_test.shape[1])",
            "print(\"---------------------------------------\")",
            "print(\"Number of Rows in Test data\",df_test.shape[0])",
            "df_train.info()",
            "# Dropping Name columns in the train and test data as they are unique identifier and not useful for predictions.",
            "df_train = df_train.drop('Name', axis=1)",
            "df_test = df_test.drop('Name', axis=1)",
            "# Descriptive analysis for categorical data",
            "df_train.describe(include='O')",
            "# Descriptive analysis for numerical data",
            "df_train.describe().style.background_gradient()",
            "fig = px.pie(df_train, names='Transported',",
            "title='Transported Distribution',",
            "color_discrete_sequence=px.colors.sequential.Mint_r,",
            "template='plotly_dark'",
            ")",
            "",
            "fig.update_traces(textposition='inside',textinfo='percent+label')",
            "",
            "fig.show()",
            "fig = px.pie(df_train, names='HomePlanet',",
            "title='HomePlanet Distribution',",
            "color_discrete_sequence=px.colors.sequential.RdBu,",
            "template='plotly_dark'",
            ")",
            "",
            "fig.update_traces(textposition='inside',textinfo='percent+label')",
            "",
            "fig.show()",
            "fig = px.pie(df_train, names='Destination',",
            "title='Destination Distribution',",
            "color_discrete_sequence=px.colors.sequential.Blues_r,",
            "template='plotly_dark'",
            ")",
            "fig.update_traces(textposition='inside',textinfo='percent+label')",
            "",
            "fig.show()",
            "fig = px.pie(df_train, names='CryoSleep',",
            "title='CryoSleep Distribution',",
            "color_discrete_sequence=px.colors.sequential.BuGn_r,",
            "template='plotly_dark'",
            ")",
            "",
            "fig.update_traces(textposition='inside',textinfo='percent+label')",
            "",
            "fig.show()",
            "fig = px.pie(df_train, names='VIP',",
            "title='VIP Distribution',",
            "color_discrete_sequence=px.colors.sequential.Burg_r,",
            "template='plotly_dark'",
            ")",
            "",
            "fig.update_traces(textposition='inside',textinfo='percent+label')",
            "",
            "fig.show()",
            "fig = px.histogram(df_train, x='Age',  title='Age Distribution',",
            "marginal='box', color_discrete_sequence=['#429ea8'],",
            "template='plotly_dark'",
            ")",
            "",
            "# Customizing the layout of the histogram",
            "fig.update_layout(",
            "xaxis=dict(tickmode='linear', dtick=5),  # Adjusting x-axis tick settings",
            "bargap=0.1  # Setting the gap between bars",
            ")",
            "",
            "fig.show()",
            "# Create subplot grid with larger size",
            "fig = make_subplots(rows=2, cols=3, subplot_titles=['RoomService Boxplot', 'FoodCourt Boxplot',",
            "'ShoppingMall Boxplot' , 'Spa Boxplot' , 'VRDeck Boxplot'],",
            "vertical_spacing=0.15, horizontal_spacing=0.15)",
            "",
            "# Iterate over each feature and add a box plot",
            "features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']",
            "",
            "for i, feature in enumerate(features, start=1):",
            "box_plot = px.box(df_train, x=feature,",
            "color_discrete_sequence=['#40db95'], template='plotly_dark')",
            "",
            "row_num = (i - 1) // 3 + 1",
            "col_num = (i - 1) % 3 + 1",
            "",
            "fig.add_trace(box_plot['data'][0], row=row_num, col=col_num)",
            "",
            "# Update layout for larger size and separation",
            "fig.update_layout(showlegend=False, height=600, template='plotly_dark')",
            "",
            "# Show the figure",
            "fig.show()",
            "fig = px.histogram(df_train, x='HomePlanet', color='Transported',",
            "title='Distribution of Transported Passengers by Home Planet',",
            "color_discrete_map={0: '#eb3134', 1: '#10c2de'},",
            "barmode='group', template='plotly_dark', text_auto=True",
            ")",
            "",
            "fig.show()",
            "fig = px.histogram(df_train, x='Destination', color='Transported',",
            "title='Distribution of Transported Passengers by Destination',",
            "color_discrete_map={0: '#eb3134', 1: '#10c2de'},",
            "barmode='group', template='plotly_dark', text_auto=True",
            ")",
            "",
            "fig.show()",
            "fig = px.histogram(df_train, x='Age', color='Transported',  title='Distribution of Transported Passengers by Age',",
            "marginal='box', color_discrete_sequence=['#eb3134', '#10c2de'],",
            "template='plotly_dark'",
            ")",
            "",
            "fig.show()",
            "fig = px.violin(df_train, x='CryoSleep', y='Age', color='CryoSleep', box=True, title='CryoSleep vs Age',",
            "color_discrete_sequence=['#eb3134', '#10c2de'], template='plotly_dark')",
            "",
            "fig.show()",
            "# Calculate missing values for both train and test data",
            "missing_values_train = df_train.isna().sum()",
            "missing_values_test = df_test.isna().sum()",
            "",
            "# Create subplot grid with 1 row and 2 columns",
            "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=['Train Data', 'Test Data'],",
            "vertical_spacing=0.1, horizontal_spacing=0.15)",
            "",
            "# Add bar chart for missing values in train data",
            "fig.add_trace(go.Bar(x=missing_values_train.index, y=missing_values_train.values, marker_color='#10c2de', showlegend=False),",
            "row=1, col=1)",
            "",
            "# Add bar chart for missing values in test data",
            "fig.add_trace(go.Bar(x=missing_values_test.index, y=missing_values_test.values, marker_color='#10c2de', showlegend=False),",
            "row=1, col=2)",
            "",
            "# Update layout",
            "fig.update_layout(title='Missing Values in Train and Test Data', template='plotly_dark', height= 400)",
            "",
            "fig.show()",
            "# Numerical Columns",
            "numerical_columns = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']",
            "",
            "# Fill missing values in numerical columns with the median",
            "df_train[numerical_columns] = df_train[numerical_columns].fillna(df_train[numerical_columns].median())",
            "df_test[numerical_columns] = df_test[numerical_columns].fillna(df_test[numerical_columns].median())",
            "",
            "# Categorical Columns",
            "categorical_columns = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']",
            "",
            "# Fill missing values in categorical columns with the mode",
            "df_train[categorical_columns] = df_train[categorical_columns].fillna(df_train[categorical_columns].mode().iloc[0])",
            "df_test[categorical_columns] = df_test[categorical_columns].fillna(df_test[categorical_columns].mode().iloc[0])",
            "total_missing_train = df_train.isna().sum().sum()",
            "total_missing_test = df_test.isna().sum().sum()",
            "",
            "print(f'Total missing values in df_train: {total_missing_train}')",
            "print(f'Total missing values in df_test: {total_missing_test}')",
            "# Function to create Age groups based on specified bins and labels",
            "def create_age_groups(age):",
            "# Define bins and corresponding labels for Age groups",
            "age_bins = [0, 12, 18, 25, 30, 50, 80]",
            "age_labels = ['0-12', '13-17', '18-25', '26-30', '31-50', '51+']",
            "",
            "# Use pd.cut to categorize Age into groups",
            "age_group = pd.cut(age, bins=age_bins, labels=age_labels, right=False)",
            "",
            "# Return the resulting Age groups",
            "return age_group",
            "# Creating a new column 'Passenger_Groups' by extracting the second part of 'PassengerId' (after the underscore) and converting it to integer.",
            "df_train['Passenger_Groups'] = df_train['PassengerId'].apply(lambda x: x.split('_')[1]).astype(int)",
            "df_test['Passenger_Groups'] = df_test['PassengerId'].apply(lambda x: x.split('_')[1]).astype(int)",
            "",
            "# Creating three new columns 'Cabin Deck', 'Cabin Num', and 'Cabin Side' by splitting the 'Cabin' column based on '/'.",
            "df_train[['Cabin_Deck', 'Cabin_Num', 'Cabin_Side']] = df_train['Cabin'].str.split('/', expand=True)",
            "df_test[['Cabin_Deck', 'Cabin_Num', 'Cabin_Side']] = df_test['Cabin'].str.split('/', expand=True)",
            "",
            "# Converting the 'Cabin Num' column to dtype integer for numerical analysis.",
            "df_train['Cabin_Num'] = df_train['Cabin_Num'].astype('int')",
            "df_test['Cabin_Num'] = df_test['Cabin_Num'].astype('int')",
            "",
            "# Creating a new column 'AmenitiesTotal' by summing the values in 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck' columns.",
            "df_train['AmenitiesTotal'] = df_train[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)",
            "df_test['AmenitiesTotal'] = df_test[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)",
            "",
            "# Create a new 'Age_Group' column in the train set using the 'create_age_groups' function",
            "df_train['Age_Group'] = create_age_groups(df_train['Age'])",
            "df_test['Age_Group'] = create_age_groups(df_test['Age'])",
            "# Dropping 'PassengerId' and 'Cabin' columns as we have extracted important features from them, and the original columns are no longer needed for analysis or modeling.",
            "df_train = df_train.drop(['PassengerId', 'Cabin'], axis=1)",
            "df_test = df_test.drop('Cabin', axis=1)",
            "df_train.head()",
            "df_test.head()",
            "fig = px.histogram(df_train, x='Passenger_Groups',  title='Passenger Groups Distribution',",
            "marginal='box', color_discrete_sequence=['#1f87d1'],",
            "template='plotly_dark', text_auto=True",
            ")",
            "",
            "# Customizing the layout of the histogram",
            "fig.update_layout(",
            "bargap=0.1  # Setting the gap between bars",
            ")",
            "",
            "fig.show()",
            "fig = px.box(df_train, x='AmenitiesTotal',  title='AmenitiesTotal Distribution',",
            "color_discrete_sequence=['#40db95'],",
            "template='plotly_dark'",
            ")",
            "",
            "fig.show()",
            "fig = px.box(df_train,y='Transported', x='AmenitiesTotal', color='Transported',  title='AmenitiesTotal vs Transported',",
            "color_discrete_sequence=['#eb3134', '#10c2de'],",
            "template='plotly_dark'",
            ")",
            "",
            "fig.show()",
            "fig = px.histogram(df_train, x='Age_Group', color='Transported',",
            "title='Transported vs Age Groups', barmode='group',",
            "color_discrete_sequence=['#eb3134', '#10c2de'],",
            "template='plotly_dark', text_auto=True, category_orders={'Age_Group': ['0-12', '13-17', '18-25', '26-30', '31-50', '51+']})",
            "",
            "fig.show()",
            "fig = px.histogram(df_train, x='Cabin_Deck',",
            "title='Cabin Deck Distribution',",
            "color_discrete_sequence=['#1a9799'],",
            "template='plotly_dark', text_auto=True",
            ")",
            "",
            "fig.show()",
            "fig = px.histogram(df_train, x='Cabin_Side',",
            "title='Cabin Side Distribution',",
            "color_discrete_sequence=['#b05325'],",
            "template='plotly_dark',text_auto=True",
            ")",
            "",
            "fig.show()",
            "fig = px.histogram(df_train, x='Cabin_Num', color='Transported',",
            "title='Distribution of Cabin_Num by Transported', nbins=20,",
            "color_discrete_sequence=['#eb3134', '#10c2de'], template='plotly_dark'",
            ")",
            "",
            "fig.show()",
            "# Function to create Cabin_Num groups based on specified bins and labels",
            "def create_cabin_groups(cabin):",
            "# Define bins and corresponding labels for Cabin_Num groups",
            "cabin_bins = [0, 300, 600, 900, 1200, 1500, 1800, 1900]",
            "cabin_labels = ['0-300', '300-600', '600-900', '900-1200', '1200-1500', '1500-1800', '1800+']",
            "",
            "# Use pd.cut to categorize Cabin_Num into groups",
            "cabin_group = pd.cut(cabin, bins=cabin_bins, labels=cabin_labels, right=False)",
            "",
            "# Return the resulting Cabin_Num groups",
            "return cabin_group",
            "# Create groups for Cabin_Num_Group",
            "df_train['Cabin_Num_Group'] = create_cabin_groups(df_train['Cabin_Num'])",
            "df_test['Cabin_Num_Group'] = create_cabin_groups(df_test['Cabin_Num'])",
            "",
            "# Dropping the original 'Cabin_Num' feature as it's not used in our model, and we've created groups from it",
            "df_train = df_train.drop('Cabin_Num', axis=1)",
            "df_test = df_test.drop('Cabin_Num', axis=1)",
            "# Create a box plot",
            "fig = px.histogram(df_train, x='Cabin_Num_Group', color='Transported',",
            "title='Transported vs Cabin_Num Groups', barmode='group',",
            "color_discrete_sequence=['#eb3134', '#10c2de'], category_orders={'Cabin_Num_Group': ['0-300', '300-600', '600-900', '900-1200',",
            "'1200-1500', '1500-1800']},",
            "template='plotly_dark', text_auto=True)",
            "",
            "fig.show()",
            "# Applying Label Encoder to Work with Ordinal Features in Train and Test Datasets.",
            "",
            "# Initialize LabelEncoder",
            "label_encoder = LabelEncoder()",
            "",
            "# Define the mapping dictionaries",
            "Age_Group_dic = {",
            "'0-12': 0, '13-17': 1,",
            "'18-25': 2, '26-30': 3,",
            "'31-50': 4, '51+': 5",
            "}",
            "Cabin_Num_Group_dic = {",
            "'0-300': 0, '300-600': 1,",
            "'600-900': 2, '900-1200': 3,",
            "'1200-1500': 4, '1500-1800': 5,",
            "'1800+': 6",
            "}",
            "",
            "# Apply label encoding to Age_Group and Cabin_Num_Group",
            "df_train['Age_Group'] = label_encoder.fit_transform(df_train['Age_Group'].map(Age_Group_dic))",
            "df_test['Age_Group'] = label_encoder.transform(df_test['Age_Group'].map(Age_Group_dic))",
            "",
            "df_train['Cabin_Num_Group'] = label_encoder.fit_transform(df_train['Cabin_Num_Group'].map(Cabin_Num_Group_dic))",
            "df_test['Cabin_Num_Group'] = label_encoder.transform(df_test['Cabin_Num_Group'].map(Cabin_Num_Group_dic))",
            "# Convert 'Transported' column to numeric values (1 for True/transported, 0 for False/not transported)",
            "df_train['Transported'] = df_train['Transported'].replace({True: 1, False: 0})",
            "# Working with Nominal Features with pandas `get_dummies` function for train and test datasets.",
            "nominal_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Passenger_Groups', 'Cabin_Deck', 'Cabin_Side']",
            "",
            "# Encoding Train Data",
            "df_train = pd.get_dummies(df_train, columns=nominal_cols)",
            "train_encoded = list(df_train.columns)",
            "print(\"{} total features after one-hot encoding for train data.\".format(len(train_encoded)))",
            "",
            "# Encoding Test Data",
            "df_test = pd.get_dummies(df_test, columns=nominal_cols)",
            "test_encoded = list(df_test.columns)",
            "print(\"{} total features after one-hot encoding for test data.\".format(len(test_encoded)))",
            "df_train.head()",
            "df_test.head()",
            "numerical_features=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'AmenitiesTotal']",
            "",
            "# Detect outliers in numerical features",
            "train_outliers_indices = detect_outliers(df_train, features=numerical_features, n=0)",
            "number_of_outliers_train = len(train_outliers_indices)",
            "",
            "test_outliers_indices = detect_outliers(df_test, features=numerical_features, n=0)",
            "number_of_outliers_test = len(test_outliers_indices)",
            "",
            "# Print the number of outliers before handling for the Train and Test data.",
            "print(f'Number of outliers in the Train Data before handling: {number_of_outliers_train}')",
            "print(f'Number of outliers in the Test Data before handling: {number_of_outliers_test}')",
            "# Perform log transformation on the specified numerical columns",
            "df_train[numerical_features] = np.log1p(df_train[numerical_features])",
            "df_test[numerical_features] = np.log1p(df_test[numerical_features])",
            "train_outliers_indices = detect_outliers(df_train, features=numerical_features, n=0)",
            "number_of_outliers_train = len(train_outliers_indices)",
            "",
            "test_outliers_indices = detect_outliers(df_test, features=numerical_features, n=0)",
            "number_of_outliers_test = len(test_outliers_indices)",
            "",
            "# Print the number of outliers after handling for the Train and Test data.",
            "print(f'Number of outliers in the Train Data after handling: {number_of_outliers_train}')",
            "print(f'Number of outliers in the Test Data after handling: {number_of_outliers_test}')",
            "# First we extract the x Featues and y Label",
            "X = df_train.drop('Transported',axis=1)",
            "y = df_train['Transported']",
            "X.shape, y.shape",
            "# Then we Split the data into training and testing sets (80% training, 20% testing)",
            "X_train, X_test, y_train, y_test = train_test_split(X,",
            "y,",
            "test_size=0.20,",
            "random_state=42,",
            "stratify=y)",
            "",
            "# Show the results of the split",
            "print(\"Training set has {} samples.\".format(X_train.shape[0]))",
            "print(\"Testing set has {} samples.\".format(X_test.shape[0]))",
            "y_train.value_counts()",
            "sm = SMOTE(random_state=42)",
            "X_train, y_train = sm.fit_resample(X_train, y_train)",
            "y_train.value_counts()",
            "numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'AmenitiesTotal']",
            "",
            "# Creating a StandardScaler instance",
            "scaler = StandardScaler()",
            "",
            "# Fitting the StandardScaler on the training data",
            "scaler.fit(X_train[numerical_features])",
            "",
            "# Transforming (standardize) the continuous features in the training and testing data",
            "X_train_cont_scaled = scaler.transform(X_train[numerical_features])",
            "X_test_cont_scaled = scaler.transform(X_test[numerical_features])",
            "",
            "# Replacing the scaled continuous features in the original data",
            "X_train[numerical_features] = X_train_cont_scaled",
            "X_test[numerical_features] = X_test_cont_scaled",
            "",
            "X_train",
            "# Create a heatmap using Plotly",
            "heatmap_data = df_train.corr().values.tolist()",
            "",
            "fig = go.Figure(data=go.Heatmap(z=heatmap_data, x=df_train.columns, y=df_train.columns, colorscale='Viridis'))",
            "",
            "# Update layout",
            "fig.update_layout(title='Correlation Heatmap',",
            "xaxis_title='Features',",
            "yaxis_title='Features',",
            "template='plotly_dark')",
            "",
            "# Show the figure",
            "fig.show()",
            "# Sort the correlation values",
            "sorted_corr = df_train.corr()['Transported'].sort_values(ascending=False).drop(['Transported'])",
            "",
            "# Create a bar plot using Plotly",
            "fig = px.bar(x=sorted_corr.index, y=sorted_corr.values, color=sorted_corr.values,",
            "color_continuous_scale='Viridis', labels={'x': 'Feature', 'y': 'Target'},",
            "title='Correlation with Target (Transported)', template='plotly_dark')",
            "",
            "# Show the figure",
            "fig.show()",
            "# List of classifiers to evaluate",
            "classifiers = [",
            "(\"Logistic Regression\", LogisticRegression(random_state=42, max_iter=1500, n_jobs=-1)),",
            "(\"KNN\", KNeighborsClassifier(n_neighbors=5, n_jobs=-1)),",
            "(\"Gaussian Naive Bayes\", GaussianNB()),",
            "(\"Decision Tree\", DecisionTreeClassifier(random_state=42)),",
            "(\"Random Forest\", RandomForestClassifier(random_state=42, n_jobs=-1)),",
            "(\"AdaBoost\", AdaBoostClassifier(random_state=42)),",
            "(\"Gradient Boosting\", GradientBoostingClassifier(random_state=42)),",
            "(\"LightGBM\", lgb.LGBMClassifier(random_state=42, verbose=-1)),",
            "(\"XGBoost\", xgb.XGBClassifier(random_state=42, n_jobs=-1)),",
            "(\"CatBoost\", CatBoostClassifier(random_state=42, verbose=0))",
            "]",
            "# Creating lists for classifier names, mean_test_accuracy_scores, cross_val_errors, and results.",
            "results = []",
            "mean_test_accuracy_scores = []",
            "cross_val_errors = []",
            "classifier_names = []",
            "",
            "for model_name, model in classifiers:",
            "",
            "# 5-fold Stratified Cross-Validation",
            "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)",
            "",
            "# Perform cross-validation with train scores",
            "cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1, return_train_score=True)",
            "",
            "# Calculate cross-validation error",
            "cross_val_error = 1 - np.mean(cv_results['test_score'])",
            "",
            "# Append results to the list",
            "results.append({",
            "\"Model Name\": model_name,",
            "\"Mean Train Accuracy\": np.mean(cv_results['train_score']),",
            "\"Mean Test Accuracy\": np.mean(cv_results['test_score']),",
            "\"Cross-Validation Error\": cross_val_error",
            "})",
            "",
            "mean_test_accuracy_scores.append(np.mean(cv_results['test_score']))",
            "cross_val_errors.append(cross_val_error)",
            "classifier_names.append(model_name)",
            "",
            "# Create a DataFrame from the results list",
            "results_df = pd.DataFrame(results)",
            "",
            "# Display the DataFrame",
            "display(results_df)",
            "# Creating a DataFrame for test accuracy and cross-validation error",
            "data = pd.DataFrame({",
            "'Classifier': classifier_names,",
            "'Test Accuracy': mean_test_accuracy_scores,",
            "'Cross-Validation Error': cross_val_errors",
            "})",
            "",
            "# Creating Plotly subplots with two columns and one row",
            "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=['Mean Test Accuracy', 'Cross-Validation Error'],",
            "vertical_spacing=0.1, horizontal_spacing=0.20)",
            "",
            "# Adding bar chart for Mean Test Accuracy",
            "fig.add_trace(go.Bar(x=data['Test Accuracy'], y=data['Classifier'], orientation='h',",
            "text=data['Test Accuracy'], marker=dict(color=data['Test Accuracy'], colorscale='RdBu'),",
            "showlegend=False),",
            "row=1, col=1)",
            "",
            "# Sort the DataFrame by Cross-Validation Error in descending order",
            "data = data.sort_values(by='Cross-Validation Error', ascending=False)",
            "",
            "# Adding bar chart for Cross-Validation Error",
            "fig.add_trace(go.Bar(x=data['Cross-Validation Error'], y=data['Classifier'], orientation='h',",
            "text=data['Cross-Validation Error'], marker=dict(color=data['Cross-Validation Error'], colorscale='RdBu'),",
            "showlegend=False),",
            "row=1, col=2)",
            "",
            "# Customizing the layout",
            "fig.update_layout(title='Model Evaluation Metrics', template='plotly_dark',",
            "xaxis=dict(range=[0, 1]), yaxis=dict(categoryorder='total ascending'))",
            "",
            "# Show the plot",
            "fig.show()",
            "# Initialize CatBoost classifier",
            "CatBoost = CatBoostClassifier(random_state=42, verbose=0)",
            "",
            "# Train the model",
            "CatBoost.fit(X_train, y_train)",
            "",
            "# Predictions on test data",
            "y_pred = CatBoost.predict(X_test)",
            "",
            "# Calculate F1-score",
            "f1 = f1_score(y_test, y_pred, average='weighted')",
            "",
            "# Printing model details",
            "print(f'Model: CatBoost')",
            "print(f'Training Accuracy: {accuracy_score(y_train, CatBoost.predict(X_train))}')",
            "print(f'Testing Accuracy: {accuracy_score(y_test, y_pred)}')",
            "print(f'F1-score: {f1}')",
            "print('------------------------------------------------------------------')",
            "# Create confusion matrix",
            "conf_matrix = confusion_matrix(y_test, y_pred)",
            "",
            "# Plot confusion matrix with plotly",
            "fig = ff.create_annotated_heatmap(z=conf_matrix, x=['Not Transported', 'Transported'], y=['Not Transported', 'Transported'],",
            "colorscale='Viridis', showscale=True)",
            "",
            "fig.update_layout(title='Confusion Matrix',",
            "xaxis_title='Predicted label',",
            "yaxis_title='True label',",
            "width=500, height=400)",
            "",
            "# Show the figure",
            "fig.show()",
            "param_grid = {",
            "'max_depth': [7, 8 ,9],",
            "'n_estimators':[300]",
            "}",
            "CatBoost = CatBoostClassifier(random_state=42, verbose=0)",
            "",
            "grid_search = GridSearchCV(estimator=CatBoost, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1,",
            "return_train_score=True)",
            "",
            "# Fit the grid search to the data",
            "grid_search.fit(X=X_train, y=y_train)",
            "",
            "# Get the best parameters and best score",
            "best_params = grid_search.best_params_",
            "best_score = grid_search.best_score_",
            "",
            "print(\"Best Parameters:\", best_params)",
            "print(\"Best Score:\", best_score)",
            "",
            "# Get the best model",
            "final_model = grid_search.best_estimator_",
            "",
            "# Evaluate the model on the test set",
            "y_pred_test = final_model.predict(X_test)",
            "",
            "# Calculate the testing accuracy",
            "testing_accuracy = accuracy_score(y_test, y_pred_test)",
            "",
            "# Print the testing accuracy",
            "print('------------------------------------')",
            "print(\"Testing Accuracy:\", testing_accuracy)",
            "",
            "# Get the mean test score and mean train score for the best estimator",
            "mean_test_score = grid_search.cv_results_['mean_test_score'][grid_search.best_index_]",
            "mean_train_score = grid_search.cv_results_['mean_train_score'][grid_search.best_index_]",
            "",
            "print('------------------------------------')",
            "print(\"Mean Train Score:\", mean_train_score)",
            "print(\"Mean Test Score:\", mean_test_score)",
            "final_model",
            "# Predict probabilities for the positive class using the final model",
            "y_probabilities = final_model.predict_proba(X_train)[:, 1]",
            "",
            "# Calculate the ROC curve and AUC score",
            "fpr, tpr, thresholds = roc_curve(y_train, y_probabilities)",
            "auc = roc_auc_score(y_train, y_probabilities)",
            "",
            "# Create ROC curve trace",
            "roc_trace = go.Scatter(",
            "x=fpr,",
            "y=tpr,",
            "mode='lines',",
            "line=dict(color='#10c2de', width=2),",
            "name=f'ROC curve (AUC = {auc:.2f})'",
            ")",
            "",
            "# Add diagonal line",
            "diagonal_trace = go.Scatter(",
            "x=[0, 1],",
            "y=[0, 1],",
            "mode='lines',",
            "line=dict(color='#eb3134', width=2, dash='dash'),",
            "name='Random Guess'",
            ")",
            "",
            "# Create layout",
            "layout = go.Layout(",
            "title='ROC Curve for CatBoost Classifier',",
            "xaxis=dict(title='False Positive Rate'),",
            "yaxis=dict(title='True Positive Rate'),",
            "showlegend=True,",
            "template='plotly_dark'",
            ")",
            "",
            "# Create figure",
            "fig = go.Figure(data=[roc_trace, diagonal_trace], layout=layout)",
            "",
            "# Show the figure",
            "fig.show()",
            "# Making predictions on the test dataset using the final trained model",
            "predictions = final_model.predict(",
            "df_test.drop('PassengerId', axis=1)",
            ")",
            "output = pd.DataFrame(",
            "{",
            "'PassengerId': df_test.PassengerId,",
            "'Transported': predictions",
            "}",
            ")",
            "output['Transported'] = predictions > 0.5",
            "output",
            "output.to_csv('submission.csv', index=False)",
            "print(\"Your submission was successfully saved!\")"
        ]
    },
    "spaceship-titanic-competition-with-ensemble-models.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__5_adends__spaceship-titanic-competition-with-ensemble-models/spaceship-titanic-competition-with-ensemble-models.ipynb",
        "code": [
            "# This Python 3 environment comes with many helpful analytics libraries installed",
            "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
            "# For example, here's several helpful packages to load",
            "",
            "import numpy as np # linear algebra",
            "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
            "from category_encoders import BinaryEncoder",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.model_selection import train_test_split, cross_val_score",
            "from sklearn.compose import ColumnTransformer",
            "from sklearn.pipeline import Pipeline",
            "from sklearn.metrics import mean_squared_log_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score",
            "from sklearn.impute import KNNImputer",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler",
            "from xgboost import XGBClassifier",
            "from sklearn.ensemble import RandomForestClassifier, VotingClassifier",
            "import matplotlib.pyplot as plt",
            "from matplotlib.colors import LinearSegmentedColormap",
            "import seaborn as sns",
            "import optuna",
            "",
            "plt.style.use('fivethirtyeight')",
            "",
            "# Input data files are available in the read-only \"/share/dutta/eyao/dataset/kaggle/\" directory",
            "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
            "",
            "import os",
            "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):",
            "for filename in filenames:",
            "print(os.path.join(dirname, filename))",
            "",
            "# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using \"Save & Run All\"",
            "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')",
            "df_test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')",
            "df.head()",
            "df.info()",
            "labels = ['Not Transported', 'Transported']",
            "",
            "fig, ax = plt.subplots(figsize=(10, 6))",
            "df['Transported'].value_counts().plot(kind='pie', ax=ax, labels=labels, autopct='%1.1f%%', colors=['coral', 'teal'])",
            "",
            "ax.set_xlabel('Passenger Fate')",
            "ax.set_ylabel('')",
            "ax.legend(title='Fates')",
            "",
            "plt.show()",
            "numeric_df = df.select_dtypes(include=['int64', 'float64'])",
            "corr = numeric_df.corr()",
            "",
            "colors = [\"coral\", \"white\", \"teal\"]",
            "cmap = LinearSegmentedColormap.from_list(\"custom_coral_teal\", colors)",
            "",
            "plt.figure(figsize=(10, 8))",
            "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=cmap,",
            "xticklabels=corr.columns, yticklabels=corr.columns,",
            "cbar_kws={'label': 'Correlation coefficient'})",
            "",
            "plt.title('Correlation Heatmap')",
            "plt.xticks(rotation=45)",
            "plt.yticks(rotation=45)",
            "plt.show()",
            "df.isna().sum()",
            "print(f'HomePlanet: {len(df.HomePlanet.unique())} \\nCabin: {len(df.Cabin.unique())}\\nDestination:{len(df.Destination.unique())}\\nName:{len(df.Name.unique())}')",
            "df[['Cabin1', 'Cabin2', 'Cabin3']] = df['Cabin'].str.split('/', expand=True)",
            "df[['FirstName', 'LastName']] = df['Name'].str.split(' ', expand=True)",
            "df['total'] = df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)",
            "df['AgeBin'] = pd.qcut(df['Age'].fillna(df['Age'].mode()[0]), q=5, labels=False)",
            "",
            "",
            "df.drop(['Cabin', 'Name'], axis=1, inplace=True)",
            "",
            "df.head()",
            "print(f\"Cabin1: {len(df.Cabin1.unique())} \\nCabin2: {len(df.Cabin2.unique())}\\nCabin3: {len(df.Cabin3.unique())}\\nFirstName: {len(df.FirstName.unique())}\\nLastName: {len(df.LastName.unique())}\")",
            "y = df['Transported'].astype(int)",
            "df.drop(['Transported'], axis=1, inplace=True)",
            "encoder = BinaryEncoder(cols=['FirstName', 'LastName', 'Cabin1', 'Cabin3', 'Destination', 'VIP', 'HomePlanet'], return_df=True)",
            "df = encoder.fit_transform(df)",
            "",
            "df.drop(['VIP_1'], axis=1,inplace=True)",
            "",
            "df.columns",
            "# scaled_cols = ['Age', 'RoomService', 'FoodCourt']",
            "",
            "X = df",
            "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=42)",
            "xgb_params = {'n_estimators': 248, 'learning_rate': 0.08276477030425759, 'max_depth': 4, 'reg_lambda': 9.144307734410582, 'subsample': 0.9761017636523421}",
            "rf_params = {'n_estimators': 829, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}",
            "",
            "xgb_model = XGBClassifier(**xgb_params)",
            "rf_model = RandomForestClassifier(**rf_params)",
            "",
            "pipeline = Pipeline([",
            "('impute', KNNImputer(weights='distance', n_neighbors=3)),",
            "('scale', MinMaxScaler()),",
            "('xgb', VotingClassifier(",
            "estimators=[",
            "('xgb', xgb_model),",
            "('rf', rf_model)",
            "]",
            ")),",
            "])",
            "",
            "best_model = pipeline.fit(X_train, Y_train)",
            "",
            "Y_pred = best_model.predict(X_test)",
            "",
            "# Y_proba = best_model.predict_proba(X_test)[:, 1]",
            "",
            "accuracy = accuracy_score(Y_test, Y_pred)",
            "print(f\"Accuracy: {accuracy}\")",
            "",
            "precision = precision_score(Y_test, Y_pred)",
            "print(f\"Precision: {precision}\")",
            "",
            "recall = recall_score(Y_test, Y_pred)",
            "print(f\"Recall: {recall}\")",
            "",
            "f1 = f1_score(Y_test, Y_pred)",
            "print(f\"F1 Score: {f1}\")",
            "",
            "# roc_auc = roc_auc_score(Y_test, Y_proba)",
            "# print(f\"ROC-AUC Score: {roc_auc}\")",
            "df_test[['Cabin1', 'Cabin2', 'Cabin3']] = df_test['Cabin'].str.split('/', expand=True)",
            "df_test[['FirstName', 'LastName']] = df_test['Name'].str.split(' ', expand=True)",
            "df_test['total'] = df_test[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)",
            "df_test['AgeBin'] = pd.qcut(df_test['Age'].fillna(df_test['Age'].mode()[0]), q=5, labels=False)",
            "df_test.drop(['Cabin', 'Name'], axis=1, inplace=True)",
            "df_test = encoder.transform(df_test)",
            "df_test.drop(['VIP_1'], axis=1,inplace=True)",
            "df_test['LastName_11'] = 0",
            "",
            "",
            "preds = best_model.predict(df_test)",
            "preds = preds.astype(bool)",
            "df_test['Transported'] = preds",
            "submission_df = df_test[['PassengerId', 'Transported']]",
            "submission_df.to_csv('submission.csv', index=False)"
        ]
    },
    "house-prices-prediction.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__dilekankaya__house-prices-prediction/house-prices-prediction.ipynb",
        "code": [
            "import numpy as np",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import warnings",
            "from catboost import CatBoostRegressor",
            "from lightgbm import LGBMRegressor",
            "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet",
            "from sklearn.neighbors import KNeighborsRegressor",
            "from sklearn.svm import SVR",
            "from sklearn.tree import DecisionTreeRegressor",
            "from xgboost import XGBRegressor",
            "from sklearn.preprocessing import LabelEncoder",
            "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score",
            "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV",
            "warnings.simplefilter(action='ignore', category=FutureWarning)",
            "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)",
            "pd.set_option('display.max_columns', None)",
            "pd.set_option('display.width', None)",
            "pd.set_option('display.float_format', lambda x: '%.3f' % x)",
            "train = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv\")",
            "test = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv\")",
            "df = pd.concat([train, test], ignore_index=True).reset_index(drop=True)",
            "def check_df(dataframe):",
            "print(\"##################### Shape #####################\")",
            "print(dataframe.shape)",
            "print(\"##################### Types #####################\")",
            "print(dataframe.dtypes)",
            "print(\"##################### Head #####################\")",
            "print(dataframe.head(3))",
            "print(\"##################### Tail #####################\")",
            "print(dataframe.tail(3))",
            "print(\"##################### NA #####################\")",
            "print(dataframe.isnull().sum())",
            "print(\"##################### Quantiles #####################\")",
            "numeric_columns = dataframe.select_dtypes(include=['number']).columns",
            "print(dataframe[numeric_columns].quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)",
            "",
            "check_df(df)",
            "def grab_col_names(dataframe, cat_th=10, car_th=20):",
            "cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]",
            "num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes != \"O\"]",
            "cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtypes == \"O\"]",
            "cat_cols = cat_cols + num_but_cat",
            "cat_cols = [col for col in cat_cols if col not in cat_but_car]",
            "num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]",
            "num_cols = [col for col in num_cols if col not in num_but_cat]",
            "print(f\"Observations: {dataframe.shape[0]}\")",
            "print(f\"Variables: {dataframe.shape[1]}\")",
            "print(f'cat_cols: {len(cat_cols)}')",
            "print(f'num_cols: {len(num_cols)}')",
            "print(f'cat_but_car: {len(cat_but_car)}')",
            "print(f'num_but_cat: {len(num_but_cat)}')",
            "return cat_cols, cat_but_car, num_cols",
            "",
            "cat_cols, cat_but_car, num_cols = grab_col_names(df)",
            "def cat_summary(dataframe, col_name, plot=False):",
            "print(pd.DataFrame({col_name: dataframe[col_name].value_counts(), \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))",
            "if plot:",
            "sns.countplot(x=dataframe[col_name], data=dataframe)",
            "plt.show()",
            "",
            "for col in cat_cols:",
            "cat_summary(df, col)",
            "def num_summary(dataframe, numerical_col, plot=False):",
            "quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]",
            "print(dataframe[numerical_col].describe(quantiles).T)",
            "if plot:",
            "dataframe[numerical_col].hist(bins=50)",
            "plt.xlabel(numerical_col)",
            "plt.title(numerical_col)",
            "plt.show()",
            "print(\"#####################################\")",
            "for col in num_cols:",
            "num_summary(df, col, True)",
            "def target_summary_with_cat(dataframe, target, categorical_col):",
            "print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")",
            "",
            "for col in cat_cols:",
            "target_summary_with_cat(df, \"SalePrice\", col)",
            "df[\"SalePrice\"].hist(bins=100)",
            "plt.show()",
            "",
            "np.log1p(df['SalePrice']).hist(bins=50)",
            "plt.show()",
            "corr = df[num_cols].corr()",
            "sns.set(rc={'figure.figsize': (12, 12)})",
            "sns.heatmap(corr, cmap=\"RdBu\")",
            "plt.show()",
            "def high_correlated_cols(dataframe, plot=False, corr_th=0.70):",
            "numeric_df = dataframe.select_dtypes(include=[np.number])",
            "corr = numeric_df.corr()",
            "cor_matrix = corr.abs()",
            "upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))",
            "drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]",
            "if plot:",
            "sns.set(rc={'figure.figsize': (15, 15)})",
            "sns.heatmap(corr, cmap=\"RdBu\")",
            "plt.show()",
            "return drop_list",
            "",
            "drop_list = high_correlated_cols(df, plot=False)",
            "print(drop_list)",
            "",
            "saleprice_corr = corr['SalePrice'].abs().sort_values(ascending=False)",
            "print(saleprice_corr)",
            "",
            "top_corr_features = saleprice_corr.index[:10]",
            "print(\"Columns with highest correlation: \", top_corr_features)",
            "def outlier_thresholds(dataframe, variable, low_quantile=0.10, up_quantile=0.90):",
            "quantile_one = dataframe[variable].quantile(low_quantile)",
            "quantile_three = dataframe[variable].quantile(up_quantile)",
            "interquantile_range = quantile_three - quantile_one",
            "up_limit = quantile_three + 1.5 * interquantile_range",
            "low_limit = quantile_one - 1.5 * interquantile_range",
            "return low_limit, up_limit",
            "",
            "def check_outlier(dataframe, col_name):",
            "low_limit, up_limit = outlier_thresholds(dataframe, col_name)",
            "if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):",
            "return True",
            "else:",
            "return False",
            "for col in num_cols:",
            "if col != \"SalePrice\":",
            "print(col, check_outlier(df, col))",
            "def replace_with_thresholds(dataframe, variable):",
            "low_limit, up_limit = outlier_thresholds(dataframe, variable)",
            "dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit",
            "dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit",
            "",
            "for col in num_cols:",
            "if col != \"SalePrice\":",
            "replace_with_thresholds(df, col)",
            "for col in num_cols:",
            "if col != \"SalePrice\":",
            "print(col, check_outlier(df, col))",
            "def missing_values_table(dataframe, na_name=False):",
            "na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]",
            "n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)",
            "ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)",
            "missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])",
            "print(missing_df, end=\"\\n\")",
            "if na_name:",
            "return na_columns",
            "",
            "missing_values_table(df)",
            "no_cols = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\",",
            "\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]",
            "",
            "for col in no_cols:",
            "df[col].fillna(\"No\", inplace=True)",
            "",
            "missing_values_table(df)",
            "",
            "def quick_missing_imp(data, num_method=\"median\", cat_length=20, target=\"SalePrice\"):",
            "variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]",
            "temp_target = data[target]",
            "print(\"# BEFORE\")",
            "print(data[variables_with_na].isnull().sum(), \"\\n\\n\")",
            "data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)",
            "if num_method == \"mean\":",
            "data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)",
            "elif num_method == \"median\":",
            "data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)",
            "data[target] = temp_target",
            "print(\"# AFTER \\n Imputation method is 'MODE' for categorical variables!\")",
            "print(\" Imputation method is '\" + num_method.upper() + \"' for numeric variables! \\n\")",
            "print(data[variables_with_na].isnull().sum(), \"\\n\\n\")",
            "return data",
            "",
            "df = quick_missing_imp(df, num_method=\"median\", cat_length=17)",
            "missing_values_table(df)",
            "def rare_analyser(dataframe, target, cat_cols):",
            "for col in cat_cols:",
            "print(col, \":\", len(dataframe[col].value_counts()))",
            "print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),",
            "\"RATIO\": dataframe[col].value_counts() / len(dataframe),",
            "\"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")",
            "",
            "rare_analyser(df, \"SalePrice\", cat_cols)",
            "",
            "def rare_encoder(dataframe, rare_perc):",
            "temp_df = dataframe.copy()",
            "rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'",
            "and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]",
            "for var in rare_columns:",
            "tmp = temp_df[var].value_counts() / len(temp_df)",
            "rare_labels = tmp[tmp < rare_perc].index",
            "temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])",
            "return temp_df",
            "",
            "df = rare_encoder(df, 0.01)",
            "df[\"NEW_1st*GrLiv\"] = df[\"1stFlrSF\"] * df[\"GrLivArea\"]",
            "df[\"NEW_Garage*GrLiv\"] = df[\"GarageArea\"] * df[\"GrLivArea\"]",
            "qual_columns = [\"OverallQual\", \"OverallCond\", \"ExterQual\", \"ExterCond\", \"BsmtCond\", \"BsmtFinType1\",",
            "\"BsmtFinType2\", \"HeatingQC\", \"KitchenQual\", \"Functional\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"Fence\"]",
            "for col in qual_columns:",
            "df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)",
            "df[\"TotalQual\"] = df[qual_columns].sum(axis=1)",
            "df[\"NEW_TotalFlrSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]",
            "df[\"NEW_TotalBsmtFin\"] = df.BsmtFinSF1 + df.BsmtFinSF2",
            "df[\"NEW_PorchArea\"] = df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch + df[\"3SsnPorch\"] + df.WoodDeckSF",
            "df[\"NEW_TotalHouseArea\"] = df.NEW_TotalFlrSF + df.TotalBsmtSF",
            "df[\"NEW_TotalSqFeet\"] = df.GrLivArea + df.TotalBsmtSF",
            "df[\"NEW_LotRatio\"] = df.GrLivArea / df.LotArea",
            "df[\"NEW_RatioArea\"] = df.NEW_TotalHouseArea / df.LotArea",
            "df[\"NEW_GarageLotRatio\"] = df.GarageArea / df.LotArea",
            "df[\"NEW_MasVnrRatio\"] = df.MasVnrArea / df.NEW_TotalHouseArea",
            "df[\"NEW_DifArea\"] = df.LotArea - df[\"1stFlrSF\"] - df.GarageArea - df.NEW_PorchArea - df.WoodDeckSF",
            "df[\"NEW_OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]",
            "df[\"NEW_Restoration\"] = df.YearRemodAdd - df.YearBuilt",
            "df[\"NEW_HouseAge\"] = df.YrSold - df.YearBuilt",
            "df[\"NEW_RestorationAge\"] = df.YrSold - df.YearRemodAdd",
            "df[\"NEW_GarageAge\"] = df.GarageYrBlt - df.YearBuilt",
            "df[\"NEW_GarageRestorationAge\"] = np.abs(df.GarageYrBlt - df.YearRemodAdd)",
            "df[\"NEW_GarageSold\"] = df.YrSold - df.GarageYrBlt",
            "drop_list = [\"Street\", \"Alley\", \"LandContour\", \"Utilities\", \"LandSlope\", \"Heating\", \"PoolQC\", \"MiscFeature\", \"Neighborhood\"]",
            "existing_columns = [col for col in drop_list if col in df.columns]",
            "df.drop(existing_columns, axis=1, inplace=True)",
            "cat_cols, cat_but_car, num_cols = grab_col_names(df)",
            "def label_encoder(dataframe, binary_col):",
            "labelencoder = LabelEncoder()",
            "dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])",
            "return dataframe",
            "",
            "binary_cols = [col for col in df.columns if df[col].dtypes == \"O\" and len(df[col].unique()) == 2]",
            "",
            "for col in binary_cols:",
            "label_encoder(df, col)",
            "print(df[binary_cols].head())",
            "df_before_encoding = df.copy()",
            "cat_cols = [col for col in cat_cols if col in df.columns]",
            "",
            "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):",
            "dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)",
            "return dataframe",
            "",
            "df = one_hot_encoder(df, cat_cols, drop_first=True)",
            "",
            "new_columns = set(df.columns) - set(df_before_encoding.columns)",
            "print(\"Changes due to one-hot encoding:\")",
            "print(df[list(new_columns)].head())",
            "train_df = df[df['SalePrice'].notnull()]",
            "test_df = df[df['SalePrice'].isnull()]",
            "",
            "y = np.log1p(train_df['SalePrice'])",
            "X = train_df.drop([\"Id\", \"SalePrice\"], axis=1)",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)",
            "models = [('LR', LinearRegression()), ('KNN', KNeighborsRegressor()), ('CART', DecisionTreeRegressor()),",
            "('RF', RandomForestRegressor()), ('GBM', GradientBoostingRegressor()),",
            "(\"XGBoost\", XGBRegressor(objective='reg:squarederror')), (\"LightGBM\", LGBMRegressor(verbose=-1))]",
            "for name, regressor in models:",
            "regressor.fit(X_train, y_train)",
            "y_pred = regressor.predict(X_test)",
            "y_pred_exp = np.expm1(y_pred)",
            "y_test_exp = np.expm1(y_test)",
            "rmse = np.sqrt(mean_squared_error(y_test_exp, y_pred_exp))",
            "print(f\"RMSE: {round(rmse, 4)} ({name})\")",
            "lgbm = LGBMRegressor(verbose=-1).fit(X_train, y_train)",
            "y_pred = lgbm.predict(X_test)",
            "def calculate_metrics(y_true, y_pred):",
            "mae = mean_absolute_error(y_true, y_pred)",
            "mse = mean_squared_error(y_true, y_pred)",
            "rmse = np.sqrt(mse)",
            "r2 = r2_score(y_true, y_pred)",
            "return mae, mse, rmse, r2",
            "",
            "def evaluate_percentiles(y_true, y_pred):",
            "percentiles = [5, 25, 50, 75, 95, 100]",
            "results = {}",
            "for percentile in percentiles:",
            "threshold = np.percentile(y_true, percentile)",
            "indices = y_true <= threshold",
            "filtered_y_true = y_true[indices]",
            "filtered_y_pred = y_pred[indices]",
            "mae, mse, rmse, r2 = calculate_metrics(filtered_y_true, filtered_y_pred)",
            "results[percentile] = (mae, mse, rmse, r2)",
            "return results",
            "def print_results(results):",
            "for percentile, metrics in results.items():",
            "mae, mse, rmse, r2 = metrics",
            "print(f\"Performance for {percentile}th Percentile:\")",
            "print(f\"  Mean Absolute Error (MAE): {mae}\")",
            "print(f\"  Mean Squared Error (MSE): {mse}\")",
            "print(f\"  Root Mean Squared Error (RMSE): {rmse}\")",
            "print(f\"  R-squared (R\u00b2): {r2}\")",
            "print()",
            "y_pred_exp = np.expm1(y_pred)",
            "y_test_exp = np.expm1(y_test)",
            "results = evaluate_percentiles(y_test_exp, y_pred_exp)",
            "print_results(results)",
            "lgbm_model = LGBMRegressor(random_state=46, verbose=-1)",
            "rmse = np.mean(np.sqrt(-cross_val_score(lgbm_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "",
            "lgbm_params = {\"learning_rate\": [0.01, 0.1], \"n_estimators\": [500, 1500]}",
            "lgbm_gs_best = GridSearchCV(lgbm_model, lgbm_params, cv=3, n_jobs=-1, verbose=True).fit(X, y)",
            "",
            "final_model = lgbm_model.set_params(**lgbm_gs_best.best_params_, random_state=46, verbose=-1).fit(X, y)",
            "rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "print(\"Mean RMSE:\", rmse)",
            "def plot_importance(model, features, num=len(X), save=False):",
            "feature_imp = pd.DataFrame({\"Value\": model.feature_importances_, \"Feature\": features.columns})",
            "plt.figure(figsize=(10, 10))",
            "sns.set(font_scale=1)",
            "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:num])",
            "plt.title(\"Features\")",
            "plt.tight_layout()",
            "plt.show()",
            "if save:",
            "plt.savefig(\"importances.png\")",
            "",
            "model = LGBMRegressor(verbose=-1)",
            "model.fit(X, y)",
            "plot_importance(model, X)",
            "model = LGBMRegressor(verbose=-1)",
            "model.fit(X, y)",
            "predictions = model.predict(test_df.drop([\"Id\", \"SalePrice\"], axis=1))",
            "real_predictions = np.exp(predictions)",
            "submission = pd.DataFrame({",
            "\"Id\": test_df[\"Id\"].astype(int),",
            "\"SalePrice\": real_predictions",
            "})",
            "submission.to_csv(\"housePricePredictions.csv\", index=False)"
        ]
    },
    "spaceship-titanic-ml.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__8_ahmedanwar89__spaceship-titanic-ml/spaceship-titanic-ml.ipynb",
        "code": [
            "import numpy as np",
            "import pandas as pd",
            "import plotly.express as px",
            "import sklearn",
            "sample_submission = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/sample_submission.csv')",
            "sample_submission.head()",
            "test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')",
            "test.head()",
            "train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')",
            "train.head()",
            "train.info()",
            "train.isnull().sum()",
            "train['Age'].fillna(train['Age'].mean().round(0),inplace=True)",
            "train['FoodCourt'].fillna(train['FoodCourt'].mean(),inplace=True)",
            "train['RoomService'].fillna(train['RoomService'].mean(),inplace=True)",
            "train['ShoppingMall'].fillna(train['ShoppingMall'].mean(),inplace=True)",
            "train['Spa'].fillna(train['Spa'].mean(),inplace=True)",
            "train['VRDeck'].fillna(train['VRDeck'].mean(),inplace=True)",
            "train.isnull().sum()",
            "train.drop(columns=['HomePlanet','CryoSleep','Cabin','Destination','VIP','Name'],axis=1,inplace=True)",
            "train.isnull().sum()",
            "train.duplicated().sum()",
            "test.info()",
            "test.isnull().sum()",
            "test['Age'].fillna(test['Age'].mean().round(0),inplace=True)",
            "test['FoodCourt'].fillna(test['FoodCourt'].mean(),inplace=True)",
            "test['RoomService'].fillna(test['RoomService'].mean(),inplace=True)",
            "test['ShoppingMall'].fillna(test['ShoppingMall'].mean(),inplace=True)",
            "test['Spa'].fillna(test['Spa'].mean(),inplace=True)",
            "test['VRDeck'].fillna(test['VRDeck'].mean(),inplace=True)",
            "test.isnull().sum()",
            "test.drop(columns=['HomePlanet','CryoSleep','Cabin','Destination','VIP','Name'],axis=1,inplace=True)",
            "test.isnull().sum()",
            "test.duplicated().sum()",
            "train.head(2)",
            "px.imshow(train.corr(numeric_only=True),aspect=True,text_auto=True,color_continuous_scale='Blues')",
            "px.bar(train.pivot_table(index='Age',columns='Transported',values='PassengerId',aggfunc='count'),barmode='group',height=400)",
            "px.bar(train.pivot_table(index='Age',columns='Transported',values='PassengerId',aggfunc='count'),barmode='group',height=400)",
            "px.scatter(data_frame=train,x='Transported',y='FoodCourt')",
            "train.head()",
            "test.head()",
            "y_train = train['Transported']",
            "y_train",
            "X_train = train.drop(columns='Transported',axis=1)",
            "X_train.head()",
            "from sklearn.linear_model import LinearRegression",
            "model = LinearRegression()",
            "model.fit(X_train,y_train)",
            "y_predict = model.predict(test)",
            "y_predict",
            "model.score(X_train,y_train)",
            "from sklearn.neighbors import KNeighborsClassifier",
            "model = KNeighborsClassifier(n_neighbors=2)",
            "model.fit(X_train,y_train)",
            "model.score(X_train,y_train)",
            "from sklearn.ensemble import RandomForestClassifier",
            "model = RandomForestClassifier()",
            "model.fit(X_train,y_train)",
            "y_predict = model.predict(test)",
            "y_predict",
            "model.score(X_train,y_train)",
            "output = pd.DataFrame({",
            "'PassengerId':test.PassengerId,",
            "'Transported':y_predict",
            "})",
            "output.shape",
            "output.to_csv(r'C:\\Users\\roaia\\Desktop\\Anwar\\New folder\\output.csv',index=False)"
        ]
    },
    "spaceship-titanic-code.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__9_ahmedgaitani__spaceship-titanic-code/spaceship-titanic-code.ipynb",
        "code": [
            "import pandas as pd",
            "import numpy as np",
            "import seaborn as sns",
            "import matplotlib.pyplot as plt",
            "# from sklearn.preprocessing import OneHotEncoder",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.preprocessing import LabelEncoder",
            "from sklearn.metrics import accuracy_score",
            "from sklearn.metrics import classification_report",
            "import xgboost as xgb",
            "train_df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')",
            "test_df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')",
            "# Display the first few rows of the training data",
            "print(\"Training Data Head:\")",
            "print(train_df.head())",
            "print(\"\\nTraining Data Info:\")",
            "print(train_df.info())",
            "print(\"\\nSummary Statistics:\")",
            "print(train_df.describe())",
            "print(\"\\nMissing Values:\")",
            "print(train_df.isnull().sum())",
            "# 1. Distribution of the Target Variable (Transported)",
            "plt.figure(figsize=(8, 6))",
            "sns.countplot(x='Transported', data=train_df)",
            "plt.title('Distribution of Target Variable (Transported)')",
            "plt.show()",
            "# 2. Age Distribution",
            "plt.figure(figsize=(8, 6))",
            "sns.histplot(train_df['Age'].dropna(), bins=30, kde=True)",
            "plt.title('Age Distribution')",
            "plt.show()",
            "# 3. Cabin Class Distribution",
            "plt.figure(figsize=(8, 6))",
            "sns.countplot(x='Cabin', data=train_df)",
            "plt.title('Cabin Class Distribution')",
            "plt.show()",
            "def fill_missing_values(df):",
            "df['HomePlanet'] = df['HomePlanet'].fillna('Earth')",
            "df['CryoSleep'] = df['CryoSleep'].fillna(False).infer_objects(copy=False)",
            "df['Cabin'] = df['Cabin'].fillna('Unknown')",
            "df['Destination'] = df['Destination'].fillna('TRAPPIST-1e')",
            "df['Age'] = df['Age'].fillna(df['Age'].median())",
            "df['VIP'] = df['VIP'].fillna(False).infer_objects(copy=False)",
            "df = df.fillna(0)",
            "return df",
            "",
            "# Apply missing value filling",
            "train_df = fill_missing_values(train_df)",
            "test_df = fill_missing_values(test_df)",
            "",
            "# Convert columns to string type to ensure uniformity",
            "label_cols = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']",
            "for col in label_cols:",
            "train_df[col] = train_df[col].astype(str)",
            "test_df[col] = test_df[col].astype(str)",
            "",
            "# Combine data from both datasets for fitting the encoder",
            "combined_data = pd.concat([train_df[label_cols], test_df[label_cols]], axis=0)",
            "",
            "# Label encoding using combined data",
            "label_encoders = {col: LabelEncoder().fit(combined_data[col]) for col in label_cols}",
            "",
            "# Apply label encoding to the train and test sets",
            "for col, le in label_encoders.items():",
            "train_df[col] = le.transform(train_df[col])",
            "test_df[col] = le.transform(test_df[col])",
            "",
            "# Prepare features and target",
            "X = train_df.drop(['PassengerId', 'Name', 'Transported'], axis=1)",
            "y = train_df['Transported'].astype(int)",
            "X_test = test_df.drop(['PassengerId', 'Name'], axis=1)",
            "# Split the data into train and validation sets",
            "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)",
            "# Train the model",
            "model = xgb.XGBClassifier( n_estimators= 200, learning_rate= 0.1, max_depth = 5)",
            "model.fit(X_train, y_train)",
            "# Predict on validation set",
            "y_pred = model.predict(X_val)",
            "# Calculate accuracy",
            "print('Classification Report:')",
            "print(classification_report(y_val, y_pred))",
            "# Predict on the test set",
            "test_pred = model.predict(X_test)",
            "# Prepare submission file",
            "submission = pd.DataFrame({",
            "'PassengerId': test_df['PassengerId'],",
            "'Transported': test_pred",
            "})",
            "# Convert boolean predictions to string (True/False)",
            "submission['Transported'] = submission['Transported'].map({1: True, 0: False})",
            "# Save the submission file",
            "submission.to_csv('submission.csv', index=False)",
            "print('Submission file saved as submission.csv')"
        ]
    },
    "house-prices-prediction-detailed-eda-10-model.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__chanchal24__house-prices-prediction-detailed-eda-10-model/house-prices-prediction-detailed-eda-10-model.ipynb",
        "code": [
            "# This Python 3 environment comes with many helpful analytics libraries installed",
            "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
            "# For example, here's several helpful packages to load",
            "",
            "import numpy as np # linear algebra",
            "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
            "",
            "# Input data files are available in the read-only \"/share/dutta/eyao/dataset/kaggle/\" directory",
            "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
            "",
            "import os",
            "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):",
            "for filename in filenames:",
            "print(os.path.join(dirname, filename))",
            "",
            "# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using \"Save & Run All\"",
            "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "import pandas as pd",
            "import numpy as np",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import plotly.express as px",
            "import math",
            "from scipy.stats import zscore",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.linear_model import Lasso",
            "from sklearn.linear_model import Ridge",
            "from sklearn.svm import SVR",
            "from sklearn.tree import DecisionTreeRegressor",
            "from sklearn.ensemble import RandomForestRegressor",
            "from sklearn.metrics import r2_score",
            "from sklearn.preprocessing import PolynomialFeatures",
            "from sklearn.metrics import mean_squared_error",
            "from xgboost import XGBRegressor",
            "import tensorflow as tf",
            "import warnings",
            "pd.set_option('display.max_columns', None)",
            "warnings.filterwarnings(\"ignore\")",
            "df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv',index_col='Id')",
            "df.head()",
            "df.tail()",
            "df.info()",
            "df.describe()",
            "print(\"Percentage of Nan in each Column\")",
            "for column, percentage in ((df.isna().sum() / df.shape[0]) * 100).items():",
            "count = math.ceil(percentage)",
            "if count > 0:",
            "print(f\"{column} : {count} %\")",
            "df.drop(columns=['MasVnrType','PoolQC','PoolArea','BsmtHalfBath','KitchenAbvGr','Utilities'],inplace=True)",
            "df['LotFrontage'] = df['LotFrontage'].fillna(0)",
            "df['Alley'] = df['Alley'].fillna(\"No Alley Acess\")",
            "df['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].mean())",
            "df['LotFrontage'] = df['LotFrontage'].fillna(0)",
            "df['BsmtQual'] = df['BsmtQual'].fillna(\"No Basment\")",
            "df['BsmtCond'] = df['BsmtCond'].fillna(\"No Basment\")",
            "df['BsmtExposure'] = df['BsmtExposure'].fillna(\"No Basment\")",
            "df['BsmtFinType1'] = df['BsmtFinType1'].fillna(\"No Basment\")",
            "df['BsmtFinType2'] = df['BsmtFinType2'].fillna(\"No Basment\")",
            "df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])",
            "df['FireplaceQu'] = df['FireplaceQu'].fillna(\"No Fireplace\")",
            "df['GarageType'] = df['GarageType'].fillna(\"No Garage\")",
            "df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)",
            "df['GarageFinish'] = df['GarageFinish'].fillna(\"No Garage\")",
            "df['GarageQual'] = df['GarageQual'].fillna(\"No Garage\")",
            "df['GarageCond'] = df['GarageCond'].fillna(\"No Garage\")",
            "df['Fence'] = df['Fence'].fillna(\"No Fence\")",
            "df['MiscFeature'] = df['MiscFeature'].fillna(\"No Miscellaneous Feature\")",
            "def remove_outlier(df, x):",
            "df[f'{x}_zscore'] = zscore(df[f'{x}'])",
            "df.drop(df[(df[f'{x}_zscore'] >= 3) | (df[f'{x}_zscore'] <= -3)].index, inplace=True)",
            "df.drop(columns=[f'{x}_zscore'], inplace=True)",
            "columns = df.select_dtypes('number').columns.tolist()",
            "for column in columns:",
            "remove_outlier(df, column)",
            "df.info()",
            "df.shape",
            "df.describe()",
            "df.hist(figsize=(20, 20), xlabelsize=10, ylabelsize=10,color='#ffc0cb');",
            "plt.figure(figsize=(30,30))",
            "sns.heatmap(df.select_dtypes('number').corr(),vmax=.8, annot=True,square=True,cmap='brg');",
            "MSSubClass = df.groupby('MSSubClass')['SalePrice'].mean()",
            "MSSubClassMap ={20 : '1-STORY 1946 & NEWER ALL STYLES',",
            "30 : '1-STORY 1945 & OLDER',",
            "40 : '1-STORY W/FINISHED ATTIC ALL AGES',",
            "45 : \"1-1/2 STORY - UNFINISHED ALL AGES\",",
            "50 : \"1-1/2 STORY FINISHED ALL AGES\",",
            "60 : \"2-STORY 1946 & NEWER\",",
            "70 : \"2-STORY 1945 & OLDER\",",
            "75 : \"2-1/2 STORY ALL AGES\",",
            "80 : \"SPLIT OR MULTI-LEVEL\",",
            "85 : \"SPLIT FOYER\",",
            "90 : \"DUPLEX - ALL STYLES AND AGES\",",
            "120 : \"1-STORY PUD (Planned Unit Development) - 1946 & NEWER\",",
            "150 : \"1-1/2 STORY PUD - ALL AGES\",",
            "160 : \"2-STORY PUD - 1946 & NEWER\",",
            "180 : \"PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\",",
            "190 : \"2 FAMILY CONVERSION - ALL STYLES AND AGES\"}",
            "",
            "index_list = MSSubClass.index.tolist()",
            "fig =px.bar(y=MSSubClass.values, x=[MSSubClassMap[i] for i in index_list], labels={'x': 'MSSubClass','y':'Value'},color=MSSubClass.values)",
            "fig.update_layout(title='Mean of SalePrice for each MSSubClass',width=1300,",
            "height=1000 )",
            "fig.show()",
            "MSZoningMap={",
            "\"A\" : \"Agriculture\",",
            "\"C (all)\" : \"Commercial\",",
            "\"FV\" : \"Floating Village Residential\",",
            "\"I\" : \"Industrial\",",
            "\"RH\" : \"Residential High Density\",",
            "\"RL\" : \"Residential Low Density\",",
            "\"RP\" : \"Residential Low Density Park\",",
            "\"RM\" : \"Residential Medium Density\"",
            "}",
            "index_list = df['MSZoning'].value_counts().index.tolist()",
            "",
            "px.pie(values=df['MSZoning'].value_counts().values,names=[MSZoningMap[i] for i in index_list],title='Number of Houses with the Zonning Classification')",
            "def plot_numeric_column_with_price(ax, x):",
            "scatter = ax.scatter(x=df[x], y=df['SalePrice'], c=df['SalePrice'], cmap='viridis', alpha=0.8, s=10)",
            "ax.set(xlabel=x, ylabel='SalePrice', title=f'{x} With Sale Price')",
            "plt.colorbar(scatter, ax=ax, label='SalePrice')",
            "",
            "columns = []",
            "for c in df.select_dtypes('number').columns.tolist():",
            "if df[f'{c}'].nunique() > 16:",
            "columns.append(c)",
            "",
            "num_rows = math.ceil(len(columns[:-1]) / 2)",
            "fig, axes = plt.subplots(num_rows, 2, figsize=(18, 6 * num_rows))",
            "axes = axes.flatten()",
            "for i, column in enumerate(columns[:-1]):",
            "if i >= len(axes):",
            "break",
            "plot_numeric_column_with_price(axes[i], column)",
            "",
            "plt.tight_layout()",
            "plt.show()",
            "def plot_categorical_column_with_price(x, ax):",
            "mean = df.groupby(x)['SalePrice'].mean()",
            "sns.barplot(x=mean.index, y=mean.values, width=0.5, ax=ax,palette='viridis')",
            "ax.set_xlabel(x)",
            "ax.set_ylabel('SalePrice')",
            "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')",
            "ax.set_title(f'Mean of Sale Price With {x}')",
            "",
            "columns = df.select_dtypes(exclude='number').columns.tolist()",
            "for c in df.select_dtypes('number').columns.tolist():",
            "if df[f'{c}'].nunique() <= 16:",
            "columns.append(c)",
            "",
            "num_columns = 3",
            "num_rows = (len(columns) + num_columns - 1) // num_columns",
            "",
            "fig, axes = plt.subplots(num_rows, num_columns, figsize=(18, 6 * num_rows))",
            "",
            "for i, column in enumerate(columns):",
            "row = i // num_columns",
            "col = i % num_columns",
            "if num_rows > 1:",
            "ax = axes[row, col]",
            "else:",
            "ax = axes[col]",
            "plot_categorical_column_with_price(column, ax)",
            "",
            "for i in range(len(columns), num_rows * num_columns):",
            "row = i // num_columns",
            "col = i % num_columns",
            "if num_rows > 1:",
            "fig.delaxes(axes[row, col])",
            "else:",
            "fig.delaxes(axes[col])",
            "",
            "plt.tight_layout()",
            "plt.show()",
            "def box_plot_categorical_column_with_price(x, ax):",
            "sns.boxplot(x=x, y='SalePrice', data=df, ax=ax, palette='viridis')",
            "ax.set_xlabel(x)",
            "ax.set_ylabel('SalePrice')",
            "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')",
            "ax.set_title(f'Box Plot of Sale Price by {x}')",
            "",
            "",
            "columns = df.select_dtypes(exclude='number').columns.tolist()",
            "for c in df.select_dtypes('number').columns.tolist():",
            "if df[f'{c}'].nunique() <= 16:",
            "columns.append(c)",
            "",
            "num_columns = 3",
            "num_rows = (len(columns) + num_columns - 1) // num_columns",
            "",
            "fig, axes = plt.subplots(num_rows, num_columns, figsize=(18, 6 * num_rows), sharey=True)",
            "axes = axes.flatten()",
            "",
            "for i, column in enumerate(columns):",
            "box_plot_categorical_column_with_price(column, axes[i])",
            "",
            "for i in range(len(columns), num_rows * num_columns):",
            "fig.delaxes(axes[i])",
            "",
            "plt.tight_layout()",
            "plt.show()",
            "df = pd.get_dummies(df,dtype=float,drop_first=True)",
            "df.head()",
            "index_to_exclude = df.columns.get_loc('SalePrice')",
            "X = df.iloc[:, [i for i in range(df.shape[1]) if i != index_to_exclude]].values",
            "y = df.iloc[:,index_to_exclude].values",
            "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=329)",
            "X_train_nstd = X_train.copy()",
            "X_test_nstd = X_test.copy()",
            "y_train_nstd = y_train.copy()",
            "",
            "sc_X = StandardScaler()",
            "X_train[:, :34] = sc_X.fit_transform(X_train[:, :34])",
            "X_test[:, :34] = sc_X.transform(X_test[:, :34])",
            "sc_y = StandardScaler()",
            "y_train = sc_y.fit_transform(y_train.reshape(-1,1)).flatten()",
            "multi_lr = LinearRegression()",
            "multi_lr.fit(X_train_nstd, y_train_nstd)",
            "y_pred = multi_lr.predict(X_test_nstd)",
            "y_train_pred = multi_lr.predict(X_train_nstd)",
            "r2_lr_train = r2_score(y_train_nstd, y_train_pred)",
            "r2_lr_test = r2_score(y_test, y_pred.reshape(-1,1))",
            "print(\"R2 Train Score:\", r2_lr_train)",
            "print(\"R2 Test Score:\", r2_lr_test)",
            "mse_lr_train = mean_squared_error(y_train_nstd, y_train_pred)",
            "mse_lr_test = mean_squared_error(y_test, y_pred.reshape(-1,1))",
            "print(\"Mean Squared Error of Train:\", mse_lr_train)",
            "print(\"Mean Squared Error of Test:\", mse_lr_test)",
            "poly_reg = PolynomialFeatures(degree = 2)",
            "X_train_poly = poly_reg.fit_transform(X_train)",
            "X_test_poly = poly_reg.transform(X_test)",
            "poly_lr = LinearRegression()",
            "poly_lr.fit(X_train_poly, y_train)",
            "y_pred = poly_lr.predict(X_test_poly)",
            "y_train_pred = poly_lr.predict(X_train_poly)",
            "r2_poly_train = r2_score(y_train,y_train_pred)",
            "r2_poly_test = r2_score(y_test,sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_poly_train)",
            "print(\"R2 Test Score:\", r2_poly_test)",
            "mse_poly_train = mean_squared_error(y_train, y_train_pred)",
            "mse_poly_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_poly_train)",
            "print(\"Mean Squared Error of Test:\", mse_poly_test)",
            "dec_tree = DecisionTreeRegressor()",
            "dec_tree.fit(X_train, y_train)",
            "y_pred = dec_tree.predict(X_test)",
            "y_train_pred = dec_tree.predict(X_train)",
            "r2_tree_train = r2_score(y_train, y_train_pred)",
            "r2_tree_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_tree_train)",
            "print(\"R2 Test Score:\", r2_tree_test)",
            "mse_tree_train = mean_squared_error(y_train, y_train_pred)",
            "mse_tree_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_tree_train)",
            "print(\"Mean Squared Error of Test:\", mse_tree_test)",
            "rdm_frst = RandomForestRegressor(n_estimators = 100)",
            "rdm_frst.fit(X_train, y_train)",
            "y_pred = rdm_frst.predict(X_test)",
            "y_train_pred = rdm_frst.predict(X_train)",
            "r2_frst_train = r2_score(y_train, y_train_pred)",
            "r2_frst_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_frst_train)",
            "print(\"R2 Test Score:\", r2_frst_test)",
            "mse_frst_train = mean_squared_error(y_train, y_train_pred)",
            "mse_frst_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_frst_train)",
            "print(\"Mean Squared Error of Test:\", mse_frst_test)",
            "svr = SVR(kernel = 'linear')",
            "svr.fit(X_train, y_train)",
            "y_pred = svr.predict(X_test)",
            "y_train_pred = svr.predict(X_train)",
            "r2_svr_lr_train = r2_score(y_train, y_train_pred)",
            "r2_svr_lr_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_svr_lr_train)",
            "print(\"R2 Test Score:\", r2_svr_lr_test)",
            "mse_svr_lr_train = mean_squared_error(y_train, y_train_pred)",
            "mse_svr_lr_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_svr_lr_train)",
            "print(\"Mean Squared Error of Test:\", mse_svr_lr_test)",
            "svr = SVR(kernel = 'rbf')",
            "svr.fit(X_train, y_train)",
            "y_pred = svr.predict(X_test)",
            "y_train_pred = svr.predict(X_train)",
            "r2_svr_train = r2_score(y_train, y_train_pred)",
            "r2_svr_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_svr_train)",
            "print(\"R2 Test Score:\", r2_svr_test)",
            "mse_svr_train = mean_squared_error(y_train, y_train_pred)",
            "mse_svr_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_svr_train)",
            "print(\"Mean Squared Error of Test:\", mse_svr_test)",
            "ridge = Ridge()",
            "ridge.fit(X_train, y_train)",
            "y_pred = ridge.predict(X_test)",
            "y_train_pred = ridge.predict(X_train)",
            "r2_ridge_train = r2_score(y_train, y_train_pred)",
            "r2_ridge_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_ridge_train)",
            "print(\"R2 Test Score:\", r2_ridge_test)",
            "mse_ridge_train = mean_squared_error(y_train, y_train_pred)",
            "mse_ridge_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_ridge_train)",
            "print(\"Mean Squared Error of Test:\", mse_ridge_test)",
            "lasso = Lasso(alpha=0.001)",
            "lasso.fit(X_train, y_train)",
            "y_pred = lasso.predict(X_test)",
            "y_train_pred = lasso.predict(X_train)",
            "r2_lasso_train = r2_score(y_train, y_train_pred)",
            "r2_lasso_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_lasso_train)",
            "print(\"R2 Test Score:\", r2_lasso_test)",
            "mse_lasso_train = mean_squared_error(y_train, y_train_pred)",
            "mse_lasso_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_lasso_train)",
            "print(\"Mean Squared Error of Test:\", mse_lasso_test)",
            "xgb = XGBRegressor(learning_rate=0.2,gamma=0.2)",
            "xgb.fit(X_train, y_train)",
            "y_pred = xgb.predict(X_test)",
            "y_train_pred = xgb.predict(X_train)",
            "r2_xgb_train = r2_score(y_train, y_train_pred)",
            "r2_xgb_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_xgb_train)",
            "print(\"R2 Test Score:\", r2_xgb_test)",
            "mse_xgb_train = mean_squared_error(y_train, y_train_pred)",
            "mse_xgb_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_xgb_train)",
            "print(\"Mean Squared Error of Test:\", mse_xgb_test)",
            "tf.random.set_seed(329)",
            "np.random.seed(329)",
            "",
            "nn_model = tf.keras.Sequential([",
            "tf.keras.layers.Dense(100),",
            "tf.keras.layers.Dense(100),",
            "tf.keras.layers.Dense(100),",
            "tf.keras.layers.Dense(100),",
            "tf.keras.layers.Dense(100),",
            "tf.keras.layers.Dense(1)",
            "])",
            "",
            "nn_model.compile(loss = tf.keras.losses.mae,",
            "optimizer=tf.keras.optimizers.Adam(lr=0.1),",
            "metrics=['mae','mse'])",
            "",
            "history = nn_model.fit(X_train,y_train,epochs=200,verbose=2)",
            "nn_model.summary()",
            "pd.DataFrame(history.history).plot()",
            "plt.title('Loss Graph')",
            "plt.ylabel('loss')",
            "plt.xlabel('epochs');",
            "y_pred = nn_model.predict(X_test)",
            "y_train_pred = nn_model.predict(X_train)",
            "r2_nn_train = r2_score(y_train, y_train_pred)",
            "r2_nn_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"R2 Train Score:\", r2_nn_train)",
            "print(\"R2 Test Score:\", r2_nn_test)",
            "mse_nn_train = mean_squared_error(y_train, y_train_pred)",
            "mse_nn_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))",
            "print(\"Mean Squared Error of Train:\", mse_nn_train)",
            "print(\"Mean Squared Error of Test:\", mse_nn_test)",
            "models = pd.DataFrame({",
            "'Model': [",
            "'Multiple Linear Regression','Polynomial Regression','Decision Tree',",
            "'Random Forest', 'Support Vector Regression','Linear Support Vector Regression','Ridge','Lasso','XGBoost','Neural Network'",
            "],",
            "'Training R2 Score': [",
            "r2_lr_train,r2_poly_train,r2_tree_train,r2_frst_train,r2_svr_train,r2_svr_lr_train,r2_ridge_train,r2_lasso_train,r2_xgb_train,r2_nn_train",
            "],",
            "'Training Mean Square Error': [",
            "mse_lr_train,mse_poly_train,mse_tree_train,mse_frst_train,mse_svr_train,mse_svr_lr_train,mse_ridge_train,mse_lasso_train,mse_xgb_train,mse_nn_train",
            "],",
            "'Testing R2 Score': [",
            "r2_lr_test,r2_poly_test,r2_tree_test,r2_frst_test,r2_svr_test,r2_svr_lr_test,r2_ridge_test,r2_lasso_test,r2_xgb_test,r2_nn_test",
            "],",
            "'Testing Mean Square Error': [",
            "mse_lr_test,mse_poly_test,mse_tree_test,mse_frst_test,mse_svr_test,mse_svr_lr_test,mse_ridge_test,mse_lasso_test,mse_xgb_test,mse_nn_test",
            "]",
            "})",
            "models.sort_values(by='Testing R2 Score', ascending=False).style.background_gradient(",
            "cmap='Blues')"
        ]
    },
    "spaceship-titanic.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__6_adhamad0__spaceship-titanic/spaceship-titanic.ipynb",
        "code": [
            "",
            "pip install lazypredict",
            "import pandas as pd",
            "import numpy as np",
            "import seaborn as sb",
            "import matplotlib.pyplot as plt",
            "from sklearn.compose import ColumnTransformer",
            "from sklearn.pipeline import Pipeline",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder , FunctionTransformer",
            "from sklearn.impute import SimpleImputer",
            "from xgboost import XGBClassifier",
            "import xgboost as xgb",
            "from sklearn.model_selection import GridSearchCV, train_test_split",
            "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier",
            "from catboost import CatBoostClassifier",
            "from sklearn.metrics import accuracy_score",
            "from sklearn.linear_model import LogisticRegression , SGDClassifier",
            "from sklearn.svm import SVC",
            "from sklearn.neighbors import KNeighborsClassifier",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.naive_bayes import GaussianNB",
            "from lazypredict.Supervised import LazyClassifier",
            "from lightgbm import LGBMClassifier",
            "import numpy as np",
            "import pandas as pd",
            "import os",
            "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):",
            "for filename in filenames:",
            "print(os.path.join(dirname, filename))",
            "data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')",
            "test_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')",
            "data.head()",
            "data.info()",
            "data.describe()",
            "data.isna().sum()",
            "testing= test_data",
            "data[['deck', 'num' , 'side']] = data['Cabin'].str.split('/', expand=True)[[0 , 1,  2]]",
            "testing[['deck', 'num' , 'side']] = data['Cabin'].str.split('/', expand=True)[[0 , 1,  2]]",
            "data['Group'] = data['PassengerId'].str.split('_' , expand = True)[0].astype(int)",
            "test_data['Group'] = test_data['PassengerId'].str.split('_' , expand = True)[0].astype(int)",
            "",
            "def wow (x):",
            "return x.astype(int)",
            "",
            "fun = FunctionTransformer(wow)",
            "X= data.drop(columns= ['Transported'])",
            "y = data['Transported']",
            "",
            "X_train , X_test , y_train , y_test =  train_test_split(X , y , test_size=0.15, random_state=42)",
            "",
            "",
            "numeric_transformer = Pipeline(steps=[",
            "('imputer', SimpleImputer(strategy='mean')),",
            "('scaler', StandardScaler())])",
            "",
            "categorical_transformer = Pipeline(steps=[",
            "('imputer', SimpleImputer(strategy='most_frequent')),",
            "('onehot', OneHotEncoder(handle_unknown='ignore'))])",
            "",
            "boolean_transformer = Pipeline(steps=[",
            "('imputer', SimpleImputer(strategy='most_frequent')) ,",
            "(\"bool_to_int\" ,fun)",
            "])",
            "",
            "numeric_features = [\"Spa\", \"Group\" , \"FoodCourt\" , \"VRDeck\" , \"RoomService\" , \"Age\" , \"ShoppingMall\" , 'num']",
            "categorical_features = [\"deck\" ,\"side\" , \"HomePlanet\" , \"Destination\"]",
            "boolean_features = [\"CryoSleep\" , \"VIP\"]",
            "preprocessor = ColumnTransformer(",
            "transformers=[",
            "('num', numeric_transformer, numeric_features),",
            "('cat', categorical_transformer, categorical_features),",
            "('bool', boolean_transformer, boolean_features)",
            "])",
            "",
            "",
            "preprocessor.fit(X_train)",
            "",
            "",
            "X_train_preprocessed = preprocessor.transform(X_train)",
            "X_test_preprocessed = preprocessor.transform(X_test)",
            "",
            "",
            "preprocessor.fit(testing)",
            "X_test2_preprocessed = preprocessor.transform(testing)",
            "",
            "ohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)",
            "",
            "",
            "feature_names = list(numeric_features) + list(boolean_features)  + list(ohe_feature_names)",
            "",
            "",
            "X_train = pd.DataFrame(X_train_preprocessed, columns=feature_names)",
            "X_test = pd.DataFrame(X_test_preprocessed, columns=feature_names)",
            "testing = pd.DataFrame(X_test2_preprocessed, columns=feature_names)",
            "",
            "X_train['num'] = X_train['num'].astype(int)",
            "X_test['num'] =X_test['num'].astype(int)",
            "",
            "testing['num']=testing['num'].astype(int)",
            "",
            "",
            "X_train.head()",
            "X_train",
            "",
            "type(X_train['num'][10])",
            "z = X_train",
            "z['Transported'] = y_train.to_list()",
            "X_train = X_train.drop(columns=['Transported'])",
            "z.isna().sum()",
            "hm = z.corr()",
            "plt.figure(figsize=(16 , 9))",
            "sb.heatmap(hm , annot=True , fmt=\".2f\"  )",
            "plt.show()",
            "bst = XGBClassifier(n_estimators=40, max_depth=6, learning_rate=0.1,min_child_weight= 5,subsample = 1,colsample_bytree= 0.75)",
            "bst.fit(X_train , y_train)",
            "y_pred = bst.score(X_test,y_test)",
            "y_pred",
            "models = {",
            "'LogisticRegression': {",
            "'model': LogisticRegression(),",
            "'params': {",
            "'C': [0.1, 1, 10, 100]",
            "}",
            "},",
            "'KNeighborsClassifier': {",
            "'model': KNeighborsClassifier(),",
            "'params': {",
            "'n_neighbors': [3, 5, 7, 9]",
            "}",
            "},",
            "'SVC': {",
            "'model': SVC(),",
            "'params': {",
            "'C': [0.1, 1, 10, 100],",
            "'gamma': [0.1, 0.01, 0.001, 0.0001]",
            "}",
            "},",
            "'XGBClassifier': {",
            "'model': XGBClassifier(),",
            "'params': {",
            "'max_depth': [6, 8, 10],",
            "'learning_rate': [0.01, 0.05, 0.1],",
            "'n_estimators': [100, 200, 300]",
            "}",
            "},",
            "'RandomForestClassifier': {",
            "'model': RandomForestClassifier(),",
            "'params': {",
            "'n_estimators': [100, 200, 300],",
            "'max_depth': [None, 5, 10, 15],",
            "'min_samples_split': [2, 5, 10]",
            "}",
            "},",
            "'GaussianNB': {",
            "'model': GaussianNB(),",
            "'params': {}",
            "},",
            "'GradientBoostingClassifier': {",
            "'model': GradientBoostingClassifier(),",
            "'params': {",
            "'learning_rate': [0.01, 0.1, 1],",
            "'n_estimators': [100, 200, 300],",
            "'max_depth': [3, 5, 8]",
            "}",
            "},",
            "'SGDClassifier': {",
            "'model': SGDClassifier(),",
            "'params': {",
            "'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],",
            "'penalty': ['l2', 'l1', 'elasticnet']",
            "}",
            "},",
            "'DecisionTreeClassifier': {",
            "'model': DecisionTreeClassifier(),",
            "'params': {",
            "'criterion': ['gini', 'entropy'],",
            "'max_depth': [None, 5, 10, 15]",
            "}",
            "},",
            "'LGBMClassifier': {",
            "'model': LGBMClassifier(),",
            "'params': {",
            "'learning_rate': [0.01, 0.1, 1],",
            "'n_estimators': [20, 40, 60, 80, 100],",
            "'num_leaves': [31, 60, 90, 120]",
            "}",
            "}",
            "}",
            "",
            "best_params_scores = {}",
            "",
            "for model_name, model_info in models.items():",
            "",
            "grid_clf = GridSearchCV(model_info['model'], model_info['params'], cv=5)",
            "grid_clf.fit(X_train, y_train)",
            "",
            "",
            "best_params_scores[model_name] = {",
            "'best_params': grid_clf.best_params_,",
            "'best_score': grid_clf.best_score_",
            "}",
            "",
            "for model_name, params_score in best_params_scores.items():",
            "print(f\"Model: {model_name}\")",
            "print(f\"Best parameters: {params_score['best_params']}\")",
            "print(f\"Best score: {params_score['best_score']}\\n\")",
            "lgb = {",
            "'model': LGBMClassifier(),",
            "'params': {",
            "'learning_rate': [0.01, 0.1, 0.05, 1],",
            "'n_estimators': [20, 40, 60, 80, 100],",
            "'num_leaves': [31, 60, 90, 120],",
            "'max_depth': [4, 6],",
            "'colsample_bytree': [0.7, 0.8, 0.9],",
            "'subsample': [0.7, 0.8, 0.9],",
            "'min_child_samples': [1, 5, 10]",
            "}",
            "}",
            "",
            "model = GridSearchCV(lgb['model'], lgb['params'], cv=5 ,",
            "n_jobs=-1,",
            "scoring='neg_root_mean_squared_error')",
            "model.fit(X_train, y_train)",
            "",
            "best_params = model.best_estimator_",
            "print(best_params)",
            "# lgb = LGBMClassifier(colsample_bytree=0.7, learning_rate=0.01, max_depth=4,",
            "#                min_child_samples=1, n_estimators=20, subsample=0.7)",
            "",
            "",
            "# lgbc.fit(X_train  , y_train)",
            "# lgbc.score(X_test , y_test)",
            "",
            "# y_pred = lgbc.predict(testing)",
            "# test_data['Transported'] =y_pred",
            "# final_test = test_data[['PassengerId' , 'Transported']]",
            "# final_test.head()",
            "# final_test.to_csv(\"submission.csv\" , index=False)",
            "clf = LazyClassifier(verbose = 0 , ignore_warnings = True)",
            "models , predictions = clf.fit(X_train, X_test , y_train , y_test)",
            "models",
            "models = {",
            "'lgbm' : LGBMClassifier(colsample_bytree=0.7, learning_rate=0.01, max_depth=4,",
            "min_child_samples=1, n_estimators=20, subsample=0.7),",
            "'random_forest' : RandomForestClassifier(max_depth =15 , min_samples_split=5 , n_estimators=300),",
            "'svc' : SVC(C=10 , gamma=0.1),",
            "'xgb' : XGBClassifier(learning_rate = 0.1 , max_depth=6 , n_estimators= 100)",
            "}",
            "",
            "for name , model in models.items() :",
            "mod= model",
            "mod.fit(X_train , y_train)",
            "score=mod.score(X_test, y_test)",
            "print(score)",
            "# y_pred = mod.predict(testing)",
            "# test_data['Transported'] = y_pred.astype(bool)",
            "# final_test = test_data[['PassengerId' , 'Transported']]",
            "",
            "# final_test.head()",
            "# final_test.to_csv(f\"{name}.csv\" , index=False)"
        ]
    },
    "17-0-9243-house-price-pred-w-xgboost.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__ibkya12__17-0-9243-house-price-pred-w-xgboost/17-0-9243-house-price-pred-w-xgboost.ipynb",
        "code": [
            "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder",
            "from sklearn.impute import KNNImputer",
            "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet",
            "from catboost import CatBoostRegressor",
            "from sklearn.neighbors import KNeighborsRegressor",
            "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor",
            "from sklearn.metrics import mean_squared_error, r2_score",
            "import matplotlib.pyplot as plt",
            "import pandas as pd",
            "import numpy as np",
            "import seaborn as sns",
            "import warnings",
            "from sklearn.exceptions import ConvergenceWarning",
            "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)",
            "def data_read():",
            "train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv') #Read Train Data",
            "test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv') #Read Test Data",
            "subsample = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/sample_submission.csv') #Read Sample Submission Data",
            "return train, test, subsample",
            "",
            "train, test, subsmaple = data_read()",
            "train.head(3) # Let's see train data with head fu",
            "test.head(3) # Let's see test data with head funciton",
            "def sale_plot():",
            "plt.figure(figsize=(18, 6))    # Create figure to analysis graphs.",
            "",
            "sns.histplot(train['SalePrice'], bins=100) # Creating Histogram Plot for see crosses in dataset.",
            "plt.xlabel('Selling price ($)', fontsize=14)",
            "plt.ylabel('Frequency', fontsize=14)",
            "plt.title('Saleprice Rate', fontdict={'fontsize': 11, 'fontweight': 'bold'})",
            "",
            "plt.show();",
            "",
            "print(train['SalePrice'].describe())",
            "",
            "sale_plot()",
            "#E\u011fitim veri seti i\u00e7in yeni \u00f6zellikler olu\u015fturma",
            "train['Total_Top_Bath'] = train['FullBath'] + (train['HalfBath'] * 0.5)",
            "train['TotalPorchSF'] = train['OpenPorchSF'] + train['EnclosedPorch'] + train['3SsnPorch'] + train['ScreenPorch']",
            "train['GarageAge'] = train['YrSold'] - train['GarageYrBlt']",
            "train['HouseAge'] = train['YrSold'] - train['YearBuilt']",
            "train['TotalSF'] = (train['1stFlrSF'] + train['2ndFlrSF'] + train['TotalBsmtSF'] + train['GarageArea'] +",
            "train['WoodDeckSF'] + train['OpenPorchSF'] + train['EnclosedPorch'] +",
            "train['3SsnPorch'] + train['ScreenPorch'])",
            "",
            "train['TotalBath'] = train['FullBath'] + (train['HalfBath'] * 0.5) + train['BsmtFullBath'] + (train['BsmtHalfBath'] * 0.5)",
            "train['Total_Bot_Bath'] = train['BsmtFullBath'] + (train['BsmtHalfBath'] * 0.5)",
            "",
            "train['HasGarage'] = train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)",
            "",
            "",
            "#Test veri seti i\u00e7in yeni \u00f6zellikler olu\u015fturma",
            "test['Total_Top_Bath'] = test['FullBath'] + (test['HalfBath'] * 0.5)",
            "test['Total_Bot_Bath'] = test['BsmtFullBath'] + (test['BsmtHalfBath'] * 0.5)",
            "",
            "test['HasGarage'] = test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)",
            "test['TotalSF'] = (test['1stFlrSF'] + test['2ndFlrSF'] + test['TotalBsmtSF'] + test['GarageArea'] +",
            "test['WoodDeckSF'] + test['OpenPorchSF'] + test['EnclosedPorch'] +",
            "test['3SsnPorch'] + test['ScreenPorch'])",
            "",
            "test['HouseAge'] = test['YrSold'] - test['YearBuilt']",
            "test['GarageAge'] = test['YrSold'] - test['GarageYrBlt']",
            "test['TotalBath'] = test['BsmtFullBath'] + (test['BsmtHalfBath'] * 0.5) + test['FullBath'] + (test['HalfBath'] * 0.5)",
            "test['TotalPorchSF'] = test['OpenPorchSF'] + test['EnclosedPorch'] + test['3SsnPorch'] + test['ScreenPorch']",
            "columns_to_impute_train = []",
            "columns_to_remove_train = []",
            "",
            "columns_to_impute_test = []",
            "columns_to_remove_test = []",
            "",
            "for column in train.columns:",
            "null_count = train[column].isnull().sum()",
            "if null_count >= 500:",
            "columns_to_remove_train.append(column)",
            "elif null_count >= 1:",
            "columns_to_impute_train.append(column)",
            "else:",
            "pass",
            "",
            "for column in test.columns:",
            "null_count = test[column].isnull().sum()",
            "if null_count >= 500:",
            "columns_to_remove_test.append(column)",
            "elif null_count >= 1:",
            "columns_to_impute_test.append(column)",
            "else:",
            "pass",
            "",
            "print(\"Columns to impute in train \", columns_to_impute_train, '\\n')",
            "print(\"Columns to remove in train: \", columns_to_remove_train, '\\n')",
            "",
            "print(\"Columns to impute in test: \", columns_to_impute_test, '\\n')",
            "print(\"Columns to remove in test: \", columns_to_remove_test)",
            "train_clean = train.drop(['Id','PoolQC','PoolArea','Fence', 'MiscFeature'], axis = 1)",
            "test_clean = test.drop(['PoolQC','PoolArea','Fence', 'MiscFeature'], axis = 1)",
            "cat_columns_train = train_clean.select_dtypes(include=['object'])",
            "cat_columns_test = test_clean.select_dtypes(include=['object'])",
            "",
            "print(cat_columns_train.columns)",
            "label_encoder = LabelEncoder()",
            "",
            "for columna in cat_columns_train.columns:",
            "train_clean[columna] = label_encoder.fit_transform(train_clean[columna])",
            "",
            "for columna in cat_columns_test.columns:",
            "test_clean[columna] = label_encoder.fit_transform(test_clean[columna])",
            "",
            "train_clean.info()",
            "knn_imputer_train = KNNImputer(n_neighbors=5, metric='nan_euclidean')",
            "knn_imputer_train.fit(train_clean[columns_to_impute_train])",
            "",
            "train_clean[columns_to_impute_train] = knn_imputer_train.transform(train_clean[columns_to_impute_train])",
            "",
            "",
            "knn_imputer_test = KNNImputer(n_neighbors=5, metric='nan_euclidean')",
            "knn_imputer_test.fit(test_clean[columns_to_impute_test])",
            "",
            "test_clean[columns_to_impute_test] = knn_imputer_test.transform(test_clean[columns_to_impute_test])",
            "",
            "",
            "print(\"No. of nulls in the dataset: \", train_clean.isnull().sum().sum())",
            "correlations = train_clean.corr()",
            "corr = train_clean.corr()",
            "corr_sale = corr['SalePrice'].sort_values(ascending=False)",
            "",
            "plt.figure(figsize=(18, 14))",
            "",
            "plt.barh(corr_sale.index, corr_sale.values)",
            "plt.xlabel(\"Correlation\", size=12)",
            "plt.ylabel(\"\")",
            "plt.title(\"Relationship of the variables with  SalePrice\", fontdict={'fontsize': 11, 'fontweight': 'bold'})",
            "plt.gca().invert_yaxis()",
            "",
            "plt.show()",
            "corr_matrix = train_clean.corr()",
            "",
            "saleprice_corr = corr_matrix['SalePrice']",
            "",
            "threshold = 0.50",
            "high_corr_vars = saleprice_corr[abs(saleprice_corr) > threshold]",
            "",
            "",
            "for var, corr_value in zip(high_corr_vars.index, high_corr_vars.values):",
            "print(f\"{var} and SalePrice Correlation value: {corr_value:.2f}\")",
            "street_count = train['Street'].value_counts()",
            "print(street_count)",
            "fig, axes = plt.subplots(1, 2, figsize=(16, 6))",
            "",
            "sns.histplot(data=train_clean, x='OverallQual', ax=axes[0], bins=range(1, 11), kde=True)",
            "axes[0].set_xlabel('Quality level', size=12)",
            "axes[0].set_ylabel('Frecuency', size=12)",
            "axes[0].set_title('Distribution of qualities', size=11, weight='bold')",
            "",
            "sns.barplot(data=train_clean, x='OverallQual', y='SalePrice', ax=axes[1])",
            "axes[1].set_xlabel('Quality level', size=12)",
            "axes[1].set_ylabel('Selling price', size=12)",
            "axes[1].set_title('Relationship between quality and sales price', size=11, weight='bold')",
            "",
            "plt.tight_layout()",
            "plt.show()",
            "fig, axes = plt.subplots(1, 2, figsize=(16, 6))",
            "",
            "sns.histplot(data=train_clean, x='GrLivArea', ax=axes[0], bins=30, kde=True)",
            "axes[0].set_xlabel('Ft2', size=12)",
            "axes[0].set_ylabel('Frecuency', size=12)",
            "axes[0].set_title('Square footage distribution ', size=11, weight='bold')",
            "",
            "sns.scatterplot(data=train, x='HouseStyle', y='SalePrice', ax=axes[1])",
            "axes[1].set_xlabel('Ft2', size=12)",
            "axes[1].set_ylabel('Selling price', size=12)",
            "axes[1].set_title('Relationship between square footage and sales price', size=11, weight='bold')",
            "",
            "plt.tight_layout()",
            "plt.show()",
            "fig, axes = plt.subplots(1, 3, figsize=(16, 6))",
            "",
            "sns.histplot(data=train_clean, x='GarageCars', ax=axes[0], bins=range(6), kde=True)",
            "axes[0].set_xlabel('Parking spaces', size=12)",
            "axes[0].set_ylabel('Frecuency', size=12)",
            "axes[0].set_title('distribution of the number of parking spaces ', size=11, weight='bold')",
            "",
            "sns.histplot(data=train_clean, x='GarageArea', ax=axes[1], bins=30, kde=True)",
            "axes[1].set_xlabel('Ft2', size=12)",
            "axes[1].set_ylabel('Frecuency', size=12)",
            "axes[1].set_title('Square footage distribution', size=11, weight='bold')",
            "",
            "sns.scatterplot(data=train_clean, x='GarageCars', y='GarageArea', hue='SalePrice', ax=axes[2])",
            "axes[2].set_xlabel('Parking spaces', size=12)",
            "axes[2].set_ylabel('Ft2', size=12)",
            "axes[2].set_title('Relationship between number of seats, size and selling price', size=11, weight='bold')",
            "",
            "plt.tight_layout()",
            "plt.show()",
            "fig, axes = plt.subplots(1, 2, figsize=(16, 6))",
            "",
            "sns.histplot(data=train_clean, x='TotalBsmtSF', ax=axes[0], bins=30, kde=True)",
            "axes[0].set_xlabel('Ft2', size=12)",
            "axes[0].set_ylabel('Frecuency', size=12)",
            "axes[0].set_title('Square footage distribution', size=11, weight='bold')",
            "",
            "sns.scatterplot(data=train_clean, x='TotalBsmtSF', y='SalePrice', ax=axes[1])",
            "axes[1].set_xlabel('Ft2', size=12)",
            "axes[1].set_ylabel('Selling price', size=12)",
            "axes[1].set_title('Selling Price and TotalBstmF', size=11, weight='bold')",
            "",
            "plt.tight_layout()",
            "plt.show()",
            "X_train = train_clean[['OverallQual']]",
            "y_train = train_clean['SalePrice']",
            "X_test = test_clean[['OverallQual']]",
            "",
            "lr_model = LinearRegression()",
            "lr_model.fit(X_train, y_train)",
            "",
            "simple_predictions = lr_model.predict(X_test)",
            "",
            "rmse_simple = np.sqrt(mean_squared_error(y_train, lr_model.predict(X_train)))",
            "r2_simple = r2_score(y_train, lr_model.predict(X_train))",
            "plt.figure(figsize=(19, 6))",
            "plt.scatter(y_train, lr_model.predict(X_train), alpha=0.6)",
            "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)",
            "plt.xlabel('')",
            "plt.ylabel('')",
            "plt.title('Simple Linear Regression')",
            "plt.show()",
            "",
            "",
            "print(f\"Simple Linear Regression RMSE: {rmse_simple}\")",
            "print(f\"Simple Linear Regression  R-squared: {r2_simple}\")",
            "X = train_clean.drop(columns=['SalePrice'])",
            "y = train_clean['SalePrice']",
            "",
            "scaler = StandardScaler()",
            "X_scaled = scaler.fit_transform(X)",
            "",
            "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)",
            "",
            "model = LinearRegression()",
            "model.fit(X_train, y_train)",
            "",
            "y_pred = model.predict(X_test)",
            "",
            "test_clean_aligned = test_clean[X.columns]",
            "test_clean_scaled = scaler.transform(test_clean_aligned)",
            "test_predictions = model.predict(test_clean_scaled)",
            "",
            "rmse_multiple = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred)))",
            "r2_multiple = r2_score(y_test, y_pred)",
            "plt.figure(figsize=(19, 6))",
            "plt.scatter(y_test, y_pred, alpha=0.6)",
            "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)",
            "plt.xlabel('')",
            "plt.ylabel('')",
            "plt.title('Multiple Linear Regression')",
            "plt.show()",
            "",
            "print(f\"Multiple Linear Regression RMSE: {rmse_multiple}\")",
            "print(f\"Multiple Linear Regression R-squared: {r2_multiple}\")",
            "ridge = Ridge()",
            "ridge_params = {'alpha': [0.01, 0.1, 1, 10,100]}",
            "ridge_grid = GridSearchCV(ridge, ridge_params, cv=10, scoring='neg_mean_squared_error')",
            "ridge_grid.fit(X_train, y_train)",
            "best_ridge = ridge_grid.best_estimator_",
            "",
            "y_pred_ridge = best_ridge.predict(X_test)",
            "test_predictions_ridge = best_ridge.predict(test_clean_scaled)",
            "",
            "rmse_ridge = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_ridge)))",
            "r2_ridge = r2_score(y_test, y_pred_ridge)",
            "",
            "print(f\"Ridge Regression RMSE: {rmse_ridge}\")",
            "print(f\"Ridge Regression R-squared: {r2_ridge}\")",
            "knn = KNeighborsRegressor(n_neighbors=5)",
            "knn.fit(X_train, y_train)",
            "",
            "y_pred_knn = knn.predict(X_test)",
            "",
            "knn_rmse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_knn)))",
            "knn_r2 = r2_score(y_test, y_pred_knn)",
            "",
            "print(f\"KNN Regression RMSE: {knn_rmse}\")",
            "print(f\"KNN Regression R-squared: {knn_r2}\")",
            "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)",
            "elastic_net.fit(X_train, y_train)",
            "",
            "",
            "y_pred_en = elastic_net.predict(X_test)",
            "",
            "",
            "en_rmse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_en)))",
            "en_r2 = r2_score(y_test, y_pred_en)",
            "",
            "print(f\"ElasticNet RMSE: {en_rmse}\")",
            "print(f\"ElasticNet R-squared: {en_r2}\")",
            "gb = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, random_state=42)",
            "gb.fit(X_train, y_train)",
            "",
            "",
            "y_pred_gb = gb.predict(X_test)",
            "",
            "",
            "gb_rmse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_gb)))",
            "gb_r2 = r2_score(y_test, y_pred_gb)",
            "",
            "print(f\"Gradient Boosting RMSE: {gb_rmse}\")",
            "print(f\"Gradient Boosting R-squared: {gb_r2}\")",
            "catb = CatBoostRegressor()",
            "catb_model = catb.fit(X_train, y_train,",
            "verbose = 0)",
            "",
            "y_pred_catb = catb_model.predict(X_test)",
            "",
            "catb_rmse_calculator = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_catb)))",
            "catboost_r2_metric = r2_score(y_test, y_pred_catb)",
            "",
            "print(f\"Category Boosting RMSE Metric: {catb_rmse_calculator}\")",
            "print(f\"Category Boosting R-squared Metric: {catboost_r2_metric}\")",
            "from xgboost import XGBRegressor",
            "xgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,",
            "max_depth=3, min_child_weight=0,",
            "gamma=0, subsample=0.7,",
            "colsample_bytree=0.7,",
            "#objective='reg:linear', nthread=-1,",
            "objective='reg:squarederror', nthread=-1,",
            "scale_pos_weight=1, seed=27,",
            "reg_alpha=0.00005)",
            "xgb_model = xgb.fit(X_train , y_train)",
            "",
            "y_pred_xgboost = xgb_model.predict(X_test)",
            "",
            "xgboost_rmse_calculator = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_xgboost)))",
            "xgboost_r2_metric = r2_score(y_test, y_pred_xgboost)",
            "",
            "print(f\"Extreme Gradient Boosting RMSE Metric: {xgboost_rmse_calculator}\")",
            "print(f\"Extreme Gradient Boosting R-squared Metric: {xgboost_r2_metric}\")",
            "test_pred_xgboost = xgb_model.predict(test_clean_scaled)",
            "",
            "submission = pd.DataFrame({",
            "'Id': test['Id'],",
            "'SalePrice': test_pred_xgboost",
            "})",
            "",
            "print(submission.head())",
            "submission.to_csv('submission_xg.csv', index=False)",
            "test_pred_xgb = xgb_model.predict(test_clean_scaled)",
            "",
            "submission = pd.DataFrame({",
            "'Id': test['Id'],",
            "'SalePrice': test_pred_xgb",
            "})",
            "",
            "print(submission.head())",
            "submission.to_csv('submission.csv', index=False)"
        ]
    },
    "house-price-prediction-using-post-lasso.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__hobeomlee__house-price-prediction-using-post-lasso/house-price-prediction-using-post-lasso.ipynb",
        "code": [
            "# This Python 3 environment comes with many helpful analytics libraries installed",
            "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
            "# For example, here's several helpful packages to load",
            "",
            "import numpy as np # linear algebra",
            "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
            "",
            "# Input data files are available in the read-only \"/share/dutta/eyao/dataset/kaggle/\" directory",
            "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
            "",
            "import os",
            "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):",
            "for filename in filenames:",
            "print(os.path.join(dirname, filename))",
            "",
            "# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using \"Save & Run All\"",
            "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "import pandas as pd",
            "import numpy as np",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "from sklearn.linear_model import LassoCV, LinearRegression, Lasso, Ridge, ElasticNet",
            "from sklearn.metrics import mean_squared_error, r2_score",
            "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.svm import SVR",
            "df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv')",
            "df.shape # 1460 Columns, 81 Rows",
            "df.head(5)",
            "df.describe()",
            "df_na = df.isna().sum().to_frame().reset_index()",
            "df_na.columns = [\"Column Name\", \"Null Count\"]",
            "df_na[\"Null Percentage\"] = round((df_na[\"Null Count\"] / len(df))*100, 2)",
            "df_na",
            "df_many_na_columns = df_na[df_na[\"Null Count\"] >= 200][\"Column Name\"].tolist()",
            "df_many_na_columns",
            "df_v1 = df.drop(columns=df_many_na_columns, axis=1)",
            "df_v1.shape # 1460 Rows 74 Columns (81 Columns -> 74 Columns)",
            "df_few_na_columns = df_na[(df_na[\"Null Count\"] < 200) & (df_na[\"Null Count\"] > 0)][\"Column Name\"].tolist()",
            "df_na[df_na[\"Column Name\"].isin(df_few_na_columns)]",
            "df_v2 = df_v1.dropna()",
            "df_v2.shape # 1338 Rows 74 Columns (1460 Rows -> 1338 Rows)",
            "df_v3 = df_v2.drop(\"Id\", axis=1)",
            "df_v3.shape # 1338 Rows 73 Columns (75 Columns -> 73 Columns)",
            "plt.figure(figsize=(10, 6))",
            "sns.boxplot(data=df_v3, x=\"Utilities\", y=\"SalePrice\")",
            "plt.title(\"House Prices by Utilities\")",
            "plt.show()",
            "df_v4 = df_v3.drop(\"Utilities\", axis = 1)",
            "df_v4.shape # 1338 Rows 72 Columns (74 Columns -> 72 Columns)",
            "print(df_v4.sort_values(by=\"SalePrice\", ascending=False)[\"SalePrice\"].head(10))",
            "print(df_v4.sort_values(by=\"SalePrice\", ascending=True)[\"SalePrice\"].head(10))",
            "expensive_top2 = df_v4.sort_values(by=\"SalePrice\", ascending=False).head(2).index",
            "cheap_top2 = df_v4.sort_values(by=\"SalePrice\", ascending=True).head(2).index",
            "drop_index = expensive_top2.union(cheap_top2)",
            "",
            "df_v5 = df_v4.drop(drop_index)",
            "df_v5.shape # 1334 Rows 72 Columns (1338 Rows -> 1334 Rows)",
            "# Numerical Variables",
            "numerical_columns = df_v5.select_dtypes(include=np.number).columns.tolist()",
            "print(numerical_columns)",
            "print(len(numerical_columns)) # 36 Columns",
            "corr_matrix = df_v5[numerical_columns].corr()",
            "# Low Correaltion ( < 0.2)",
            "low_corr = corr_matrix[corr_matrix[\"SalePrice\"].abs() < 0.2].index.tolist()",
            "print(low_corr)",
            "print(len(low_corr)) # 15 Columns",
            "df_v6 = df_v5.drop(columns=low_corr, axis=1)",
            "df_v6.shape # 1334 Rows 57 Columns (73 Columns -> 57 Columns)",
            "# high Correlation ( >= 0.2)",
            "high_corr = corr_matrix[corr_matrix[\"SalePrice\"].abs() >= 0.2].index.tolist()",
            "print(high_corr)",
            "print(len(high_corr)) # 21 Columns",
            "high_correlation = df_v6[high_corr].corr()",
            "",
            "plt.figure(figsize=(12, 9))",
            "sns.heatmap(high_correlation, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)",
            "plt.title(\"High Correlation Heatmap\")",
            "plt.show()",
            "df_v7 = df_v6.drop(columns=[\"TotalBsmtSF\", \"GarageYrBlt\", \"TotRmsAbvGrd\"], axis=1)",
            "df_v7.shape # 1334 Rows 54 Columns (58 columns -> 54 columns)",
            "plt.figure(figsize=(10,6))",
            "sns.histplot(df_v7[\"SalePrice\"], kde=True, bins=50)",
            "plt.title(\"Distribution of SalePrice\")",
            "plt.grid(True)",
            "plt.show()",
            "",
            "# Right-Skewed",
            "df_v7[\"Log_SalePrice\"] = np.log(df_v7[\"SalePrice\"])",
            "df_v7.shape # 1334 Rows 55 Columns (54 Columns -> 55 Columns)",
            "# After Log Transformation",
            "plt.figure(figsize=(10,6))",
            "sns.histplot(df_v7[\"Log_SalePrice\"], kde=True, bins=50)",
            "plt.title(\"Distribution of Log_SalePrice\")",
            "plt.grid(True)",
            "plt.show()",
            "# Categorical Variables",
            "categorical_columns = df_v7.select_dtypes(include=['object']).columns.tolist()",
            "print(categorical_columns)",
            "print(len(categorical_columns)) # 36",
            "df_v8 = pd.get_dummies(df_v7, columns=categorical_columns, drop_first=True)",
            "df_v8.shape # 1334 Rows 206 Columns (56 Columns -> 206 Columns)",
            "# Dataset",
            "X = df_v8.drop(columns=[\"SalePrice\", \"Log_SalePrice\"], axis=1)",
            "y = df_v8[\"Log_SalePrice\"]",
            "",
            "# Split",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
            "",
            "# Standardization",
            "scaler = StandardScaler()",
            "X_train_scaled = scaler.fit_transform(X_train)",
            "X_test_scaled = scaler.fit_transform(X_test)",
            "# LassoCV",
            "alphas = np.logspace(-4, 4, 50)",
            "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000, random_state=42)",
            "lasso_cv.fit(X_train_scaled, y_train)",
            "print(f\"Optimal Alpha Value: {lasso_cv.alpha_}\") # Optimal Alpha Value: 0.004291934260128779",
            "remove_features = np.array(X.columns)[lasso_cv.coef_ == 0]",
            "print(remove_features)",
            "print(len(remove_features)) # 109",
            "df_lasso = df_v8.drop(columns=remove_features, axis=1)",
            "df_lasso.shape # 1334 Rows 97 Columns (206 Columns -> 97 Columns)",
            "# Data",
            "X = df_lasso.drop(columns = [\"SalePrice\", \"Log_SalePrice\"])",
            "y = df_lasso[\"Log_SalePrice\"]",
            "",
            "# Split",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
            "",
            "# Standardization",
            "scaler = StandardScaler()",
            "X_train_scaled = scaler.fit_transform(X_train)",
            "X_test_scaled = scaler.fit_transform(X_test)",
            "# Model",
            "linear_model = LinearRegression()",
            "",
            "# Cross Validation",
            "cv_score = cross_val_score(linear_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')",
            "",
            "# Training",
            "linear_model.fit(X_train_scaled, y_train)",
            "y_pred = linear_model.predict(X_test_scaled)",
            "linear_mse = mean_squared_error(y_test, y_pred)",
            "linear_r2 = r2_score(y_test, y_pred)",
            "print(linear_mse) # 0.013932131659777207",
            "print(linear_r2) # 0.8852401953026643",
            "# parameters",
            "alphas = [0.001, 0.01, 0.1, 1, 10]",
            "best_alpha = None",
            "best_score = float('inf')",
            "",
            "for alpha in alphas:",
            "lasso = Lasso(alpha=alpha)",
            "cv_scores = cross_val_score(lasso, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')",
            "mean_cv_scores = -cv_scores.mean()",
            "",
            "if mean_cv_scores < best_score:",
            "best_score = mean_cv_scores",
            "best_alpha = alpha",
            "print(best_alpha) # 0.001",
            "print(best_score) # 0.019635386910392665",
            "# Best Lasso Model",
            "lasso_best = Lasso(alpha=best_alpha)",
            "lasso_best.fit(X_train_scaled, y_train)",
            "",
            "y_pred = lasso_best.predict(X_test_scaled)",
            "lasso_mse = mean_squared_error(y_test, y_pred)",
            "lasso_r2 = r2_score(y_test, y_pred)",
            "print(lasso_mse) # 0.013429016176088117",
            "print(lasso_r2) # 0.8893843877391352",
            "ridge = Ridge()",
            "parameters = {'alpha': alphas}",
            "",
            "ridge_model = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)",
            "ridge_model.fit(X_train_scaled, y_train)",
            "best_ridge = ridge_model.best_estimator_",
            "y_pred = best_ridge.predict(X_test_scaled)",
            "ridge_mse = mean_squared_error(y_test, y_pred)",
            "ridge_r2 = r2_score(y_test, y_pred)",
            "print(ridge_mse) # 0.013824855747062038",
            "print(ridge_r2) # 0.8861238334344703",
            "l1_ratios = [0.2, 0.5, 0.8]",
            "elastic_net = ElasticNet()",
            "parameters = {'alpha':alphas, 'l1_ratio': l1_ratios}",
            "",
            "elastic_net_model = GridSearchCV(elastic_net, parameters, scoring='neg_mean_squared_error', cv=5)",
            "elastic_net_model.fit(X_train_scaled, y_train)",
            "best_elastic_net = elastic_net_model.best_estimator_",
            "y_pred = best_elastic_net.predict(X_test_scaled)",
            "elastic_net_mse = mean_squared_error(y_test, y_pred)",
            "elastic_net_r2 = r2_score(y_test, y_pred)",
            "print(elastic_net_mse) # 0.013513780328747907",
            "print(elastic_net_r2) # 0.888686180326076",
            "svr = SVR()",
            "parameters = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}",
            "svr_model = GridSearchCV(svr, parameters, scoring='neg_mean_squared_error', cv=5)",
            "svr_model.fit(X_train_scaled, y_train)",
            "best_svr = svr_model.best_estimator_",
            "y_pred = best_svr.predict(X_test_scaled)",
            "svr_mse = mean_squared_error(y_test, y_pred)",
            "svr_r2 = r2_score(y_test, y_pred)",
            "print(svr_mse) # 0.012982683368125891",
            "print(svr_r2) # 0.8930608578675101",
            "final_results = pd.DataFrame({\"Model\": [\"Linear Regression\", \"Lasso\", \"Ridge\", \"Elastic Net\", \"SVM\"],",
            "\"MSE\": [linear_mse, lasso_mse, ridge_mse, elastic_net_mse, svr_mse],",
            "\"R-Squared\": [linear_r2, lasso_r2, ridge_r2, elastic_net_r2, svr_r2]})",
            "final_results",
            "# Visualization",
            "plt.figure(figsize=(12, 6))",
            "",
            "plt.subplot(1, 2, 1)",
            "plt.bar(final_results[\"Model\"], final_results[\"MSE\"], color='skyblue')",
            "plt.title(\"MSE Score per Model\")",
            "plt.ylabel(\"MSE\")",
            "plt.xticks(rotation=45)",
            "mse_min, mse_max = final_results[\"MSE\"].min(), final_results[\"MSE\"].max()",
            "plt.ylim(mse_min - 0.0001, mse_max + 0.0001)",
            "",
            "plt.subplot(1, 2, 2)",
            "plt.bar(final_results[\"Model\"], final_results[\"R-Squared\"], color='salmon')",
            "plt.title(\"R-Squared Score per Model\")",
            "plt.ylabel('R-Squared')",
            "plt.xticks(rotation=45)",
            "r2_min, r2_max = final_results[\"R-Squared\"].min(), final_results[\"R-Squared\"].max()",
            "plt.ylim(r2_min - 0.005, r2_max + 0.005)",
            "",
            "plt.tight_layout()",
            "plt.show()"
        ]
    },
    "spaceship-titanic-tensorflow-80.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__1_aadarshvelu__spaceship-titanic-tensorflow-80/spaceship-titanic-tensorflow-80.ipynb",
        "code": [
            "import numpy as np",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import warnings",
            "",
            "plt.style.use(\"dark_background\")",
            "warnings.filterwarnings(\"ignore\")",
            "# ds = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv\")",
            "",
            "ds = pd.read_csv(\"spaceship-titanic-data/train.csv\")",
            "",
            "ds",
            "ts = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv\")",
            "",
            "ss = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/spaceship-titanic/sample_submission.csv\")",
            "",
            "ts = pd.merge(ts, ss, on='PassengerId')",
            "",
            "ts",
            "ds.info()",
            "ds.isnull().sum()",
            "ds.isna().sum()",
            "ds.describe().T",
            "ds.drop(columns=[\"PassengerId\", \"Name\"], axis=1, inplace=True)",
            "ts.drop(columns=[\"PassengerId\", \"Name\"], axis=1, inplace=True)",
            "spe = ds",
            "spe.dropna(inplace=True)",
            "",
            "numCol = spe.select_dtypes('number').columns",
            "",
            "spe",
            "fig, ax = plt.subplots(4, 3, figsize=(20, 15))",
            "",
            "for idx, (col, ax) in enumerate(zip([*numCol, *numCol], ax.flatten())):",
            "if len(numCol) <= idx:",
            "sns.boxplot(y=spe[col], ax=ax)",
            "else:",
            "sns.kdeplot(x=spe[col], ax=ax, fill=True)",
            "from sklearn.preprocessing import LabelEncoder",
            "",
            "for col in ds.select_dtypes(include=[object, bool]):",
            "ds[col] = LabelEncoder().fit_transform(ds[col])",
            "",
            "for col in ts.select_dtypes(include=[object, bool]):",
            "ts[col] = LabelEncoder().fit_transform(ts[col])",
            "",
            "ds",
            "ds.isnull().sum()",
            "for col in ds.columns:",
            "ds[col].fillna = np.median(ds[col])",
            "",
            "for col in ts.columns:",
            "ts[col].fillna = np.median(ts[col])",
            "",
            "ds",
            "ds.Transported.value_counts()",
            "from sklearn.preprocessing import MinMaxScaler",
            "",
            "y = ds.Transported.values",
            "",
            "ds = pd.DataFrame(MinMaxScaler().fit_transform(ds), columns=ds.columns)",
            "",
            "ds.Transported = y",
            "",
            "y = ts.Transported.values",
            "",
            "ts = pd.DataFrame(MinMaxScaler().fit_transform(ts), columns=ts.columns)",
            "",
            "ts.Transported = y",
            "",
            "ds",
            "from sklearn.model_selection import train_test_split",
            "",
            "x_train, x_val, y_train, y_val = train_test_split(ds.iloc[:, :-1], ds.iloc[:, -1], train_size=.8, random_state=10)",
            "",
            "x_train.shape, x_val.shape, y_train.shape, y_val.shape",
            "from tensorflow.keras.models import Sequential",
            "from tensorflow.keras.layers import Dense",
            "from tensorflow.keras.optimizers import Adam",
            "",
            "model = Sequential([",
            "Dense(x_train.shape[1], activation=\"relu\"),",
            "Dense(24, activation=\"relu\"),",
            "Dense(12, activation=\"relu\"),",
            "Dense(10, activation=\"relu\"),",
            "Dense(1, activation=\"sigmoid\")",
            "])",
            "",
            "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(.03), metrics=[\"Accuracy\"])",
            "",
            "history = model.fit(x_train, y_train, epochs=50, validation_batch_size=64, validation_data=(x_val, y_val))",
            "loss = history.history['loss']",
            "acc = history.history['Accuracy']",
            "",
            "fig, ax = plt.subplots(1, 2, figsize=(20, 5))",
            "",
            "ax[0].plot(acc)",
            "ax[0].set_xlabel(\"Accuracy\")",
            "",
            "ax[1].plot(loss)",
            "ax[1].set_xlabel(\"Loss\")",
            "loss = history.history['val_loss']",
            "acc = history.history['val_Accuracy']",
            "",
            "fig, ax = plt.subplots(1, 2, figsize=(20, 5))",
            "",
            "ax[0].plot(acc)",
            "ax[0].set_xlabel(\"Accuracy\")",
            "",
            "ax[1].plot(loss)",
            "ax[1].set_xlabel(\"Loss\")",
            "yPred = model.predict(ts.iloc[:, :-1])",
            "",
            "yPred = pd.Series(np.ravel(yPred) < .5)",
            "",
            "yPred.value_counts()",
            "ss[\"Transported\"] = yPred",
            "",
            "ss",
            "ss.to_csv('submission.csv', index = False)"
        ]
    },
    "spaceship-titanic-with-randomforestclassifier.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__3_abdullahalbunni__spaceship-titanic-with-randomforestclassifier/spaceship-titanic-with-randomforestclassifier.ipynb",
        "code": [
            "import pandas as pd",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.ensemble import RandomForestClassifier",
            "from sklearn.preprocessing import OneHotEncoder, StandardScaler",
            "from sklearn.metrics import accuracy_score",
            "from sklearn.impute import SimpleImputer",
            "train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')",
            "train",
            "test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')",
            "test",
            "imputer = SimpleImputer(strategy='median')",
            "train[['Age']] = imputer.fit_transform(train[['Age']])",
            "train['Age'].fillna(train['Age'].median())",
            "train['HomePlanet'].fillna('Unknown')",
            "train['CryoSleep'].fillna(False)",
            "categorical_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']",
            "train = pd.get_dummies(train, columns=categorical_cols)",
            "train['Total_Billed'] = train[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)",
            "X = train.drop(['PassengerId', 'Name', 'Cabin', 'Transported'], axis=1)",
            "y = train['Transported'].astype(int)",
            "X.fillna(0, inplace=True)",
            "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)",
            "model = RandomForestClassifier(n_estimators=100, random_state=42)",
            "model.fit(X_train, y_train)",
            "y_pred = model.predict(X_val)",
            "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred)}\")",
            "test[['Age']] = imputer.transform(test[['Age']])",
            "test['HomePlanet'].fillna('Unknown')",
            "test['CryoSleep'].fillna(False)",
            "test['VIP'].fillna(False)",
            "test = pd.get_dummies(test, columns=categorical_cols)",
            "test['Total_Billed'] = test[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)",
            "X_test = test.drop(['PassengerId', 'Name', 'Cabin'], axis=1)",
            "X_test = X_test.reindex(columns=X.columns, fill_value=0)",
            "",
            "# Check for any remaining NaN values in X_test and handle them",
            "X_test.fillna(0, inplace=True)",
            "test_preds = model.predict(X_test)",
            "submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Transported': test_preds})",
            "submission.to_csv('submission.csv', index=False)"
        ]
    },
    "house-price-prediction.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__furkansukan__house-price-prediction/house-price-prediction.ipynb",
        "code": [
            "# \u0130\u015f Problemi",
            "",
            "# Her bir eve ait \u00f6zelliklerin ve ev fiyatlar\u0131n\u0131n bulundu\u011fu veriseti kullan\u0131larak,",
            "# farkl\u0131 tipteki evlerin fiyatlar\u0131na ili\u015fkin bir makine \u00f6\u011frenmesi projesi",
            "# ger\u00e7ekle\u015ftirilmek istenmektedir.",
            "",
            "# Business Problem",
            "",
            "# Using a dataset of properties and house prices for each house,",
            "# a machine learning project on the prices of different types of houses",
            "# is intended to be realized.",
            "# Veri Seti Hikayesi",
            "",
            "# Ames, Lowa\u2019daki konut evlerinden olu\u015fan bu veri seti i\u00e7erisinde 79 a\u00e7\u0131klay\u0131c\u0131 de\u011fi\u015fken bulunduruyor. Kaggle \u00fczerinde bir yar\u0131\u015fmas\u0131",
            "# da bulunan projenin veri seti ve yar\u0131\u015fma sayfas\u0131na a\u015fa\u011f\u0131daki linkten ula\u015fabilirsiniz. Veri seti bir kaggle yar\u0131\u015fmas\u0131na ait",
            "# oldu\u011fundan dolay\u0131 train ve test olmak \u00fczere iki farkl\u0131 csv dosyas\u0131 vard\u0131r. Test veri setinde ev fiyatlar\u0131 bo\u015f b\u0131rak\u0131lm\u0131\u015f olup, bu",
            "# de\u011ferleri sizin tahmin etmeniz beklenmektedir",
            "",
            "",
            "# Dataset Story",
            "",
            "# This dataset of residential homes in Ames, Iowa contains 79 explanatory variables. A contest on Kaggle",
            "# You can access the dataset and the competition page of the project from the link below. The dataset belongs to a kaggle competition",
            "# Therefore, there are two different csv files, train and test. In the test dataset, house prices are left blank and this",
            "# you are expected to estimate the values",
            "import numpy as np",
            "import pandas as pd",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import warnings",
            "from catboost import CatBoostRegressor",
            "from lightgbm import LGBMRegressor",
            "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet",
            "from sklearn.neighbors import KNeighborsRegressor",
            "from sklearn.svm import SVR",
            "from sklearn.tree import DecisionTreeRegressor",
            "from xgboost import XGBRegressor",
            "from sklearn.preprocessing import LabelEncoder",
            "from sklearn.metrics import mean_squared_error",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.preprocessing import MinMaxScaler",
            "",
            "warnings.simplefilter(action='ignore', category=FutureWarning)",
            "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)",
            "",
            "",
            "pd.set_option('display.max_columns', None)",
            "pd.set_option('display.max_rows', None)",
            "pd.set_option('display.width', None)",
            "pd.set_option('display.float_format', lambda x: '%.3f' % x)",
            "# Ad\u0131m 1: Train ve Test veri setlerini okutup birle\u015ftiriniz. Birle\u015ftirdi\u011finiz veri \u00fczerinden ilerleyiniz",
            "",
            "test_df = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv\")",
            "train_df = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv\")",
            "",
            "df = pd.concat([train_df, test_df], ignore_index=True)",
            "",
            "# reset_index() kullanarak indeksi s\u0131f\u0131rlay\u0131n",
            "df.reset_index(drop=True, inplace=True)",
            "",
            "df.head()",
            "def check_df(dataframe, head=5):",
            "print(\"##################### Shape #####################\")",
            "print(dataframe.shape)",
            "print(\"##################### Types #####################\")",
            "print(dataframe.dtypes)",
            "print(\"##################### Head #####################\")",
            "print(dataframe.head(head))",
            "print(\"##################### Tail #####################\")",
            "print(dataframe.tail(head))",
            "print(\"##################### NA #####################\")",
            "print(dataframe.isnull().sum())",
            "print(\"##################### Quantiles #####################\")",
            "print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)",
            "",
            "# check_df(df)",
            "# Ad\u0131m 2: Numerik ve kategorik de\u011fi\u015fkenleri yakalay\u0131n\u0131z",
            "def grab_col_names(dataframe, cat_th=10, car_th=20):",
            "\"\"\"",
            "grab_col_names for given dataframe",
            "",
            ":param dataframe:",
            ":param cat_th:",
            ":param car_th:",
            ":return:",
            "\"\"\"",
            "",
            "cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]",
            "",
            "num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and",
            "dataframe[col].dtypes != \"O\"]",
            "",
            "cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and",
            "dataframe[col].dtypes == \"O\"]",
            "",
            "cat_cols = cat_cols + num_but_cat",
            "cat_cols = [col for col in cat_cols if col not in cat_but_car]",
            "",
            "num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]",
            "num_cols = [col for col in num_cols if col not in num_but_cat]",
            "",
            "print(f\"Observations: {dataframe.shape[0]}\")",
            "print(f\"Variables: {dataframe.shape[1]}\")",
            "print(f'cat_cols: {len(cat_cols)}')",
            "print(f'num_cols: {len(num_cols)}')",
            "print(f'cat_but_car: {len(cat_but_car)}')",
            "print(f'num_but_cat: {len(num_but_cat)}')",
            "",
            "# cat_cols + num_cols + cat_but_car = de\u011fi\u015fken say\u0131s\u0131.",
            "# num_but_cat cat_cols'un i\u00e7erisinde zaten.",
            "# dolay\u0131s\u0131yla t\u00fcm \u015fu 3 liste ile t\u00fcm de\u011fi\u015fkenler se\u00e7ilmi\u015f olacakt\u0131r: cat_cols + num_cols + cat_but_car",
            "# num_but_cat sadece raporlama i\u00e7in verilmi\u015ftir.",
            "",
            "return cat_cols, cat_but_car, num_cols",
            "",
            "cat_cols, cat_but_car, num_cols = grab_col_names(df)",
            "# Ad\u0131m 3: Gerekli d\u00fczenlemeleri yap\u0131n\u0131z. (Tip hatas\u0131 olan de\u011fi\u015fkenler gibi)",
            "",
            "cat_cols += [\"Neighborhood\"]",
            "# Ad\u0131m 4: Numerik ve kategorik de\u011fi\u015fkenlerin veri i\u00e7indeki da\u011f\u0131l\u0131m\u0131n\u0131 g\u00f6zlemleyiniz.",
            "",
            "def cat_summary(dataframe, col_name, plot=False):",
            "print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),",
            "\"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))",
            "",
            "if plot:",
            "sns.countplot(x=dataframe[col_name], data=dataframe)",
            "plt.show()",
            "",
            "",
            "for col in cat_cols:",
            "cat_summary(df, col)",
            "def num_summary(dataframe, numerical_col, plot=False):",
            "quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]",
            "print(dataframe[numerical_col].describe(quantiles).T)",
            "",
            "if plot:",
            "dataframe[numerical_col].hist(bins=50)",
            "plt.xlabel(numerical_col)",
            "plt.title(numerical_col)",
            "plt.show()",
            "",
            "print(\"#####################################\")",
            "",
            "",
            "for col in num_cols:",
            "num_summary(df, col, True)",
            "# Ad\u0131m 5: Kategorik de\u011fi\u015fkenler ile hedef de\u011fi\u015fken incelemesini yap\u0131n\u0131z.",
            "",
            "def target_summary_with_cat(dataframe, target, categorical_col):",
            "print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")",
            "",
            "",
            "for col in cat_cols:",
            "target_summary_with_cat(df,\"SalePrice\",col)",
            "# Ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin incelenmesi",
            "df[\"SalePrice\"].hist(bins=100)",
            "plt.show()",
            "# Ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin logaritmas\u0131n\u0131n incelenmesi",
            "np.log1p(df['SalePrice']).hist(bins=50)",
            "plt.show()",
            "corr = df[num_cols].corr()",
            "corr",
            "",
            "# Korelasyonlar\u0131n g\u00f6sterilmesi",
            "sns.set(rc={'figure.figsize': (12, 12)})",
            "sns.heatmap(corr, cmap=\"RdBu\")",
            "plt.show()",
            "# Ad\u0131m 1: Eksik ve ayk\u0131r\u0131 g\u00f6zlemler i\u00e7in gerekli i\u015flemleri yap\u0131n\u0131z.",
            "",
            "",
            "# Ayk\u0131r\u0131 de\u011ferlerin bask\u0131lanmas\u0131",
            "",
            "def outlier_thresholds(dataframe, variable, low_quantile=0.10, up_quantile=0.90):",
            "quantile_one = dataframe[variable].quantile(low_quantile)",
            "quantile_three = dataframe[variable].quantile(up_quantile)",
            "interquantile_range = quantile_three - quantile_one",
            "up_limit = quantile_three + 1.5 * interquantile_range",
            "low_limit = quantile_one - 1.5 * interquantile_range",
            "return low_limit, up_limit",
            "",
            "# Ayk\u0131r\u0131 de\u011fer kontrol\u00fc",
            "def check_outlier(dataframe, col_name):",
            "low_limit, up_limit = outlier_thresholds(dataframe, col_name)",
            "if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):",
            "return True",
            "else:",
            "return False",
            "",
            "",
            "for col in num_cols:",
            "if col != \"SalePrice\":",
            "print(col, check_outlier(df, col))",
            "",
            "",
            "# Ayk\u0131r\u0131 de\u011ferlerin bask\u0131lanmas\u0131",
            "def replace_with_thresholds(dataframe, variable):",
            "low_limit, up_limit = outlier_thresholds(dataframe, variable)",
            "dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit",
            "dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit",
            "",
            "",
            "for col in num_cols:",
            "if col != \"SalePrice\":",
            "replace_with_thresholds(df,col)",
            "def missing_values_table(dataframe, na_name=False):",
            "na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]",
            "",
            "n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)",
            "",
            "ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)",
            "",
            "missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])",
            "",
            "print(missing_df, end=\"\\n\")",
            "",
            "if na_name:",
            "return na_columns",
            "",
            "missing_values_table(df)",
            "",
            "",
            "df[\"Alley\"].value_counts()",
            "df[\"BsmtQual\"].value_counts()",
            "",
            "",
            "# Baz\u0131 de\u011fi\u015fkenlerdeki bo\u015f de\u011ferler evin o \u00f6zelli\u011fe sahip olmad\u0131\u011f\u0131n\u0131 ifade etmektedir",
            "no_cols = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"FireplaceQu\",",
            "\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PoolQC\",\"Fence\",\"MiscFeature\"]",
            "",
            "# Kolonlardaki bo\u015fluklar\u0131n \"No\" ifadesi ile doldurulmas\u0131",
            "for col in no_cols:",
            "df[col].fillna(\"No\",inplace=True)",
            "",
            "missing_values_table(df)",
            "",
            "",
            "",
            "# Bu fonsksiyon eksik de\u011ferlerin median veya mean ile doldurulmas\u0131n\u0131 sa\u011flar",
            "",
            "def quick_missing_imp(data, num_method=\"median\", cat_length=20, target=\"SalePrice\"):",
            "variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]  # Eksik de\u011fere sahip olan de\u011fi\u015fkenler listelenir",
            "",
            "temp_target = data[target]",
            "",
            "print(\"# BEFORE\")",
            "print(data[variables_with_na].isnull().sum(), \"\\n\\n\")  # Uygulama \u00f6ncesi de\u011fi\u015fkenlerin eksik de\u011ferlerinin say\u0131s\u0131",
            "",
            "# de\u011fi\u015fken object ve s\u0131n\u0131f say\u0131s\u0131 cat_lengthe e\u015fit veya alt\u0131ndaysa bo\u015f de\u011ferleri mode ile doldur",
            "data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)",
            "",
            "# num_method mean ise tipi object olmayan de\u011fi\u015fkenlerin bo\u015f de\u011ferleri ortalama ile dolduruluyor",
            "if num_method == \"mean\":",
            "data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)",
            "# num_method median ise tipi object olmayan de\u011fi\u015fkenlerin bo\u015f de\u011ferleri ortalama ile dolduruluyor",
            "elif num_method == \"median\":",
            "data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)",
            "",
            "data[target] = temp_target",
            "",
            "print(\"# AFTER \\n Imputation method is 'MODE' for categorical variables!\")",
            "print(\" Imputation method is '\" + num_method.upper() + \"' for numeric variables! \\n\")",
            "print(data[variables_with_na].isnull().sum(), \"\\n\\n\")",
            "",
            "return data",
            "",
            "",
            "df = quick_missing_imp(df, num_method=\"median\", cat_length=17)",
            "# Ad\u0131m 2: Rare Encoder uygulay\u0131n\u0131z.",
            "",
            "",
            "# Kategorik kolonlar\u0131n da\u011f\u0131l\u0131m\u0131n\u0131n incelenmesi",
            "",
            "def rare_analyser(dataframe, target, cat_cols):",
            "for col in cat_cols:",
            "print(col, \":\", len(dataframe[col].value_counts()))",
            "print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),",
            "\"RATIO\": dataframe[col].value_counts() / len(dataframe),",
            "\"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")",
            "",
            "rare_analyser(df, \"SalePrice\", cat_cols)",
            "# Nadir s\u0131n\u0131flar\u0131n tespit edilmesi",
            "def rare_encoder(dataframe, rare_perc):",
            "temp_df = dataframe.copy()",
            "",
            "rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'",
            "and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]",
            "",
            "for var in rare_columns:",
            "tmp = temp_df[var].value_counts() / len(temp_df)",
            "rare_labels = tmp[tmp < rare_perc].index",
            "temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])",
            "",
            "return temp_df",
            "",
            "",
            "rare_encoder(df,0.01)",
            "",
            "df.head()",
            "# Ad\u0131m 3: Yeni de\u011fi\u015fkenler olu\u015fturunuz.",
            "",
            "df[\"NEW_1st*GrLiv\"] = df[\"1stFlrSF\"] * df[\"GrLivArea\"]",
            "",
            "df[\"NEW_Garage*GrLiv\"] = (df[\"GarageArea\"] * df[\"GrLivArea\"])",
            "",
            "# df[\"TotalQual\"] = df[[\"OverallQual\", \"OverallCond\", \"ExterQual\", \"ExterCond\", \"BsmtCond\", \"BsmtFinType1\",",
            "# \"BsmtFinType2\", \"HeatingQC\", \"KitchenQual\", \"Functional\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"Fence\"]].sum(axis = 1) # 42",
            "",
            "# Total Floor",
            "df[\"NEW_TotalFlrSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"] # 32",
            "",
            "# Total Finished Basement Area",
            "df[\"NEW_TotalBsmtFin\"] = df.BsmtFinSF1 + df.BsmtFinSF2 # 56",
            "",
            "# Porch Area",
            "df[\"NEW_PorchArea\"] = df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch + df[\"3SsnPorch\"] + df.WoodDeckSF # 93",
            "",
            "# Total House Area",
            "df[\"NEW_TotalHouseArea\"] = df.NEW_TotalFlrSF + df.TotalBsmtSF # 156",
            "",
            "df[\"NEW_TotalSqFeet\"] = df.GrLivArea + df.TotalBsmtSF # 35",
            "",
            "",
            "# Lot Ratio",
            "df[\"NEW_LotRatio\"] = df.GrLivArea / df.LotArea # 64",
            "",
            "df[\"NEW_RatioArea\"] = df.NEW_TotalHouseArea / df.LotArea # 57",
            "",
            "df[\"NEW_GarageLotRatio\"] = df.GarageArea / df.LotArea # 69",
            "",
            "# MasVnrArea",
            "df[\"NEW_MasVnrRatio\"] = df.MasVnrArea / df.NEW_TotalHouseArea # 36",
            "",
            "# Dif Area",
            "df[\"NEW_DifArea\"] = (df.LotArea - df[\"1stFlrSF\"] - df.GarageArea - df.NEW_PorchArea - df.WoodDeckSF) # 73",
            "",
            "",
            "df[\"NEW_OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"] # 61",
            "",
            "",
            "df[\"NEW_Restoration\"] = df.YearRemodAdd - df.YearBuilt # 31",
            "",
            "df[\"NEW_HouseAge\"] = df.YrSold - df.YearBuilt # 73",
            "",
            "df[\"NEW_RestorationAge\"] = df.YrSold - df.YearRemodAdd # 40",
            "",
            "df[\"NEW_GarageAge\"] = df.GarageYrBlt - df.YearBuilt # 17",
            "",
            "df[\"NEW_GarageRestorationAge\"] = np.abs(df.GarageYrBlt - df.YearRemodAdd) # 30",
            "",
            "df[\"NEW_GarageSold\"] = df.YrSold - df.GarageYrBlt # 48",
            "",
            "",
            "",
            "drop_list = [\"Street\", \"Alley\", \"LandContour\", \"Utilities\", \"LandSlope\",\"Heating\", \"PoolQC\", \"MiscFeature\",\"Neighborhood\"]",
            "",
            "# drop_list'teki de\u011fi\u015fkenlerin d\u00fc\u015f\u00fcr\u00fclmesi",
            "df.drop(drop_list, axis=1, inplace=True)",
            "",
            "# df[\"NEW_MSZoning_LotShape\"] = df.MSZoning + \"_\" + df[\"LotShape\"]",
            "",
            "# df[\"NEW_BldgType_LotConfig\"] = df[\"BldgType\"] + \"_\" + df[\"LotConfig\"]",
            "",
            "# df[\"NEW_MSZoning_Condition1\"] = df[\"MSZoning\"] + \"_\" + df[\"Condition1\"]",
            "",
            "# df[\"NEW_MSZoning_Condition2\"] = df[\"MSZoning\"] + \"_\" + df[\"Condition2\"]",
            "",
            "# df[\"NEW_MSZoning_BldgType\"] = df[\"MSZoning\"] + \"_\" + df.BldgType",
            "",
            "# df[\"NEW_MSZoning_HouseStyle\"] = df[\"MSZoning\"] + \"_\" + df[\"HouseStyle\"]",
            "",
            "# df[\"NEW_MSZoning_RoofStyle\"] = df[\"MSZoning\"] + \"_\" + df[\"RoofStyle\"]",
            "",
            "# df[\"NEW_MSZoning_RoofMatl\"] = df[\"MSZoning\"] + \"_\" + df[\"RoofMatl\"]",
            "",
            "# df[\"NEW_MasVnrArea/YearBuilt*OverallQual\"] = df[\"MasVnrArea\"] / df[\"YearBuilt\"] * df[\"OverallQual\"]",
            "",
            "# df[\"NEW_OverallQual*OverallCond-YearBuilt\"] = df[\"OverallQual\"] * df[\"OverallCond\"] - df[\"YearBuilt\"]",
            "",
            "# df[\"NEW_LotFrontage+LotArea\"] = df[\"LotFrontage\"] + df[\"LotArea\"]",
            "",
            "# df[\"NEW_TotalBath\"] = df[\"BsmtFullBath\"] + df[\"BsmtHalfBath\"] + df[\"FullBath\"] + df[\"HalfBath\"]",
            "",
            "# df[\"NEW_Age\"] = df[\"YrSold\"] - df[\"YearBuilt\"]",
            "",
            "# df[\"NEW_Pool_Garage\"] = df[\"GarageCars\"] + df[\"PoolArea\"]",
            "",
            "# df[\"NEW_BldgType_HouseStyle\"] = df[\"BldgType\"] + \"_\" + df[\"HouseStyle\"]",
            "",
            "# df[\"NEW_RoofStyle_RoofMatl\"] = df[\"RoofStyle\"] + \"_\" + df[\"RoofMatl\"]",
            "",
            "# df[\"NEW_BedroomAbvGr_KitchenAbvGr\"] = df[\"BedroomAbvGr\"] + df[\"KitchenAbvGr\"]",
            "",
            "# df[\"NEW_GarageType_Finish\"] = df[\"GarageType\"] + \"_\" + df[\"GarageFinish\"]",
            "",
            "# df[\"NEW_GarageCars_GarageArea_GarageYrBlt\"] = df[\"GarageYrBlt\"] / df[\"GarageCars\"] * df[\"GarageArea\"]",
            "",
            "# df[\"NEW_BsmtFinType1_BsmtFinType2\"] = df[\"BsmtFinType1\"] + \"_\" + df[\"BsmtFinType2\"]",
            "",
            "# df[\"NEW_PavedDrive_GarageQual\"] = df[\"GarageQual\"] + \"_\" + df[\"PavedDrive\"]",
            "",
            "# df[\"NEW_RoofStyle_HouseStyle\"] = df[\"HouseStyle\"] + \"_\" + df[\"RoofStyle\"]",
            "",
            "# df[\"NEW_Exterior1st_Exterior2nd\"] = df[\"Exterior1st\"] + \"_\" + df[\"Exterior2nd\"]",
            "",
            "# df[\"NEW_Fence_PavedDrive\"] = df[\"PavedDrive\"] + \"_\" + df[\"Fence\"]",
            "df.head()",
            "# Ad\u0131m 4: Encoding i\u015flemlerini ger\u00e7ekle\u015ftiriniz.",
            "",
            "cat_cols, cat_but_car, num_cols = grab_col_names(df)",
            "def label_encoder(dataframe, binary_col):",
            "labelencoder = LabelEncoder()",
            "dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])",
            "return dataframe",
            "binary_cols = [col for col in df.columns if df[col].dtypes == \"O\" and len(df[col].unique()) == 2]",
            "for col in binary_cols:",
            "label_encoder(df, col)",
            "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):",
            "dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)",
            "return dataframe",
            "",
            "df = one_hot_encoder(df, cat_cols, drop_first=True)",
            "for col in num_cols:",
            "if col != \"SalePrice\":",
            "print(col, check_outlier(df, col))",
            "",
            "for col in num_cols:",
            "if col != [\"SalePrice\", \"Id\"]:",
            "replace_with_thresholds(df,col)",
            "",
            "for col in num_cols:",
            "if col != \"SalePrice\":",
            "print(col, check_outlier(df, col))",
            "\"\"\"def minmax_scaler(dataframe, num_cols):",
            "minmaxscaler = MinMaxScaler()",
            "dataframe[num_cols] = minmaxscaler.fit_transform(dataframe[num_cols].values.reshape(-1, 1))",
            "return dataframe",
            "",
            "num_cols = [col for col in df[num_cols] if col != \"Id\"]",
            "",
            "for col in num_cols:",
            "minmax_scaler(df, col)\"\"\"",
            "# df[\"NEW_MasVnrRatio\"] = df[\"NEW_MasVnrRatio\"].fillna(df[\"NEW_MasVnrRatio\"].median())",
            "# Ad\u0131m 1: Train ve Test verisini ay\u0131r\u0131n\u0131z. (SalePrice de\u011fi\u015fkeni bo\u015f olan de\u011ferler test verisidir.",
            "train_df = df[df['SalePrice'].notnull()]",
            "test_df = df[df['SalePrice'].isnull()]",
            "y = train_df['SalePrice'] # np.log1p(df['SalePrice'])",
            "X = train_df.drop([\"Id\", \"SalePrice\"], axis=1)",
            "# Ad\u0131m 2: Train verisi ile model kurup, model ba\u015far\u0131s\u0131n\u0131 de\u011ferlendiriniz",
            "",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)",
            "",
            "lr = LinearRegression()",
            "",
            "rmse = np.mean(np.sqrt(-cross_val_score(lr, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "rmse",
            "knn = KNeighborsRegressor()",
            "rmse = np.mean(np.sqrt(-cross_val_score(knn, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "rmse",
            "CART = DecisionTreeRegressor()",
            "rmse = np.mean(np.sqrt(-cross_val_score(CART, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "rmse",
            "GBM = GradientBoostingRegressor()",
            "rmse = np.mean(np.sqrt(-cross_val_score(GBM, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "rmse",
            "LightGBM = LGBMRegressor()",
            "rmse = np.mean(np.sqrt(-cross_val_score(LightGBM, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "rmse",
            "\"\"\"",
            "RMSE: 26139.034 (LR)",
            "RMSE: 45084.477 (KNN)",
            "RMSE: 37916.060 (CART)",
            "RMSE: 25101.333 (GBM)",
            "RMSE: 25188.703 (LightGBM)",
            "",
            "\"\"\"",
            "df[\"SalePrice\"].mean()",
            "# 180450.736",
            "df[\"SalePrice\"].std()",
            "# 76826.747",
            "",
            "##################",
            "# BONUS : Log d\u00f6n\u00fc\u015f\u00fcm\u00fc yaparak model kurunuz ve rmse sonu\u00e7lar\u0131n\u0131 g\u00f6zlemleyiniz.",
            "# Not: Log'un tersini (inverse) almay\u0131 unutmay\u0131n\u0131z.",
            "##################",
            "",
            "# Log d\u00f6n\u00fc\u015f\u00fcm\u00fcn\u00fcn ger\u00e7ekle\u015ftirilmesi",
            "",
            "",
            "train_df = df[df[\"SalePrice\"].notnull()]",
            "test_df = df[df[\"SalePrice\"].isnull()]",
            "",
            "y = np.log1p(train_df[\"SalePrice\"])",
            "X = train_df.drop([\"Id\", \"SalePrice\"], axis=1)",
            "",
            "# Verinin e\u011fitim ve tet verisi olarak b\u00f6l\u00fcnmesi",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)",
            "",
            "# lgbm_tuned = LGBMRegressor(**lgbm_gs_best.best_params_).fit(X_train, y_train)",
            "",
            "lgbm = LGBMRegressor().fit(X_train, y_train)",
            "y_pred = lgbm.predict(X_test)",
            "",
            "y_pred",
            "# Yap\u0131lan LOG d\u00f6n\u00fc\u015f\u00fcm\u00fcn\u00fcn tersinin (inverse'nin) al\u0131nmas\u0131",
            "new_y = np.expm1(y_pred)",
            "new_y",
            "new_y_test = np.expm1(y_test)",
            "new_y_test",
            "",
            "np.sqrt(mean_squared_error(new_y_test, new_y))",
            "",
            "# RMSE: 25188.703 (LightGBM)",
            "# RMSE: 24509.170 (LightGBM)",
            "gbm = GradientBoostingRegressor().fit(X_train, y_train)",
            "y_pred = gbm.predict(X_test)",
            "",
            "y_pred",
            "# Yap\u0131lan LOG d\u00f6n\u00fc\u015f\u00fcm\u00fcn\u00fcn tersinin (inverse'nin) al\u0131nmas\u0131",
            "new_y = np.expm1(y_pred)",
            "new_y",
            "new_y_test = np.expm1(y_test)",
            "new_y_test",
            "",
            "np.sqrt(mean_squared_error(new_y_test, new_y))",
            "# RMSE: 25101.333 (GBM)",
            "# RMSE: 24947.151 (GBM)",
            "# hiperparametre optimizasyonlar\u0131n\u0131 ger\u00e7ekle\u015ftiriniz.",
            "",
            "lgbm_model = LGBMRegressor(random_state=46)",
            "",
            "rmse = np.mean(np.sqrt(-cross_val_score(lgbm_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "print(rmse)",
            "",
            "lgbm_params = {\"learning_rate\": [0.01, 0.1],",
            "\"n_estimators\": [500, 1500]",
            "#\"colsample_bytree\": [0.5, 0.7, 1]",
            "}",
            "",
            "lgbm_gs_best = GridSearchCV(lgbm_model,",
            "lgbm_params,",
            "cv=3,",
            "n_jobs=-1,",
            "verbose=True).fit(X_train, y_train)",
            "final_model = lgbm_model.set_params(**lgbm_gs_best.best_params_).fit(X, y)",
            "",
            "rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))",
            "print(rmse)",
            "# De\u011fi\u015fkenlerin \u00f6nem d\u00fczeyini belirten feature_importance fonksiyonunu kullanarak \u00f6zelliklerin s\u0131ralamas\u0131n\u0131 \u00e7izdiriniz.",
            "",
            "# feature importance",
            "def plot_importance(model, features, num=len(X), save=False):",
            "",
            "feature_imp = pd.DataFrame({\"Value\": model.feature_importances_, \"Feature\": features.columns})",
            "plt.figure(figsize=(40, 40))",
            "sns.set(font_scale=1)  # Yaz\u0131 boyutunu d\u00fc\u015f\u00fcrd\u00fck",
            "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:num])",
            "plt.title(\"Features\")",
            "plt.tight_layout()",
            "plt.show()",
            "if save:",
            "plt.savefig(\"importances.png\")",
            "",
            "model = LGBMRegressor()",
            "model.fit(X, y)",
            "",
            "plot_importance(model, X)",
            "test_df['Id'] = test_df['Id'].astype(int)",
            "test_df['Id'].dtype",
            "# test dataframeindeki bo\u015f olan salePrice de\u011fi\u015fkenlerini tahminleyiniz ve",
            "# Kaggle sayfas\u0131na submit etmeye uygun halde bir dataframe olu\u015fturunuz. (Id, SalePrice)",
            "",
            "model = LGBMRegressor()",
            "model.fit(X, y)",
            "predictions = model.predict(test_df.drop([\"Id\", \"SalePrice\"], axis=1))",
            "",
            "dictionary = {\"Id\": test_df['Id'], \"SalePrice\": predictions}",
            "dfSubmission = pd.DataFrame(dictionary)",
            "dfSubmission.to_csv(\"housePricePredictions.csv\", index=False)",
            "test_df.tail()"
        ]
    },
    "spaceship-titanic-classification.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__7_aestroe__spaceship-titanic-classification/spaceship-titanic-classification.ipynb",
        "code": [
            "# This Python 3 environment comes with many helpful analytics libraries installed",
            "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
            "# For example, here's several helpful packages to load",
            "",
            "import numpy as np # linear algebra",
            "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
            "",
            "# Input data files are available in the read-only \"/share/dutta/eyao/dataset/kaggle/\" directory",
            "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
            "",
            "import os",
            "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):",
            "for filename in filenames:",
            "print(os.path.join(dirname, filename))",
            "",
            "# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using \"Save & Run All\"",
            "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "import pandas as pd",
            "import numpy as np",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "",
            "from sklearn.preprocessing import StandardScaler",
            "from sklearn.model_selection import train_test_split, GridSearchCV",
            "",
            "from sklearn.linear_model import LogisticRegression",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.ensemble import RandomForestClassifier",
            "from xgboost import XGBClassifier",
            "import xgboost as xgb",
            "from lightgbm import LGBMClassifier",
            "import lightgbm as lgb",
            "from catboost import CatBoostClassifier",
            "from sklearn.metrics import accuracy_score, log_loss",
            "from sklearn.metrics import confusion_matrix",
            "from sklearn.metrics import classification_report",
            "",
            "from sklearn.preprocessing import LabelEncoder",
            "train_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')",
            "test_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')",
            "sample_submission_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/sample_submission.csv')",
            "train_data.head()",
            "test_data.head()",
            "train_data.info()",
            "train_data.isnull().sum()",
            "train_data.shape",
            "train_data.drop_duplicates(inplace = True)",
            "train_data.shape",
            "train_data.describe()",
            "train_data['Age'].hist(figsize=(5,3), color='g')",
            "train_data['HomePlanet'].value_counts()",
            "sns.countplot(x = 'HomePlanet', hue = 'Transported' ,data = train_data)",
            "train_data['CryoSleep'].value_counts()",
            "sns.countplot(x='CryoSleep', hue='Transported',data=train_data)",
            "train_data['Age'].mode()[0]",
            "train_data['Age'].median()",
            "for col_name in train_data.columns:",
            "if train_data[col_name].dtypes=='object':",
            "train_data[col_name] = train_data[col_name].fillna(train_data[col_name].mode()[0])",
            "else:",
            "train_data[col_name] = train_data[col_name].fillna(train_data[col_name].median())",
            "print(train_data.shape)",
            "for col_name in test_data.columns:",
            "if test_data[col_name].dtypes=='object':",
            "test_data[col_name] = test_data[col_name].fillna(test_data[col_name].mode()[0])",
            "else:",
            "test_data[col_name] = test_data[col_name].fillna(test_data[col_name].median())",
            "print(test_data.shape)",
            "encoder = LabelEncoder()",
            "for col_name in train_data.columns:",
            "if train_data[col_name].dtypes == 'object':",
            "train_data[col_name] = encoder.fit_transform(train_data[col_name])",
            "object_columns = test_data.select_dtypes(include='object').columns.difference(['PassengerId'])",
            "encoder = LabelEncoder()",
            "for col_name in object_columns:",
            "if test_data[col_name].dtype == 'object':",
            "test_data[col_name] = encoder.fit_transform(test_data[col_name])",
            "plt.figure(figsize=(18,12))",
            "sns.heatmap(train_data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")",
            "X_train = train_data.drop(['Transported','PassengerId','Name','ShoppingMall'], axis=1)",
            "y_train = train_data['Transported']",
            "X_test = test_data.drop(['PassengerId','Name','ShoppingMall'], axis=1)",
            "sc = StandardScaler()",
            "X_train = sc.fit_transform(X_train)",
            "X_test = sc.transform(X_test)",
            "model_XGB = XGBClassifier(learning_rate = 0.1 , max_depth = 4, n_estimators = 100)",
            "model_XGB.fit(X_train, y_train)",
            "XGB_pred = model_XGB.predict(X_test)",
            "# param_grid = {",
            "#     'learning_rate': [0.01, 0.1, 0.2],",
            "#     'max_depth': [3, 4, 5, 7, 8, 10],",
            "#     'n_estimators': [50, 100, 150, 200]",
            "# }",
            "# grid_search = GridSearchCV(estimator = model_XGB,param_grid = param_grid, cv = 5, scoring = 'roc_auc' )",
            "# grid_search.fit(X_train, y_train)",
            "# best_params = grid_search.best_params_",
            "# print(\"Best Hyperparameters:\", best_params)",
            "submission = pd.DataFrame({",
            "'PassengerId': test_data.PassengerId,",
            "'Transported': XGB_pred",
            "})",
            "submission['Transported'] = submission['Transported'].astype(bool)",
            "print(submission.head())",
            "submission.to_csv('submission.csv', index = False)"
        ]
    },
    "house-price-prediction-using-randomforest.ipynb": {
        "file_path": "../kaggle_notebooks/notebooks/kaggle__harinuu__house-price-prediction-using-randomforest/house-price-prediction-using-randomforest.ipynb",
        "code": [
            "# This Python 3 environment comes with many helpful analytics libraries installed",
            "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
            "# For example, here's several helpful packages to load",
            "",
            "import numpy as np # linear algebra",
            "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
            "",
            "# Input data files are available in the read-only \"/share/dutta/eyao/dataset/kaggle/\" directory",
            "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
            "",
            "import os",
            "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):",
            "for filename in filenames:",
            "print(os.path.join(dirname, filename))",
            "",
            "# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using \"Save & Run All\"",
            "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
            "import numpy as np",
            "import pandas as pd",
            "import seaborn as sns",
            "from matplotlib import pyplot as plt",
            "train_data = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv\")",
            "test_data = pd.read_csv(\"/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv\")",
            "test_ids = test_data['Id']",
            "print(\"Shape:\", train_data.shape)",
            "print(\"Duplicated data :\", train_data.duplicated().sum())",
            "fig, ax = plt.subplots(figsize=(25,10))",
            "sns.heatmap(data=train_data.isnull(), yticklabels=False, ax=ax)",
            "fig, ax = plt.subplots(figsize=(25,10))",
            "sns.countplot(x=train_data['SaleCondition'])",
            "sns.histplot(x=train_data['SaleType'], kde=True, ax=ax)",
            "sns.violinplot(x=train_data['HouseStyle'], y=train_data['SalePrice'],ax=ax)",
            "sns.scatterplot(x=train_data[\"Foundation\"], y=train_data[\"SalePrice\"], palette='deep', ax=ax)",
            "plt.grid()",
            "train_data['FireplaceQu'].fillna(\"No\", inplace=True)",
            "train_data['BsmtQual'].fillna(\"No\", inplace=True)",
            "train_data['BsmtCond'].fillna(\"No\", inplace=True)",
            "train_data['BsmtFinType1'].fillna(\"No\", inplace=True)",
            "train_data['BsmtFinType2'].fillna(\"No\", inplace=True)",
            "train_data['BsmtFinType2'].fillna(\"None\", inplace=True)",
            "",
            "def fill_all_missing_values(data):",
            "for col in data.columns:",
            "if((data[col].dtype == 'float64') or (data[col].dtype == 'int64')):",
            "data[col].fillna(data[col].mean(), inplace=True)",
            "else:",
            "data[col].fillna(data[col].mode()[0], inplace=True)",
            "",
            "",
            "fill_all_missing_values(train_data)",
            "fill_all_missing_values(test_data)",
            "drop_col = ['Id', 'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'MoSold', 'YrSold', 'MSSubClass',",
            "'GarageType', 'GarageArea', 'GarageYrBlt', 'GarageFinish', 'YearRemodAdd', 'LandSlope',",
            "'BsmtUnfSF', 'BsmtExposure', '2ndFlrSF', 'LowQualFinSF', 'Condition1', 'Condition2', 'Heating',",
            "'Exterior1st', 'Exterior2nd', 'HouseStyle', 'LotShape', 'LandContour', 'LotConfig', 'Functional',",
            "'BsmtFinSF1', 'BsmtFinSF2', 'FireplaceQu', 'WoodDeckSF', 'GarageQual', 'GarageCond', 'OverallCond'",
            "]",
            "",
            "train_data.drop(drop_col, axis=1, inplace=True)",
            "test_data.drop(drop_col, axis=1, inplace=True)",
            "from sklearn.preprocessing import OrdinalEncoder",
            "",
            "ordinal_col = ['GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'ExterQual', 'ExterCond', 'KitchenQual', 'FireplaceQu',",
            "'PavedDrive', 'Functional', 'Electrical', 'Heating', 'BsmtFinType1', 'BsmtFinType2', 'Utilities']",
            "",
            "OE = OrdinalEncoder(categories=[['No', 'Po', 'Fa', 'TA', 'Gd', 'Ex']])",
            "train_data['BsmtQual'] = OE.fit_transform(train_data[['BsmtQual']])",
            "test_data['BsmtQual'] = OE.transform(test_data[['BsmtQual']])",
            "",
            "",
            "OE = OrdinalEncoder(categories=[['No', 'Po', 'Fa', 'TA', 'Gd', 'Ex']])",
            "train_data['BsmtCond'] = OE.fit_transform(train_data[['BsmtCond']])",
            "test_data['BsmtCond'] = OE.transform(test_data[['BsmtCond']])",
            "",
            "",
            "OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])",
            "train_data['ExterQual'] = OE.fit_transform(train_data[['ExterQual']])",
            "test_data['ExterQual'] = OE.transform(test_data[['ExterQual']])",
            "",
            "",
            "OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])",
            "train_data['ExterCond'] = OE.fit_transform(train_data[['ExterCond']])",
            "test_data['ExterCond'] = OE.transform(test_data[['ExterCond']])",
            "",
            "",
            "OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])",
            "train_data['KitchenQual'] = OE.fit_transform(train_data[['KitchenQual']])",
            "test_data['KitchenQual'] = OE.transform(test_data[['KitchenQual']])",
            "OE = OrdinalEncoder(categories=[['N', 'P', 'Y']])",
            "train_data['PavedDrive'] = OE.fit_transform(train_data[['PavedDrive']])",
            "test_data['PavedDrive'] = OE.transform(test_data[['PavedDrive']])",
            "",
            "",
            "OE = OrdinalEncoder(categories=[['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr']])",
            "train_data['Electrical'] = OE.fit_transform(train_data[['Electrical']])",
            "test_data['Electrical'] = OE.transform(test_data[['Electrical']])",
            "",
            "OE = OrdinalEncoder(categories=[['No', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']])",
            "train_data['BsmtFinType1'] = OE.fit_transform(train_data[['BsmtFinType1']])",
            "test_data['BsmtFinType1'] = OE.transform(test_data[['BsmtFinType1']])",
            "OE = OrdinalEncoder(categories=[['No', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']])",
            "train_data['BsmtFinType2'] = OE.fit_transform(train_data[['BsmtFinType2']])",
            "test_data['BsmtFinType2'] = OE.transform(test_data[['BsmtFinType2']])",
            "",
            "",
            "OE = OrdinalEncoder(categories=[['ELO', 'NoSeWa', 'NoSewr', 'AllPub']])",
            "train_data['Utilities'] = OE.fit_transform(train_data[['Utilities']])",
            "test_data['Utilities'] = OE.transform(test_data[['Utilities']])",
            "",
            "OE = OrdinalEncoder(categories=[['C (all)', 'RH', 'RM', 'RL', 'FV']])",
            "train_data['MSZoning'] = OE.fit_transform(train_data[['MSZoning']])",
            "test_data['MSZoning'] = OE.transform(test_data[['MSZoning']])",
            "",
            "OE = OrdinalEncoder(categories=[['Slab', 'BrkTil', 'Stone', 'CBlock', 'Wood', 'PConc']])",
            "train_data['Foundation'] = OE.fit_transform(train_data[['Foundation']])",
            "test_data['Foundation'] = OE.transform(test_data[['Foundation']])",
            "",
            "OE = OrdinalEncoder(categories=[['MeadowV', 'IDOTRR', 'BrDale', 'Edwards', 'BrkSide', 'OldTown', 'NAmes', 'Sawyer', 'Mitchel', 'NPkVill', 'SWISU', 'Blueste', 'SawyerW', 'NWAmes', 'Gilbert', 'Blmngtn', 'ClearCr', 'Crawfor', 'CollgCr', 'Veenker', 'Timber', 'Somerst', 'NoRidge', 'StoneBr', 'NridgHt']])",
            "train_data['Neighborhood'] = OE.fit_transform(train_data[['Neighborhood']])",
            "test_data['Neighborhood'] = OE.transform(test_data[['Neighborhood']])",
            "",
            "OE = OrdinalEncoder(categories=[['None', 'BrkCmn', 'BrkFace', 'Stone']])",
            "train_data['MasVnrType'] = OE.fit_transform(train_data[['MasVnrType']])",
            "test_data['MasVnrType'] = OE.transform(test_data[['MasVnrType']])",
            "OE = OrdinalEncoder(categories=[['AdjLand', 'Abnorml','Alloca', 'Family', 'Normal', 'Partial']])",
            "train_data['SaleCondition'] = OE.fit_transform(train_data[['SaleCondition']])",
            "test_data['SaleCondition'] = OE.transform(test_data[['SaleCondition']])",
            "",
            "OE = OrdinalEncoder(categories=[['Gambrel', 'Gable','Hip', 'Mansard', 'Flat', 'Shed']])",
            "train_data['RoofStyle'] = OE.fit_transform(train_data[['RoofStyle']])",
            "test_data['RoofStyle'] = OE.transform(test_data[['RoofStyle']])",
            "",
            "OE = OrdinalEncoder(categories=[['ClyTile', 'CompShg', 'Roll','Metal', 'Tar&Grv','Membran', 'WdShake', 'WdShngl']])",
            "train_data['RoofMatl'] = OE.fit_transform(train_data[['RoofMatl']])",
            "test_data['RoofMatl'] = OE.transform(test_data[['RoofMatl']])",
            "Level_col = ['Street' ,'BldgType', 'SaleType', 'CentralAir']",
            "",
            "from sklearn.preprocessing import LabelEncoder",
            "encoder = LabelEncoder()",
            "def encode_catagorical_columns(train, test):",
            "for col in Level_col:",
            "train[col] = encoder.fit_transform(train[col])",
            "test[col]  = encoder.transform(test[col])",
            "encode_catagorical_columns(train_data, test_data)",
            "train_data['BsmtRating'] = train_data['BsmtCond'] * train_data['BsmtQual']",
            "train_data['ExterRating'] = train_data['ExterCond'] * train_data['ExterQual']",
            "train_data['BsmtFinTypeRating'] = train_data['BsmtFinType1'] * train_data['BsmtFinType2']",
            "",
            "train_data['BsmtBath'] = train_data['BsmtFullBath'] + train_data['BsmtHalfBath']",
            "train_data['Bath'] = train_data['FullBath'] + train_data['HalfBath']",
            "train_data['PorchArea'] = train_data['OpenPorchSF'] + train_data['EnclosedPorch'] + train_data['3SsnPorch'] + train_data['ScreenPorch']",
            "",
            "test_data['BsmtRating'] = test_data['BsmtCond'] * test_data['BsmtQual']",
            "test_data['ExterRating'] = test_data['ExterCond'] * test_data['ExterQual']",
            "test_data['BsmtFinTypeRating'] = test_data['BsmtFinType1'] * test_data['BsmtFinType2']",
            "",
            "test_data['BsmtBath'] = test_data['BsmtFullBath'] + test_data['BsmtHalfBath']",
            "test_data['Bath'] = test_data['FullBath'] + test_data['HalfBath']",
            "test_data['PorchArea'] = test_data['OpenPorchSF'] + test_data['EnclosedPorch'] + test_data['3SsnPorch'] + test_data['ScreenPorch']",
            "drop_col = ['OverallQual',",
            "'ExterCond', 'ExterQual',",
            "'BsmtCond', 'BsmtQual',",
            "'BsmtFinType1', 'BsmtFinType2',",
            "'HeatingQC',",
            "'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',",
            "'BsmtFullBath', 'BsmtHalfBath',",
            "'FullBath', 'HalfBath',",
            "]",
            "",
            "train_data.drop(drop_col, axis=1, inplace=True)",
            "test_data.drop(drop_col, axis=1, inplace=True)",
            "",
            "print(train_data.shape)",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.preprocessing import PowerTransformer",
            "ft = FunctionTransformer(func=np.log1p)",
            "pt = PowerTransformer()",
            "",
            "train_data['LotArea'] = ft.fit_transform(train_data['LotArea'])",
            "test_data['LotArea'] = ft.transform(test_data['LotArea'])",
            "from sklearn.ensemble import RandomForestRegressor",
            "from sklearn.model_selection import cross_val_score",
            "from sklearn.metrics import mean_squared_error",
            "",
            "",
            "y = train_data['SalePrice']",
            "X = train_data.drop(['SalePrice'], axis=1)",
            "",
            "candidate_max_leaf_nodes = [250]",
            "#model = LinearRegression()",
            "",
            "for node in candidate_max_leaf_nodes:",
            "model = RandomForestRegressor(max_leaf_nodes=node,)",
            "model.fit(X, y)",
            "score = cross_val_score(model, X, y, cv=10)",
            "print(score.mean())",
            "price = model.predict(test_data)",
            "submission = pd.DataFrame({",
            "\"Id\": test_ids,",
            "\"SalePrice\": price",
            "})",
            "",
            "submission.to_csv(\"submission.csv\", index=False)",
            "submission.sample(10)"
        ]
    }
}