{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class JupyterPipelineAnalyzer:\n",
    "    def __init__(self, folder_path, output_file=\"extracted_code.json\"):\n",
    "        self.folder_path = folder_path\n",
    "        self.output_file = output_file\n",
    "        self.notebook_files = self._find_notebooks()\n",
    "        self.notebooks_data = {}  # Dictionary to store extracted code per notebook\n",
    "        self.import_counts = Counter()\n",
    "        self.function_calls = Counter()\n",
    "\n",
    "    def _find_notebooks(self):\n",
    "        \"\"\"Recursively find all Jupyter notebooks in the given folder.\"\"\"\n",
    "        notebooks = []\n",
    "        for root, _, files in os.walk(self.folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".ipynb\"):\n",
    "                    notebooks.append(os.path.join(root, file))\n",
    "        return notebooks\n",
    "\n",
    "    def _extract_code_cells(self, notebook_path):\n",
    "        \"\"\"Extract all code cells from a Jupyter notebook.\"\"\"\n",
    "        try:\n",
    "            with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "                notebook_data = json.load(f)\n",
    "            \n",
    "            return [line.strip() for cell in notebook_data[\"cells\"] if cell[\"cell_type\"] == \"code\" for line in cell[\"source\"]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {notebook_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _analyze_imports_and_functions(self, lines):\n",
    "        \"\"\"Identify imported libraries and function calls within a list of lines.\"\"\"\n",
    "        for line in lines:\n",
    "            import_match = re.match(r'^\\s*(import|from)\\s+([\\w\\.]+)', line)\n",
    "            if import_match:\n",
    "                self.import_counts[import_match.group(2)] += 1\n",
    "\n",
    "            function_matches = re.findall(r'\\b(\\w+)\\s*\\(', line)\n",
    "            for func in function_matches:\n",
    "                self.function_calls[func] += 1\n",
    "\n",
    "    def process_notebooks(self):\n",
    "        \"\"\"Parse all notebooks and extract relevant code information.\"\"\"\n",
    "        for notebook in self.notebook_files:\n",
    "            notebook_name = os.path.basename(notebook)  # Get only the file name\n",
    "            code_lines = self._extract_code_cells(notebook)\n",
    "\n",
    "            if code_lines:\n",
    "                self.notebooks_data[notebook_name] = {\n",
    "                    \"file_path\": notebook,\n",
    "                    \"code\": code_lines\n",
    "                }\n",
    "                self._analyze_imports_and_functions(code_lines)\n",
    "\n",
    "        self._save_code_json()\n",
    "\n",
    "    def _save_code_json(self):\n",
    "        \"\"\"Save all extracted code lines to a JSON file.\"\"\"\n",
    "        with open(self.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.notebooks_data, f, indent=4)\n",
    "        print(f\"âœ… Extracted code from {len(self.notebook_files)} notebooks and saved to {self.output_file}\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary statistics of the extracted code.\"\"\"\n",
    "        print(\"\\nğŸ“Š Summary Statistics:\")\n",
    "        print(f\"ğŸ“‚ Found {len(self.notebook_files)} notebooks.\")\n",
    "        print(f\"ğŸ“œ Extracted code from {len(self.notebooks_data)} notebooks.\")\n",
    "\n",
    "        print(\"\\nğŸ” Top Imported Libraries:\")\n",
    "        for lib, count in self.import_counts.most_common(20):\n",
    "            print(f\"  - {lib}: {count} times\")\n",
    "\n",
    "        print(\"\\nğŸ”§ Most Used Functions:\")\n",
    "        for func, count in self.function_calls.most_common(20):\n",
    "            print(f\"  - {func}(): {count} times\")\n",
    "\n",
    "# # ==================== USAGE ====================\n",
    "# if __name__ == \"__main__\":\n",
    "#     folder_path = \"path/to/kaggle/notebooks\"  # Replace with your actual path\n",
    "#     analyzer = JupyterPipelineAnalyzer(folder_path)\n",
    "#     analyzer.process_notebooks()\n",
    "#     analyzer.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted code from 19 notebooks and saved to extracted_code.json\n",
      "\n",
      "ğŸ“Š Summary Statistics:\n",
      "ğŸ“‚ Found 19 notebooks.\n",
      "ğŸ“œ Extracted code from 16 notebooks.\n",
      "\n",
      "ğŸ” Top Imported Libraries:\n",
      "  - sklearn.preprocessing: 28 times\n",
      "  - pandas: 26 times\n",
      "  - numpy: 24 times\n",
      "  - sklearn.metrics: 23 times\n",
      "  - sklearn.model_selection: 22 times\n",
      "  - sklearn.ensemble: 16 times\n",
      "  - matplotlib.pyplot: 15 times\n",
      "  - seaborn: 15 times\n",
      "  - xgboost: 14 times\n",
      "  - sklearn.linear_model: 14 times\n",
      "  - os: 10 times\n",
      "  - warnings: 8 times\n",
      "  - lightgbm: 7 times\n",
      "  - catboost: 7 times\n",
      "  - sklearn.neighbors: 7 times\n",
      "  - sklearn.tree: 7 times\n",
      "  - sklearn.impute: 6 times\n",
      "  - sklearn.svm: 6 times\n",
      "  - sklearn.compose: 4 times\n",
      "  - tensorflow: 3 times\n",
      "\n",
      "ğŸ”§ Most Used Functions:\n",
      "  - print(): 342 times\n",
      "  - fillna(): 107 times\n",
      "  - fit(): 99 times\n",
      "  - predict(): 94 times\n",
      "  - drop(): 71 times\n",
      "  - show(): 67 times\n",
      "  - len(): 57 times\n",
      "  - sum(): 56 times\n",
      "  - fit_transform(): 56 times\n",
      "  - mean_squared_error(): 52 times\n",
      "  - mean(): 50 times\n",
      "  - head(): 49 times\n",
      "  - isnull(): 47 times\n",
      "  - read_csv(): 45 times\n",
      "  - DataFrame(): 40 times\n",
      "  - astype(): 40 times\n",
      "  - transform(): 39 times\n",
      "  - r2_score(): 34 times\n",
      "  - value_counts(): 28 times\n",
      "  - to_csv(): 25 times\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../kaggle_notebooks/notebooks\"  # Update this with the actual path\n",
    "analyzer = JupyterPipelineAnalyzer(folder_path)\n",
    "analyzer.process_notebooks()\n",
    "analyzer.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š **Summary Statistics**\n",
      "Total Notebooks Analyzed: 16\n",
      "\n",
      "ğŸ”¹ **Overall Code Line Counts Across All Notebooks:**\n",
      "  - Feature Augmentation: 65 lines\n",
      "  - Feature Reduction: 24 lines\n",
      "  - Feature Engineering: 111 lines\n",
      "  - Other: 3906 lines\n",
      "\n",
      "ğŸ“‚ **Per-Notebook Breakdown:**\n",
      "\n",
      "ğŸ“˜ simple-house-prices-prediction-with-explanation.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 1 lines\n",
      "  - Feature Engineering: 4 lines\n",
      "  - Other: 115 lines\n",
      "\n",
      "ğŸ“˜ reduce-complexity-for-house-prices-predictions.ipynb:\n",
      "  - Feature Augmentation: 1 lines\n",
      "  - Feature Reduction: 8 lines\n",
      "  - Feature Engineering: 6 lines\n",
      "  - Other: 436 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic-eda-predictions.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 5 lines\n",
      "  - Other: 630 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic-competition-with-ensemble-models.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 5 lines\n",
      "  - Other: 133 lines\n",
      "\n",
      "ğŸ“˜ house-prices-prediction.ipynb:\n",
      "  - Feature Augmentation: 20 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 5 lines\n",
      "  - Other: 301 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic-ml.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 3 lines\n",
      "  - Feature Engineering: 12 lines\n",
      "  - Other: 52 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic-code.ipynb:\n",
      "  - Feature Augmentation: 1 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 8 lines\n",
      "  - Other: 84 lines\n",
      "\n",
      "ğŸ“˜ house-prices-prediction-detailed-eda-10-model.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 2 lines\n",
      "  - Feature Engineering: 20 lines\n",
      "  - Other: 358 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 2 lines\n",
      "  - Feature Engineering: 1 lines\n",
      "  - Other: 257 lines\n",
      "\n",
      "ğŸ“˜ 17-0-9243-house-price-pred-w-xgboost.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 1 lines\n",
      "  - Feature Engineering: 8 lines\n",
      "  - Other: 345 lines\n",
      "\n",
      "ğŸ“˜ house-price-prediction-using-post-lasso.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 5 lines\n",
      "  - Feature Engineering: 4 lines\n",
      "  - Other: 220 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic-tensorflow-80.ipynb:\n",
      "  - Feature Augmentation: 1 lines\n",
      "  - Feature Reduction: 2 lines\n",
      "  - Feature Engineering: 3 lines\n",
      "  - Other: 115 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic-with-randomforestclassifier.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 8 lines\n",
      "  - Other: 32 lines\n",
      "\n",
      "ğŸ“˜ house-price-prediction.ipynb:\n",
      "  - Feature Augmentation: 42 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 5 lines\n",
      "  - Other: 537 lines\n",
      "\n",
      "ğŸ“˜ spaceship-titanic-classification.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 7 lines\n",
      "  - Other: 97 lines\n",
      "\n",
      "ğŸ“˜ house-price-prediction-using-randomforest.ipynb:\n",
      "  - Feature Augmentation: 0 lines\n",
      "  - Feature Reduction: 0 lines\n",
      "  - Feature Engineering: 10 lines\n",
      "  - Other: 194 lines\n",
      "\n",
      "ğŸ“Š Summary Statistics:\n",
      "ğŸ“‚ Found 19 notebooks.\n",
      "ğŸ“œ Extracted code from 16 notebooks.\n",
      "\n",
      "ğŸ” Top Imported Libraries:\n",
      "  - sklearn.preprocessing: 28 times\n",
      "  - pandas: 26 times\n",
      "  - numpy: 24 times\n",
      "  - sklearn.metrics: 23 times\n",
      "  - sklearn.model_selection: 22 times\n",
      "  - sklearn.ensemble: 16 times\n",
      "  - matplotlib.pyplot: 15 times\n",
      "  - seaborn: 15 times\n",
      "  - xgboost: 14 times\n",
      "  - sklearn.linear_model: 14 times\n",
      "  - os: 10 times\n",
      "  - warnings: 8 times\n",
      "  - lightgbm: 7 times\n",
      "  - catboost: 7 times\n",
      "  - sklearn.neighbors: 7 times\n",
      "  - sklearn.tree: 7 times\n",
      "  - sklearn.impute: 6 times\n",
      "  - sklearn.svm: 6 times\n",
      "  - sklearn.compose: 4 times\n",
      "  - tensorflow: 3 times\n",
      "\n",
      "ğŸ”§ Most Used Functions:\n",
      "  - print(): 342 times\n",
      "  - fillna(): 107 times\n",
      "  - fit(): 99 times\n",
      "  - predict(): 94 times\n",
      "  - drop(): 71 times\n",
      "  - show(): 67 times\n",
      "  - len(): 57 times\n",
      "  - sum(): 56 times\n",
      "  - fit_transform(): 56 times\n",
      "  - mean_squared_error(): 52 times\n",
      "  - mean(): 50 times\n",
      "  - head(): 49 times\n",
      "  - isnull(): 47 times\n",
      "  - read_csv(): 45 times\n",
      "  - DataFrame(): 40 times\n",
      "  - astype(): 40 times\n",
      "  - transform(): 39 times\n",
      "  - r2_score(): 34 times\n",
      "  - value_counts(): 28 times\n",
      "  - to_csv(): 25 times\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the categorized JSON file\n",
    "categorized_file_path = \"categorized_code.json\"  # Update this path if needed\n",
    "with open(categorized_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    categorized_code = json.load(f)\n",
    "\n",
    "# Initialize summary statistics\n",
    "summary_stats = {\n",
    "    \"total_notebooks\": len(categorized_code),\n",
    "    \"overall_counts\": {\n",
    "        \"Feature Augmentation\": 0,\n",
    "        \"Feature Reduction\": 0,\n",
    "        \"Feature Engineering\": 0,\n",
    "        \"Other\": 0\n",
    "    },\n",
    "    \"notebook_counts\": {}\n",
    "}\n",
    "\n",
    "# Process each notebook\n",
    "for notebook, data in categorized_code.items():\n",
    "    category_counts = {\n",
    "        \"Feature Augmentation\": len(data[\"Feature Augmentation\"]),\n",
    "        \"Feature Reduction\": len(data[\"Feature Reduction\"]),\n",
    "        \"Feature Engineering\": len(data[\"Feature Engineering\"]),\n",
    "        \"Other\": len(data[\"Other\"])\n",
    "    }\n",
    "    \n",
    "    # Store per-notebook statistics\n",
    "    summary_stats[\"notebook_counts\"][notebook] = category_counts\n",
    "    \n",
    "    # Aggregate overall statistics\n",
    "    for category, count in category_counts.items():\n",
    "        summary_stats[\"overall_counts\"][category] += count\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nğŸ“Š **Summary Statistics**\")\n",
    "print(f\"Total Notebooks Analyzed: {summary_stats['total_notebooks']}\\n\")\n",
    "\n",
    "print(\"ğŸ”¹ **Overall Code Line Counts Across All Notebooks:**\")\n",
    "for category, count in summary_stats[\"overall_counts\"].items():\n",
    "    print(f\"  - {category}: {count} lines\")\n",
    "\n",
    "print(\"\\nğŸ“‚ **Per-Notebook Breakdown:**\")\n",
    "for notebook, counts in summary_stats[\"notebook_counts\"].items():\n",
    "    print(f\"\\nğŸ“˜ {notebook}:\")\n",
    "    for category, count in counts.items():\n",
    "        print(f\"  - {category}: {count} lines\")\n",
    "\n",
    "\n",
    "analyzer.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
