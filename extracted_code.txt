# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

# Input data files are available in the read-only "/share/dutta/eyao/dataset/kaggle/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv')
test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv')
train.head()
train.info()
train.shape
train.isnull().sum()
test.isnull().sum()
def fill_missing_values(df):
for column in df.columns:
if df[column].dtype == np.float64:
df[column].fillna(df[column].mean(), inplace=True)
elif (df[column] == 'O').any():
df[column].fillna(df[column].mode().iloc[0], inplace=True)
fill_missing_values(train)
train.isnull().sum()
fill_missing_values(test)
# columns_to_drop = ['Id']
# train = train.drop(columns=columns_to_drop)
from sklearn.preprocessing import LabelEncoder

def encode_categorical_columns(df):
label_encoder = LabelEncoder()

for column in df.columns:
if df[column].dtype == 'object':
df[column] = label_encoder.fit_transform(df[column])
elif df[column].dtype == 'float64':
df[column] = df[column].fillna(df[column].mean())

return df
train = encode_categorical_columns(train)
test = encode_categorical_columns(test)
# display_column_data_types(train)
X = train.drop(['SalePrice','Id'], axis=1)
y = train['SalePrice']
X_val = test.drop(['Id'],axis = 1)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)
X_val = sc.fit_transform(X_val)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# def encode_labels(y):
#     label_encoder = LabelEncoder()
#     y_encoded = label_encoder.fit_transform(y)
#     return y_encoded
# y_train_encoded = encode_labels(y_train)
# y_test_encoded = encode_labels(y_test)
from sklearn.metrics import mean_squared_error
from sklearn.metrics import  accuracy_score
# seed = np.random.seed(0)
# from sklearn.ensemble import RandomForestRegressor
# rfmodel = RandomForestRegressor(n_estimators = 1000,random_state = 0,criterion="absolute_error")
# rfmodel.fit(X_train,y_train)
# print("MSE of Random Forest Regressor: ",mean_square_error(y_test,rfmodel.predict(X_test)))
from xgboost import XGBRegressor

xgb_regressor = XGBRegressor(random_state=0,learning_rate=0.5, max_depth=5, n_estimators=400)
xgb_regressor.fit(X_train, y_train)
y_test.head()
print(f"Mean Squared Error: {mean_squared_error(y_test, xgb_regressor.predict(X_test))}")
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error
lgbmodel = LGBMRegressor(random_state=0,max_depth= 8,n_estimators= 5000)
lgbmodel.fit(X_train,y_train)
print("MSE of Light Gradient Boosted Machine Regressor: ",mean_squared_error(y_test,lgbmodel.predict(X_test)))
from catboost import CatBoostRegressor
catmodel = CatBoostRegressor(random_state = 0,loss_function="MAE",verbose=False)
catmodel.fit(X_train,y_train)
print("MSE of Category Boosting Regressor: ",mean_squared_error(y_test,catmodel.predict(X_test)))

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
random_forest = RandomForestRegressor(random_state=0)
distributions = {
'n_estimators': randint(low=10, high=200),
'max_depth': randint(low=1, high=20),
'min_samples_split': randint(low=2, high=20),
'min_samples_leaf': randint(low=1, high=20),
}
rf_search = RandomizedSearchCV(random_forest, distributions, random_state=0)
rf_search.fit(X_train, y_train)
rf_search.best_params_
best_params = rf_search.best_params_
best_rf_model = RandomForestRegressor(random_state=0, **best_params)
best_rf_model.fit(X_train, y_train)
y_pred = best_rf_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
submission = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/sample_submission.csv')
submission.head()
# test_without_id = test.drop('Id', axis=1)
predictions = xgb_regressor.predict(X_val)
# submission['SalePrice'] = predictions

submission = pd.DataFrame({'Id': test.Id ,'SalePrice': predictions})

submission.to_csv('submission.csv', index=False)

print(submission.head())
import warnings
warnings.filterwarnings("ignore")

import tensorflow as tf
import pandas as pd
import numpy as np
import copy
import seaborn as sns
import matplotlib.pyplot as plt


from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
%matplotlib inline
data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv')
print("Table Shape: {}".format(data.shape))
data.head(7) # See 7 information row at the top of dataset
null_class = data.isnull().sum()
print(null_class[null_class != 0])
data.describe()
def preprocess(df, train_data=True):
drop_rows_list = ["MasVnrType", "MasVnrArea", "Electrical"]
if train_data:
# Drop NaN rows
for sample_row in drop_rows_list:
df.drop(df[df[sample_row].isnull()].index, inplace=True)

# Replace NaN with the most common value
most_common_list = ["BsmtQual", "BsmtCond", "BsmtExposure","BsmtFinType1", "BsmtFinType2",
"GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond"]
for sample_row in most_common_list:
df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)

# Replace NaN with Mean
df.LotFrontage.fillna(df.LotFrontage.mean(), inplace=True)

# Remove some unecessary columns
removed_features_list = ["Alley", "PoolQC", "Fence", "MiscFeature", "FireplaceQu", "Id"]
for feature in removed_features_list:
del df[feature]

if not train_data:
# Replace NaN with the  most common value for test set
for sample_row in test_set.isnull().columns:
df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)

return df

def preprocess_no_obj_feature(df, train_data=True):
drop_rows_list = ["MasVnrType", "MasVnrArea", "Electrical"]
if train_data:
# Drop NaN rows
for sample_row in drop_rows_list:
df.drop(df[df[sample_row].isnull()].index, inplace=True)

# Replace NaN with the most common value
most_common_list = ["BsmtQual", "BsmtCond", "BsmtExposure","BsmtFinType1", "BsmtFinType2",
"GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond"]
for sample_row in  data.select_dtypes(include=['object']).columns:
df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)

# Replace NaN with Mean
df.LotFrontage.fillna(df.LotFrontage.mean(), inplace=True)

# Remove some unecessary columns
removed_features_list = ["Alley", "PoolQC", "Fence", "MiscFeature", "FireplaceQu", "Id"]
for feature in removed_features_list:
del df[feature]

# Delete all remaining object features
for feature in df.select_dtypes(include=['object']).columns:
del df[feature]

if not train_data:
# Replace NaN with the  most common value for test set
for sample_row in test_set.isnull().columns:
df[sample_row].fillna(df[sample_row].mode()[0], inplace=True)

return df

# We can choose 1 of 2 preprocess methods.
# data = preprocess(data, train_data=True) # This preprocess for using object features
data = preprocess_no_obj_feature(data, train_data=True) # Do not use object features
data.head(3)
correlation_matrix = pd.DataFrame.corr(data)
correlation_matrix
upper_bound_threshold, lower_bound_threshold = 0.8, -0.3
# Find features with high correlation scores
high_corr_features = np.where(correlation_matrix > upper_bound_threshold)
neg_corr_features = np.where(correlation_matrix < lower_bound_threshold)
high_corr_features = [(correlation_matrix.columns[i], correlation_matrix.columns[j])
for i, j in zip(*high_corr_features) if i != j]
neg_corr_features = [(correlation_matrix.columns[i], correlation_matrix.columns[j])
for i, j in zip(*neg_corr_features) if i != j]
# Convert to a set of unique features
high_corr_features = set(feature for pair in high_corr_features for feature in pair)
neg_corr_features = set(feature for pair in neg_corr_features for feature in pair)
# Remove high correlated features from the dataset
data_without_high_corr = data.drop(columns=high_corr_features)
# data_without_high_corr = data_without_high_corr.drop(columns=neg_corr_features.difference(high_corr_features))
# # SKIP this cell if do not use object features. Uncomment if use object features
# # Visualize the boxplot to find the outlier for all object features
# feats_for_find_outlier = (data_without_high_corr.dtypes[data_without_high_corr.dtypes == object]).keys().values.reshape(-1, 2)
# num_row, num_col = feats_for_find_outlier.shape[0], feats_for_find_outlier.shape[1]
# fig, ax = plt.subplots(num_row, num_col, figsize=(12, 60))
# for row in range(num_row):
#     for col in range(num_col):
#         sns.boxplot(data=data_without_high_corr, x=feats_for_find_outlier[row,col], y='SalePrice', ax = ax[row,col], dodge=False)
# plt.tight_layout()
# plt.show()
# # SKIP this cell if do not use object features. Uncomment if use object features
# col_feats = ["RoofStyle", "BsmtCond", "SaleCondition"]
# categorical_of_col = ["Gable","TA","Abnorml"]

# def find_combination(categorical_of_col, num_items_to_select):
#     combinations_of_cat = list(combinations(categorical_of_col, num_items_to_select))
#     return combinations_of_cat

# def find_outlier_threshold(df_in, target_col, in_col, in_category):
#     price_by_cat = df_in[target_col][df_in[in_col]==in_category]
#     q1 = price_by_cat.quantile(0.25)
#     q3 = price_by_cat.quantile(0.75)
#     iqr = q3-q1
#     f_low  = q1 - 1.5 * iqr
#     f_high = q3 + 1.5 * iqr
#     return f_low, f_high

# def remove_outlier(df_in, target_col, col_feats, categorical_of_col, fence_low, fence_high):
#     remove_ind = []
#     for ind in df_in.index:
#         if df_in[col_feats][ind] == categorical_of_col:
#             if (df_in[target_col][ind] > fence_low) and (df_in[target_col][ind] < fence_high):
#                 remove_ind.append(ind)
#     for ind in remove_ind:
#         df_in = df_in.drop(ind)
#     return df_in

# # Find threshold for each feature
# fence_low, fence_high = [0]*len(categorical_of_col), [0]*len(categorical_of_col)
# for i in range(len(categorical_of_col)):
#     fence_low[i], fence_high[i] = find_outlier_threshold(data_without_high_corr, "SalePrice", col_feats[i], categorical_of_col[i])
# for i in range(len(fence_low)):
#     removed_outlier_data = remove_outlier(data_without_high_corr, "SalePrice", col_feats[i], categorical_of_col[i], fence_low[i], fence_high[i])
# # SKIP this cell if do not use object features.
# # For sure test and train have the same dummies
# test_set = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv')
# ids = test_set.Id
# test_set = preprocess(test_set, train_data=False)
# test_set = test_set.drop(columns=high_corr_features)
# test_set = test_set.drop(columns=neg_corr_features.difference(high_corr_features))
# target = removed_outlier_data.SalePrice
# removed_outlier_data = removed_outlier_data.drop(columns=["SalePrice"])

# # Create dummies and remove object features
# data_type = removed_outlier_data.dtypes
# object_features = data_type[data_type==object]
# non_object_features = data_type[data_type!=object]
# object_data = removed_outlier_data[object_features.keys()]
# # Categorical data
# len_train = len(removed_outlier_data)
# dataset = pd.concat(objs=[removed_outlier_data, test_set], axis=0)
# dataset = pd.get_dummies(dataset)
# test_set = copy.copy(dataset[len_train:])
# source = copy.copy(dataset[:len_train])
# Function to remove outlier without object features
def remove_outlier(df_in, col_name):
q1 = df_in[col_name].quantile(0.25) # Q1
q3 = df_in[col_name].quantile(0.75) # Q3
iqr = q3-q1 # Interquartile range
fence_low  = q1-1.5*iqr
fence_high = q3+1.5*iqr
df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]
return df_out

removed_outlier_data = remove_outlier(data_without_high_corr, 'SalePrice')
target = removed_outlier_data.SalePrice
test_set = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv')
ids = test_set.Id
test_set = preprocess_no_obj_feature(test_set, train_data=False)
test_set = test_set.drop(columns=high_corr_features)
# test_set = test_set.drop(columns=neg_corr_features.difference(high_corr_features))
removed_outlier_data = removed_outlier_data.drop(columns=["SalePrice"])
source = removed_outlier_data
X, y = source, target
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
# Initialization
LR = LinearRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_val)
plt.scatter(y_val,y_pred)
plt.plot([y_val.min(), y_val.max()], [y_pred.min(), y_pred.max()], 'k--', lw=3)
plt.xlabel('y_predicted')
plt.ylabel('y_val')
plt.title('Linear Regression')
plt.show()
print("Train R2 Score: {}".format(LR.score(X_train,y_train)))
print("Test R2 Score: {}".format(LR.score(X_val,y_val)))
y_pred_kFolds = cross_val_predict(LR, X.values, y.values, cv = 5)
plt.scatter(y, y_pred_kFolds)
plt.plot([y_val.min(), y_val.max()], [y_pred_kFolds.min(), y_pred_kFolds.max()], 'k--', lw=3)
plt.xlabel('y_Predicted')
plt.ylabel('y_Test')
plt.title('Linear Regression with K-Folds')
plt.show()
cv_r2_scores = cross_val_score(LR, source, target, scoring='r2')
print("Mean 5-Folds R Squared: {}".format(np.mean(cv_r2_scores)))
pca = PCA(n_components=15)
pca_fit = pca.fit_transform(source)
pca_df = pd.DataFrame(data = pca_fit, columns = ['pca1','pca2','pca3','pca4','pca5',
'pca6','pca7','pca8','pca9','pca10','pca11',
'pca12','pca13','pca14','pca15'])
X_pca_train, X_pca_val, y_train_pca, y_val_pca = train_test_split(pca_df, y, test_size=0.2)
LR_pca = LinearRegression()
LR_pca.fit(X_pca_train, y_train_pca)
y_pred_pca = LR_pca.predict(X_pca_val)
plt.scatter(y_val_pca,y_pred_pca)
plt.plot([y_val_pca.min(), y_val_pca.max()], [y_pred_pca.min(), y_pred_pca.max()], 'k--', lw=3)
plt.xlabel('y_predicted')
plt.ylabel('y_val')
plt.title('Linear Regression with PCA')
plt.show()
print("Train R2 Score: {}".format(LR_pca.score(X_pca_train,y_train_pca)))
print("Test R2 Score: {}".format(LR_pca.score(X_pca_val,y_val_pca)))
# Calculate residuals
residuals = y_val - y_pred
# Create a residual plot
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, residuals, color='blue')
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()
plt.hist(residuals, bins=200)
plt.title('Distribution of Residuals')
plt.ylabel('Residuals')
plt.show()
X, y = source, target
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)
learning_rate = [0.01, 0.02, 0.05, 0.1]
subsample = [0.5, 0.2, 0.1]
n_estimators = [100, 500, 1000, 1500]
max_depth = [None, 3, 6, 9, 12]

param_grid = {'learning_rate':learning_rate,
'subsample':subsample,
'n_estimators':n_estimators,
'max_depth':max_depth}

GBR = GradientBoostingRegressor()
GBR = GridSearchCV(estimator=GBR, param_grid=param_grid, cv=2,n_jobs=-1)
GBR.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", GBR.best_estimator_)
print("\n The best score across ALL searched params:\n", GBR.best_score_)
print("\n The best parameters across ALL searched params:\n", GBR.best_params_)
GBR_best = GradientBoostingRegressor(**GBR.best_params_) # train with best parameter
GBR_best.fit(X_train, y_train)
y_pred = GBR_best.predict(X_val)
print('\n\nR-squared val set: ')
print(GBR_best.score(X_val, y_val))
print('\nMAE val set: ')
print(mean_absolute_error(y_val, y_pred))
print('\nMSE val set: ')
print(mean_squared_error(y_val, y_pred))
learning_rate = [0.01, 0.02, 0.05, 0.1]
loss = ['linear', 'square', 'exponential']
n_estimators = [25, 50, 100, 120]

param_grid = {'learning_rate':learning_rate,
'loss': loss,
'n_estimators': n_estimators}

ABR = AdaBoostRegressor()
ABR = GridSearchCV(estimator=ABR, param_grid=param_grid, cv=2,n_jobs=-1)
ABR.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", ABR.best_estimator_)
print("\n The best score across ALL searched params:\n", ABR.best_score_)
print("\n The best parameters across ALL searched params:\n", ABR.best_params_)
ABR_best = AdaBoostRegressor(**ABR.best_params_) # train with best parameter
ABR_best.fit(X_train, y_train)
y_pred = ABR_best.predict(X_val)
print('\n\nR-squared val set: ')
print(ABR_best.score(X_val, y_val))
print('\nMAE val set: ')
print(mean_absolute_error(y_val, y_pred))
print('\nMSE val set: ')
print(mean_squared_error(y_val, y_pred))
n_estimators = [50, 100, 150, 300]
max_depth = [None, 4, 6, 8, 10]
max_features = ['sqrt', 'log2', None, int, float]
param_grid = {'n_estimators': n_estimators,
'max_depth': max_depth,
'max_features': max_features}

RFR = RandomForestRegressor()
RFR = GridSearchCV(estimator=RFR, param_grid=param_grid, cv=2,n_jobs=-1)
RFR.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", RFR.best_estimator_)
print("\n The best score across ALL searched params:\n", RFR.best_score_)
print("\n The best parameters across ALL searched params:\n", RFR.best_params_)
RFR_best = RandomForestRegressor(**RFR.best_params_) # train with best parameter
RFR_best.fit(X_train, y_train)
y_pred = RFR_best.predict(X_val)
print('\n\nR-squared val set: ')
print(RFR_best.score(X_val, y_val))
print('\nMAE val set: ')
print(mean_absolute_error(y_val, y_pred))
print('\nMSE val set: ')
print(mean_squared_error(y_val, y_pred))
n_neighbors = [3, 5, 7, 10, 15]
weights = ['uniform', 'distance']
algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']
leaf_size = [20, 30, 50, 70]
p = [1, 2, 3]

param_grid = {'n_neighbors':n_neighbors,
'weights':weights,
'algorithm':algorithm,
'leaf_size':leaf_size,
'p': p}

KNR = KNeighborsRegressor()
KNR = GridSearchCV(estimator=KNR, param_grid=param_grid, cv=2,n_jobs=-1)
KNR.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", KNR.best_estimator_)
print("\n The best score across ALL searched params:\n", KNR.best_score_)
print("\n The best parameters across ALL searched params:\n", KNR.best_params_)
KNR_best = KNeighborsRegressor(**KNR.best_params_) # train with best parameter
KNR_best.fit(X_train, y_train)
y_pred = KNR_best.predict(X_val)
print('\n\nR-squared val set: ')
print(KNR_best.score(X_val, y_val))
print('\nMAE val set: ')
print(mean_absolute_error(y_val, y_pred))
print('\nMSE val set: ')
print(mean_squared_error(y_val, y_pred))
n_estimators = [5, 10, 20, 25]
max_features  = [0.1, 0.2, 0.3, 0.5, 1.0]
max_samples = [0.1, 0.2, 0.3, 0.5, 1.0]

param_grid = {'n_estimators': n_estimators,
'max_features': max_features,
'max_samples': max_samples}

BR = BaggingRegressor()
BR = GridSearchCV(estimator=BR, param_grid=param_grid, cv=2,n_jobs=-1)
BR.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", BR.best_estimator_)
print("\n The best score across ALL searched params:\n", BR.best_score_)
print("\n The best parameters across ALL searched params:\n", BR.best_params_)
BR_best = BaggingRegressor(**BR.best_params_) # train with best parameter
BR_best.fit(X_train, y_train)
y_pred = BR_best.predict(X_val)
print('\n\nR-squared val set: ')
print(BR_best.score(X_val, y_val))
print('\nMAE val set: ')
print(mean_absolute_error(y_val, y_pred))
print('\nMSE val set: ')
print(mean_squared_error(y_val, y_pred))
criterion = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']
splitter  = ['best', 'random']
max_depth = [None, 4, 6, 8, 10]

param_grid = {'criterion': criterion,
'splitter': splitter,
'max_depth': max_depth}

DTR = DecisionTreeRegressor()
DTR = GridSearchCV(estimator=DTR, param_grid=param_grid, cv=2,n_jobs=-1)
DTR.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", DTR.best_estimator_)
print("\n The best score across ALL searched params:\n", DTR.best_score_)
print("\n The best parameters across ALL searched params:\n", DTR.best_params_)
DTR_best = DecisionTreeRegressor(**DTR.best_params_) # train with best parameter
DTR_best.fit(X_train, y_train)
y_pred = DTR_best.predict(X_val)
print('\n\nR-squared val set: ')
print(DTR_best.score(X_val, y_val))
print('\nMAE val set: ')
print(mean_absolute_error(y_val, y_pred))
print('\nMSE val set: ')
print(mean_squared_error(y_val, y_pred))
learning_rate = [0.01, 0.03, 0.05, 0.07]
max_depth = [5, 6, 7]
subsample = [0.1, 0.2, 0.5, 0.7]

param_grid = {'learning_rate': learning_rate,
'max_depth': max_depth,
'subsample':subsample}

XGB = XGBRegressor()
XGB = GridSearchCV(estimator=XGB, param_grid=param_grid, cv=2,n_jobs=-1)
XGB.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", XGB.best_estimator_)
print("\n The best score across ALL searched params:\n", XGB.best_score_)
print("\n The best parameters across ALL searched params:\n", XGB.best_params_)
XGB_best = XGBRegressor(**XGB.best_params_) # train with best parameter
XGB_best.fit(X_train, y_train)
y_pred = XGB_best.predict(X_val)
print('\n\nR-squared val set: ')
print(DTR_best.score(X_val, y_val))
print('\nMAE val set: ')
print(mean_absolute_error(y_val, y_pred))
print('\nMSE val set: ')
print(mean_squared_error(y_val, y_pred))
X, y = source, target
X_train, y_train = X, y
X_train.shape
learning_rate = [0.01, 0.02, 0.05, 0.1]
subsample = [0.5, 0.2, 0.1]
n_estimators = [100, 500, 1000, 1500]
max_depth = [None, 3, 6, 9, 12]

param_grid = {'learning_rate':learning_rate,
'subsample':subsample,
'n_estimators':n_estimators,
'max_depth':max_depth}

GBR = GradientBoostingRegressor()
GBR = GridSearchCV(estimator=GBR, param_grid=param_grid, cv=2,n_jobs=-1)
GBR.fit(X_train, y_train)
print("Results from Grid Search")
print("\n The best estimator across ALL searched params:\n", GBR.best_estimator_)
print("\n The best score across ALL searched params:\n", GBR.best_score_)
print("\n The best parameters across ALL searched params:\n", GBR.best_params_)
GBR_best = GradientBoostingRegressor(**GBR.best_params_) # train with best parameter
GBR_best.fit(X_train, y_train)
y_pred = GBR_best.predict(test_set)
output = pd.DataFrame({'Id': ids, 'SalePrice': y_pred.squeeze()})
output.head()
output.to_csv('submission_GBR.csv', index=False)
y_pred_LR = LR.predict(test_set)
output_LR = pd.DataFrame({'Id': ids, 'SalePrice': y_pred.squeeze()})
output_LR.head()
output_LR.to_csv('submission_LR.csv', index=False)
pip install datasist
# EDA Libraries
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import plotly.subplots as sp
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_style("whitegrid")

# Data Preprocessing Libraries
from datasist.structdata import detect_outliers
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, LabelEncoder
from category_encoders import BinaryEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

# Machine Learing (classification models) Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_regression, RFE, SelectFromModel
from imblearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_curve, roc_auc_score
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from sklearn.model_selection import GridSearchCV
# Reading Train csv file
df_train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
df_train.sample(10)
# check the train dataset shape
print("Number of Columns in Train data",df_train.shape[1])
print("---------------------------------------")
print("Number of Rows in Train data",df_train.shape[0])
# Reading Test csv file
df_test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
df_test.sample(10)
# check the test dataset shape
print("Number of Columns in Test data",df_test.shape[1])
print("---------------------------------------")
print("Number of Rows in Test data",df_test.shape[0])
df_train.info()
# Dropping Name columns in the train and test data as they are unique identifier and not useful for predictions.
df_train = df_train.drop('Name', axis=1)
df_test = df_test.drop('Name', axis=1)
# Descriptive analysis for categorical data
df_train.describe(include='O')
# Descriptive analysis for numerical data
df_train.describe().style.background_gradient()
fig = px.pie(df_train, names='Transported',
title='Transported Distribution',
color_discrete_sequence=px.colors.sequential.Mint_r,
template='plotly_dark'
)

fig.update_traces(textposition='inside',textinfo='percent+label')

fig.show()
fig = px.pie(df_train, names='HomePlanet',
title='HomePlanet Distribution',
color_discrete_sequence=px.colors.sequential.RdBu,
template='plotly_dark'
)

fig.update_traces(textposition='inside',textinfo='percent+label')

fig.show()
fig = px.pie(df_train, names='Destination',
title='Destination Distribution',
color_discrete_sequence=px.colors.sequential.Blues_r,
template='plotly_dark'
)
fig.update_traces(textposition='inside',textinfo='percent+label')

fig.show()
fig = px.pie(df_train, names='CryoSleep',
title='CryoSleep Distribution',
color_discrete_sequence=px.colors.sequential.BuGn_r,
template='plotly_dark'
)

fig.update_traces(textposition='inside',textinfo='percent+label')

fig.show()
fig = px.pie(df_train, names='VIP',
title='VIP Distribution',
color_discrete_sequence=px.colors.sequential.Burg_r,
template='plotly_dark'
)

fig.update_traces(textposition='inside',textinfo='percent+label')

fig.show()
fig = px.histogram(df_train, x='Age',  title='Age Distribution',
marginal='box', color_discrete_sequence=['#429ea8'],
template='plotly_dark'
)

# Customizing the layout of the histogram
fig.update_layout(
xaxis=dict(tickmode='linear', dtick=5),  # Adjusting x-axis tick settings
bargap=0.1  # Setting the gap between bars
)

fig.show()
# Create subplot grid with larger size
fig = make_subplots(rows=2, cols=3, subplot_titles=['RoomService Boxplot', 'FoodCourt Boxplot',
'ShoppingMall Boxplot' , 'Spa Boxplot' , 'VRDeck Boxplot'],
vertical_spacing=0.15, horizontal_spacing=0.15)

# Iterate over each feature and add a box plot
features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']

for i, feature in enumerate(features, start=1):
box_plot = px.box(df_train, x=feature,
color_discrete_sequence=['#40db95'], template='plotly_dark')

row_num = (i - 1) // 3 + 1
col_num = (i - 1) % 3 + 1

fig.add_trace(box_plot['data'][0], row=row_num, col=col_num)

# Update layout for larger size and separation
fig.update_layout(showlegend=False, height=600, template='plotly_dark')

# Show the figure
fig.show()
fig = px.histogram(df_train, x='HomePlanet', color='Transported',
title='Distribution of Transported Passengers by Home Planet',
color_discrete_map={0: '#eb3134', 1: '#10c2de'},
barmode='group', template='plotly_dark', text_auto=True
)

fig.show()
fig = px.histogram(df_train, x='Destination', color='Transported',
title='Distribution of Transported Passengers by Destination',
color_discrete_map={0: '#eb3134', 1: '#10c2de'},
barmode='group', template='plotly_dark', text_auto=True
)

fig.show()
fig = px.histogram(df_train, x='Age', color='Transported',  title='Distribution of Transported Passengers by Age',
marginal='box', color_discrete_sequence=['#eb3134', '#10c2de'],
template='plotly_dark'
)

fig.show()
fig = px.violin(df_train, x='CryoSleep', y='Age', color='CryoSleep', box=True, title='CryoSleep vs Age',
color_discrete_sequence=['#eb3134', '#10c2de'], template='plotly_dark')

fig.show()
# Calculate missing values for both train and test data
missing_values_train = df_train.isna().sum()
missing_values_test = df_test.isna().sum()

# Create subplot grid with 1 row and 2 columns
fig = sp.make_subplots(rows=1, cols=2, subplot_titles=['Train Data', 'Test Data'],
vertical_spacing=0.1, horizontal_spacing=0.15)

# Add bar chart for missing values in train data
fig.add_trace(go.Bar(x=missing_values_train.index, y=missing_values_train.values, marker_color='#10c2de', showlegend=False),
row=1, col=1)

# Add bar chart for missing values in test data
fig.add_trace(go.Bar(x=missing_values_test.index, y=missing_values_test.values, marker_color='#10c2de', showlegend=False),
row=1, col=2)

# Update layout
fig.update_layout(title='Missing Values in Train and Test Data', template='plotly_dark', height= 400)

fig.show()
# Numerical Columns
numerical_columns = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']

# Fill missing values in numerical columns with the median
df_train[numerical_columns] = df_train[numerical_columns].fillna(df_train[numerical_columns].median())
df_test[numerical_columns] = df_test[numerical_columns].fillna(df_test[numerical_columns].median())

# Categorical Columns
categorical_columns = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']

# Fill missing values in categorical columns with the mode
df_train[categorical_columns] = df_train[categorical_columns].fillna(df_train[categorical_columns].mode().iloc[0])
df_test[categorical_columns] = df_test[categorical_columns].fillna(df_test[categorical_columns].mode().iloc[0])
total_missing_train = df_train.isna().sum().sum()
total_missing_test = df_test.isna().sum().sum()

print(f'Total missing values in df_train: {total_missing_train}')
print(f'Total missing values in df_test: {total_missing_test}')
# Function to create Age groups based on specified bins and labels
def create_age_groups(age):
# Define bins and corresponding labels for Age groups
age_bins = [0, 12, 18, 25, 30, 50, 80]
age_labels = ['0-12', '13-17', '18-25', '26-30', '31-50', '51+']

# Use pd.cut to categorize Age into groups
age_group = pd.cut(age, bins=age_bins, labels=age_labels, right=False)

# Return the resulting Age groups
return age_group
# Creating a new column 'Passenger_Groups' by extracting the second part of 'PassengerId' (after the underscore) and converting it to integer.
df_train['Passenger_Groups'] = df_train['PassengerId'].apply(lambda x: x.split('_')[1]).astype(int)
df_test['Passenger_Groups'] = df_test['PassengerId'].apply(lambda x: x.split('_')[1]).astype(int)

# Creating three new columns 'Cabin Deck', 'Cabin Num', and 'Cabin Side' by splitting the 'Cabin' column based on '/'.
df_train[['Cabin_Deck', 'Cabin_Num', 'Cabin_Side']] = df_train['Cabin'].str.split('/', expand=True)
df_test[['Cabin_Deck', 'Cabin_Num', 'Cabin_Side']] = df_test['Cabin'].str.split('/', expand=True)

# Converting the 'Cabin Num' column to dtype integer for numerical analysis.
df_train['Cabin_Num'] = df_train['Cabin_Num'].astype('int')
df_test['Cabin_Num'] = df_test['Cabin_Num'].astype('int')

# Creating a new column 'AmenitiesTotal' by summing the values in 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck' columns.
df_train['AmenitiesTotal'] = df_train[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)
df_test['AmenitiesTotal'] = df_test[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)

# Create a new 'Age_Group' column in the train set using the 'create_age_groups' function
df_train['Age_Group'] = create_age_groups(df_train['Age'])
df_test['Age_Group'] = create_age_groups(df_test['Age'])
# Dropping 'PassengerId' and 'Cabin' columns as we have extracted important features from them, and the original columns are no longer needed for analysis or modeling.
df_train = df_train.drop(['PassengerId', 'Cabin'], axis=1)
df_test = df_test.drop('Cabin', axis=1)
df_train.head()
df_test.head()
fig = px.histogram(df_train, x='Passenger_Groups',  title='Passenger Groups Distribution',
marginal='box', color_discrete_sequence=['#1f87d1'],
template='plotly_dark', text_auto=True
)

# Customizing the layout of the histogram
fig.update_layout(
bargap=0.1  # Setting the gap between bars
)

fig.show()
fig = px.box(df_train, x='AmenitiesTotal',  title='AmenitiesTotal Distribution',
color_discrete_sequence=['#40db95'],
template='plotly_dark'
)

fig.show()
fig = px.box(df_train,y='Transported', x='AmenitiesTotal', color='Transported',  title='AmenitiesTotal vs Transported',
color_discrete_sequence=['#eb3134', '#10c2de'],
template='plotly_dark'
)

fig.show()
fig = px.histogram(df_train, x='Age_Group', color='Transported',
title='Transported vs Age Groups', barmode='group',
color_discrete_sequence=['#eb3134', '#10c2de'],
template='plotly_dark', text_auto=True, category_orders={'Age_Group': ['0-12', '13-17', '18-25', '26-30', '31-50', '51+']})

fig.show()
fig = px.histogram(df_train, x='Cabin_Deck',
title='Cabin Deck Distribution',
color_discrete_sequence=['#1a9799'],
template='plotly_dark', text_auto=True
)

fig.show()
fig = px.histogram(df_train, x='Cabin_Side',
title='Cabin Side Distribution',
color_discrete_sequence=['#b05325'],
template='plotly_dark',text_auto=True
)

fig.show()
fig = px.histogram(df_train, x='Cabin_Num', color='Transported',
title='Distribution of Cabin_Num by Transported', nbins=20,
color_discrete_sequence=['#eb3134', '#10c2de'], template='plotly_dark'
)

fig.show()
# Function to create Cabin_Num groups based on specified bins and labels
def create_cabin_groups(cabin):
# Define bins and corresponding labels for Cabin_Num groups
cabin_bins = [0, 300, 600, 900, 1200, 1500, 1800, 1900]
cabin_labels = ['0-300', '300-600', '600-900', '900-1200', '1200-1500', '1500-1800', '1800+']

# Use pd.cut to categorize Cabin_Num into groups
cabin_group = pd.cut(cabin, bins=cabin_bins, labels=cabin_labels, right=False)

# Return the resulting Cabin_Num groups
return cabin_group
# Create groups for Cabin_Num_Group
df_train['Cabin_Num_Group'] = create_cabin_groups(df_train['Cabin_Num'])
df_test['Cabin_Num_Group'] = create_cabin_groups(df_test['Cabin_Num'])

# Dropping the original 'Cabin_Num' feature as it's not used in our model, and we've created groups from it
df_train = df_train.drop('Cabin_Num', axis=1)
df_test = df_test.drop('Cabin_Num', axis=1)
# Create a box plot
fig = px.histogram(df_train, x='Cabin_Num_Group', color='Transported',
title='Transported vs Cabin_Num Groups', barmode='group',
color_discrete_sequence=['#eb3134', '#10c2de'], category_orders={'Cabin_Num_Group': ['0-300', '300-600', '600-900', '900-1200',
'1200-1500', '1500-1800']},
template='plotly_dark', text_auto=True)

fig.show()
# Applying Label Encoder to Work with Ordinal Features in Train and Test Datasets.

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Define the mapping dictionaries
Age_Group_dic = {
'0-12': 0, '13-17': 1,
'18-25': 2, '26-30': 3,
'31-50': 4, '51+': 5
}
Cabin_Num_Group_dic = {
'0-300': 0, '300-600': 1,
'600-900': 2, '900-1200': 3,
'1200-1500': 4, '1500-1800': 5,
'1800+': 6
}

# Apply label encoding to Age_Group and Cabin_Num_Group
df_train['Age_Group'] = label_encoder.fit_transform(df_train['Age_Group'].map(Age_Group_dic))
df_test['Age_Group'] = label_encoder.transform(df_test['Age_Group'].map(Age_Group_dic))

df_train['Cabin_Num_Group'] = label_encoder.fit_transform(df_train['Cabin_Num_Group'].map(Cabin_Num_Group_dic))
df_test['Cabin_Num_Group'] = label_encoder.transform(df_test['Cabin_Num_Group'].map(Cabin_Num_Group_dic))
# Convert 'Transported' column to numeric values (1 for True/transported, 0 for False/not transported)
df_train['Transported'] = df_train['Transported'].replace({True: 1, False: 0})
# Working with Nominal Features with pandas `get_dummies` function for train and test datasets.
nominal_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Passenger_Groups', 'Cabin_Deck', 'Cabin_Side']

# Encoding Train Data
df_train = pd.get_dummies(df_train, columns=nominal_cols)
train_encoded = list(df_train.columns)
print("{} total features after one-hot encoding for train data.".format(len(train_encoded)))

# Encoding Test Data
df_test = pd.get_dummies(df_test, columns=nominal_cols)
test_encoded = list(df_test.columns)
print("{} total features after one-hot encoding for test data.".format(len(test_encoded)))
df_train.head()
df_test.head()
numerical_features=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'AmenitiesTotal']

# Detect outliers in numerical features
train_outliers_indices = detect_outliers(df_train, features=numerical_features, n=0)
number_of_outliers_train = len(train_outliers_indices)

test_outliers_indices = detect_outliers(df_test, features=numerical_features, n=0)
number_of_outliers_test = len(test_outliers_indices)

# Print the number of outliers before handling for the Train and Test data.
print(f'Number of outliers in the Train Data before handling: {number_of_outliers_train}')
print(f'Number of outliers in the Test Data before handling: {number_of_outliers_test}')
# Perform log transformation on the specified numerical columns
df_train[numerical_features] = np.log1p(df_train[numerical_features])
df_test[numerical_features] = np.log1p(df_test[numerical_features])
train_outliers_indices = detect_outliers(df_train, features=numerical_features, n=0)
number_of_outliers_train = len(train_outliers_indices)

test_outliers_indices = detect_outliers(df_test, features=numerical_features, n=0)
number_of_outliers_test = len(test_outliers_indices)

# Print the number of outliers after handling for the Train and Test data.
print(f'Number of outliers in the Train Data after handling: {number_of_outliers_train}')
print(f'Number of outliers in the Test Data after handling: {number_of_outliers_test}')
# First we extract the x Featues and y Label
X = df_train.drop('Transported',axis=1)
y = df_train['Transported']
X.shape, y.shape
# Then we Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X,
y,
test_size=0.20,
random_state=42,
stratify=y)

# Show the results of the split
print("Training set has {} samples.".format(X_train.shape[0]))
print("Testing set has {} samples.".format(X_test.shape[0]))
y_train.value_counts()
sm = SMOTE(random_state=42)
X_train, y_train = sm.fit_resample(X_train, y_train)
y_train.value_counts()
numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'AmenitiesTotal']

# Creating a StandardScaler instance
scaler = StandardScaler()

# Fitting the StandardScaler on the training data
scaler.fit(X_train[numerical_features])

# Transforming (standardize) the continuous features in the training and testing data
X_train_cont_scaled = scaler.transform(X_train[numerical_features])
X_test_cont_scaled = scaler.transform(X_test[numerical_features])

# Replacing the scaled continuous features in the original data
X_train[numerical_features] = X_train_cont_scaled
X_test[numerical_features] = X_test_cont_scaled

X_train
# Create a heatmap using Plotly
heatmap_data = df_train.corr().values.tolist()

fig = go.Figure(data=go.Heatmap(z=heatmap_data, x=df_train.columns, y=df_train.columns, colorscale='Viridis'))

# Update layout
fig.update_layout(title='Correlation Heatmap',
xaxis_title='Features',
yaxis_title='Features',
template='plotly_dark')

# Show the figure
fig.show()
# Sort the correlation values
sorted_corr = df_train.corr()['Transported'].sort_values(ascending=False).drop(['Transported'])

# Create a bar plot using Plotly
fig = px.bar(x=sorted_corr.index, y=sorted_corr.values, color=sorted_corr.values,
color_continuous_scale='Viridis', labels={'x': 'Feature', 'y': 'Target'},
title='Correlation with Target (Transported)', template='plotly_dark')

# Show the figure
fig.show()
# List of classifiers to evaluate
classifiers = [
("Logistic Regression", LogisticRegression(random_state=42, max_iter=1500, n_jobs=-1)),
("KNN", KNeighborsClassifier(n_neighbors=5, n_jobs=-1)),
("Gaussian Naive Bayes", GaussianNB()),
("Decision Tree", DecisionTreeClassifier(random_state=42)),
("Random Forest", RandomForestClassifier(random_state=42, n_jobs=-1)),
("AdaBoost", AdaBoostClassifier(random_state=42)),
("Gradient Boosting", GradientBoostingClassifier(random_state=42)),
("LightGBM", lgb.LGBMClassifier(random_state=42, verbose=-1)),
("XGBoost", xgb.XGBClassifier(random_state=42, n_jobs=-1)),
("CatBoost", CatBoostClassifier(random_state=42, verbose=0))
]
# Creating lists for classifier names, mean_test_accuracy_scores, cross_val_errors, and results.
results = []
mean_test_accuracy_scores = []
cross_val_errors = []
classifier_names = []

for model_name, model in classifiers:

# 5-fold Stratified Cross-Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation with train scores
cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1, return_train_score=True)

# Calculate cross-validation error
cross_val_error = 1 - np.mean(cv_results['test_score'])

# Append results to the list
results.append({
"Model Name": model_name,
"Mean Train Accuracy": np.mean(cv_results['train_score']),
"Mean Test Accuracy": np.mean(cv_results['test_score']),
"Cross-Validation Error": cross_val_error
})

mean_test_accuracy_scores.append(np.mean(cv_results['test_score']))
cross_val_errors.append(cross_val_error)
classifier_names.append(model_name)

# Create a DataFrame from the results list
results_df = pd.DataFrame(results)

# Display the DataFrame
display(results_df)
# Creating a DataFrame for test accuracy and cross-validation error
data = pd.DataFrame({
'Classifier': classifier_names,
'Test Accuracy': mean_test_accuracy_scores,
'Cross-Validation Error': cross_val_errors
})

# Creating Plotly subplots with two columns and one row
fig = sp.make_subplots(rows=1, cols=2, subplot_titles=['Mean Test Accuracy', 'Cross-Validation Error'],
vertical_spacing=0.1, horizontal_spacing=0.20)

# Adding bar chart for Mean Test Accuracy
fig.add_trace(go.Bar(x=data['Test Accuracy'], y=data['Classifier'], orientation='h',
text=data['Test Accuracy'], marker=dict(color=data['Test Accuracy'], colorscale='RdBu'),
showlegend=False),
row=1, col=1)

# Sort the DataFrame by Cross-Validation Error in descending order
data = data.sort_values(by='Cross-Validation Error', ascending=False)

# Adding bar chart for Cross-Validation Error
fig.add_trace(go.Bar(x=data['Cross-Validation Error'], y=data['Classifier'], orientation='h',
text=data['Cross-Validation Error'], marker=dict(color=data['Cross-Validation Error'], colorscale='RdBu'),
showlegend=False),
row=1, col=2)

# Customizing the layout
fig.update_layout(title='Model Evaluation Metrics', template='plotly_dark',
xaxis=dict(range=[0, 1]), yaxis=dict(categoryorder='total ascending'))

# Show the plot
fig.show()
# Initialize CatBoost classifier
CatBoost = CatBoostClassifier(random_state=42, verbose=0)

# Train the model
CatBoost.fit(X_train, y_train)

# Predictions on test data
y_pred = CatBoost.predict(X_test)

# Calculate F1-score
f1 = f1_score(y_test, y_pred, average='weighted')

# Printing model details
print(f'Model: CatBoost')
print(f'Training Accuracy: {accuracy_score(y_train, CatBoost.predict(X_train))}')
print(f'Testing Accuracy: {accuracy_score(y_test, y_pred)}')
print(f'F1-score: {f1}')
print('------------------------------------------------------------------')
# Create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix with plotly
fig = ff.create_annotated_heatmap(z=conf_matrix, x=['Not Transported', 'Transported'], y=['Not Transported', 'Transported'],
colorscale='Viridis', showscale=True)

fig.update_layout(title='Confusion Matrix',
xaxis_title='Predicted label',
yaxis_title='True label',
width=500, height=400)

# Show the figure
fig.show()
param_grid = {
'max_depth': [7, 8 ,9],
'n_estimators':[300]
}
CatBoost = CatBoostClassifier(random_state=42, verbose=0)

grid_search = GridSearchCV(estimator=CatBoost, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1,
return_train_score=True)

# Fit the grid search to the data
grid_search.fit(X=X_train, y=y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

# Get the best model
final_model = grid_search.best_estimator_

# Evaluate the model on the test set
y_pred_test = final_model.predict(X_test)

# Calculate the testing accuracy
testing_accuracy = accuracy_score(y_test, y_pred_test)

# Print the testing accuracy
print('------------------------------------')
print("Testing Accuracy:", testing_accuracy)

# Get the mean test score and mean train score for the best estimator
mean_test_score = grid_search.cv_results_['mean_test_score'][grid_search.best_index_]
mean_train_score = grid_search.cv_results_['mean_train_score'][grid_search.best_index_]

print('------------------------------------')
print("Mean Train Score:", mean_train_score)
print("Mean Test Score:", mean_test_score)
final_model
# Predict probabilities for the positive class using the final model
y_probabilities = final_model.predict_proba(X_train)[:, 1]

# Calculate the ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_train, y_probabilities)
auc = roc_auc_score(y_train, y_probabilities)

# Create ROC curve trace
roc_trace = go.Scatter(
x=fpr,
y=tpr,
mode='lines',
line=dict(color='#10c2de', width=2),
name=f'ROC curve (AUC = {auc:.2f})'
)

# Add diagonal line
diagonal_trace = go.Scatter(
x=[0, 1],
y=[0, 1],
mode='lines',
line=dict(color='#eb3134', width=2, dash='dash'),
name='Random Guess'
)

# Create layout
layout = go.Layout(
title='ROC Curve for CatBoost Classifier',
xaxis=dict(title='False Positive Rate'),
yaxis=dict(title='True Positive Rate'),
showlegend=True,
template='plotly_dark'
)

# Create figure
fig = go.Figure(data=[roc_trace, diagonal_trace], layout=layout)

# Show the figure
fig.show()
# Making predictions on the test dataset using the final trained model
predictions = final_model.predict(
df_test.drop('PassengerId', axis=1)
)
output = pd.DataFrame(
{
'PassengerId': df_test.PassengerId,
'Transported': predictions
}
)
output['Transported'] = predictions > 0.5
output
output.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from category_encoders import BinaryEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_log_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
import seaborn as sns
import optuna

plt.style.use('fivethirtyeight')

# Input data files are available in the read-only "/share/dutta/eyao/dataset/kaggle/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
df_test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
df.head()
df.info()
labels = ['Not Transported', 'Transported']

fig, ax = plt.subplots(figsize=(10, 6))
df['Transported'].value_counts().plot(kind='pie', ax=ax, labels=labels, autopct='%1.1f%%', colors=['coral', 'teal'])

ax.set_xlabel('Passenger Fate')
ax.set_ylabel('')
ax.legend(title='Fates')

plt.show()
numeric_df = df.select_dtypes(include=['int64', 'float64'])
corr = numeric_df.corr()

colors = ["coral", "white", "teal"]
cmap = LinearSegmentedColormap.from_list("custom_coral_teal", colors)

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap=cmap,
xticklabels=corr.columns, yticklabels=corr.columns,
cbar_kws={'label': 'Correlation coefficient'})

plt.title('Correlation Heatmap')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.show()
df.isna().sum()
print(f'HomePlanet: {len(df.HomePlanet.unique())} \nCabin: {len(df.Cabin.unique())}\nDestination:{len(df.Destination.unique())}\nName:{len(df.Name.unique())}')
df[['Cabin1', 'Cabin2', 'Cabin3']] = df['Cabin'].str.split('/', expand=True)
df[['FirstName', 'LastName']] = df['Name'].str.split(' ', expand=True)
df['total'] = df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)
df['AgeBin'] = pd.qcut(df['Age'].fillna(df['Age'].mode()[0]), q=5, labels=False)


df.drop(['Cabin', 'Name'], axis=1, inplace=True)

df.head()
print(f"Cabin1: {len(df.Cabin1.unique())} \nCabin2: {len(df.Cabin2.unique())}\nCabin3: {len(df.Cabin3.unique())}\nFirstName: {len(df.FirstName.unique())}\nLastName: {len(df.LastName.unique())}")
y = df['Transported'].astype(int)
df.drop(['Transported'], axis=1, inplace=True)
encoder = BinaryEncoder(cols=['FirstName', 'LastName', 'Cabin1', 'Cabin3', 'Destination', 'VIP', 'HomePlanet'], return_df=True)
df = encoder.fit_transform(df)

df.drop(['VIP_1'], axis=1,inplace=True)

df.columns
# scaled_cols = ['Age', 'RoomService', 'FoodCourt']

X = df
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=42)
xgb_params = {'n_estimators': 248, 'learning_rate': 0.08276477030425759, 'max_depth': 4, 'reg_lambda': 9.144307734410582, 'subsample': 0.9761017636523421}
rf_params = {'n_estimators': 829, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}

xgb_model = XGBClassifier(**xgb_params)
rf_model = RandomForestClassifier(**rf_params)

pipeline = Pipeline([
('impute', KNNImputer(weights='distance', n_neighbors=3)),
('scale', MinMaxScaler()),
('xgb', VotingClassifier(
estimators=[
('xgb', xgb_model),
('rf', rf_model)
]
)),
])

best_model = pipeline.fit(X_train, Y_train)

Y_pred = best_model.predict(X_test)

# Y_proba = best_model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

precision = precision_score(Y_test, Y_pred)
print(f"Precision: {precision}")

recall = recall_score(Y_test, Y_pred)
print(f"Recall: {recall}")

f1 = f1_score(Y_test, Y_pred)
print(f"F1 Score: {f1}")

# roc_auc = roc_auc_score(Y_test, Y_proba)
# print(f"ROC-AUC Score: {roc_auc}")
df_test[['Cabin1', 'Cabin2', 'Cabin3']] = df_test['Cabin'].str.split('/', expand=True)
df_test[['FirstName', 'LastName']] = df_test['Name'].str.split(' ', expand=True)
df_test['total'] = df_test[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)
df_test['AgeBin'] = pd.qcut(df_test['Age'].fillna(df_test['Age'].mode()[0]), q=5, labels=False)
df_test.drop(['Cabin', 'Name'], axis=1, inplace=True)
df_test = encoder.transform(df_test)
df_test.drop(['VIP_1'], axis=1,inplace=True)
df_test['LastName_11'] = 0


preds = best_model.predict(df_test)
preds = preds.astype(bool)
df_test['Transported'] = preds
submission_df = df_test[['PassengerId', 'Transported']]
submission_df.to_csv('submission.csv', index=False)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

train_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv')
test_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv')

X = train_data.drop(['Id', 'SalePrice'], axis=1)
y = np.log(train_data['SalePrice'])

test_ids = test_data['Id']
X_test = test_data.drop(['Id'], axis=1)

numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

numerical_pipeline = Pipeline(steps=[
('imputer', SimpleImputer(strategy='median')),
('poly', PolynomialFeatures(degree=2, include_bias=False)),
('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
('imputer', SimpleImputer(strategy='most_frequent')),
('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
transformers=[
('num', numerical_pipeline, numerical_features),
('cat', categorical_pipeline, categorical_features)
])

xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
gb_model = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05, random_state=42)

xgb_pipeline = Pipeline(steps=[
('preprocessor', preprocessor),
('model', xgb_model)
])

rf_pipeline = Pipeline(steps=[
('preprocessor', preprocessor),
('model', rf_model)
])

gb_pipeline = Pipeline(steps=[
('preprocessor', preprocessor),
('model', gb_model)
])

param_grid = {
'model__n_estimators': [500, 1000, 2000],
'model__learning_rate': [0.01, 0.05, 0.1],
'model__max_depth': [3, 5, 7]
}

grid_search = GridSearchCV(xgb_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)
grid_search.fit(X, y)

best_xgb_model = grid_search.best_estimator_

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

best_xgb_model.fit(X_train, y_train)
y_preds_xgb = best_xgb_model.predict(X_valid)
rmse_xgb = np.sqrt(mean_squared_error(y_valid, y_preds_xgb))
print(f'XGBoost RMSE: {rmse_xgb}')

rf_pipeline.fit(X_train, y_train)
y_preds_rf = rf_pipeline.predict(X_valid)
rmse_rf = np.sqrt(mean_squared_error(y_valid, y_preds_rf))
print(f'Random Forest RMSE: {rmse_rf}')

gb_pipeline.fit(X_train, y_train)
y_preds_gb = gb_pipeline.predict(X_valid)
rmse_gb = np.sqrt(mean_squared_error(y_valid, y_preds_gb))
print(f'Gradient Boosting RMSE: {rmse_gb}')

y_preds_ensemble = (y_preds_xgb + y_preds_rf + y_preds_gb) / 3
rmse_ensemble = np.sqrt(mean_squared_error(y_valid, y_preds_ensemble))
print(f'Ensemble RMSE: {rmse_ensemble}')

best_xgb_model.fit(X, y)
rf_pipeline.fit(X, y)
gb_pipeline.fit(X, y)

test_preds_xgb = np.exp(best_xgb_model.predict(X_test))
test_preds_rf = np.exp(rf_pipeline.predict(X_test))
test_preds_gb = np.exp(gb_pipeline.predict(X_test))
test_preds_ensemble = (test_preds_xgb + test_preds_rf + test_preds_gb) / 3

submission = pd.DataFrame({'Id': test_ids, 'SalePrice': test_preds_ensemble})
submission.to_csv('submission4.csv', index=False)
import numpy as np
import pandas as pd
import plotly.express as px
import sklearn
sample_submission = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/sample_submission.csv')
sample_submission.head()
test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
test.head()
train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
train.head()
train.info()
train.isnull().sum()
train['Age'].fillna(train['Age'].mean().round(0),inplace=True)
train['FoodCourt'].fillna(train['FoodCourt'].mean(),inplace=True)
train['RoomService'].fillna(train['RoomService'].mean(),inplace=True)
train['ShoppingMall'].fillna(train['ShoppingMall'].mean(),inplace=True)
train['Spa'].fillna(train['Spa'].mean(),inplace=True)
train['VRDeck'].fillna(train['VRDeck'].mean(),inplace=True)
train.isnull().sum()
train.drop(columns=['HomePlanet','CryoSleep','Cabin','Destination','VIP','Name'],axis=1,inplace=True)
train.isnull().sum()
train.duplicated().sum()
test.info()
test.isnull().sum()
test['Age'].fillna(test['Age'].mean().round(0),inplace=True)
test['FoodCourt'].fillna(test['FoodCourt'].mean(),inplace=True)
test['RoomService'].fillna(test['RoomService'].mean(),inplace=True)
test['ShoppingMall'].fillna(test['ShoppingMall'].mean(),inplace=True)
test['Spa'].fillna(test['Spa'].mean(),inplace=True)
test['VRDeck'].fillna(test['VRDeck'].mean(),inplace=True)
test.isnull().sum()
test.drop(columns=['HomePlanet','CryoSleep','Cabin','Destination','VIP','Name'],axis=1,inplace=True)
test.isnull().sum()
test.duplicated().sum()
train.head(2)
px.imshow(train.corr(numeric_only=True),aspect=True,text_auto=True,color_continuous_scale='Blues')
px.bar(train.pivot_table(index='Age',columns='Transported',values='PassengerId',aggfunc='count'),barmode='group',height=400)
px.bar(train.pivot_table(index='Age',columns='Transported',values='PassengerId',aggfunc='count'),barmode='group',height=400)
px.scatter(data_frame=train,x='Transported',y='FoodCourt')
train.head()
test.head()
y_train = train['Transported']
y_train
X_train = train.drop(columns='Transported',axis=1)
X_train.head()
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train,y_train)
y_predict = model.predict(test)
y_predict
model.score(X_train,y_train)
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=2)
model.fit(X_train,y_train)
model.score(X_train,y_train)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train,y_train)
y_predict = model.predict(test)
y_predict
model.score(X_train,y_train)
output = pd.DataFrame({
'PassengerId':test.PassengerId,
'Transported':y_predict
})
output.shape
output.to_csv(r'C:\Users\roaia\Desktop\Anwar\New folder\output.csv',index=False)
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import xgboost as xgb
train_df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
test_df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
# Display the first few rows of the training data
print("Training Data Head:")
print(train_df.head())
print("\nTraining Data Info:")
print(train_df.info())
print("\nSummary Statistics:")
print(train_df.describe())
print("\nMissing Values:")
print(train_df.isnull().sum())
# 1. Distribution of the Target Variable (Transported)
plt.figure(figsize=(8, 6))
sns.countplot(x='Transported', data=train_df)
plt.title('Distribution of Target Variable (Transported)')
plt.show()
# 2. Age Distribution
plt.figure(figsize=(8, 6))
sns.histplot(train_df['Age'].dropna(), bins=30, kde=True)
plt.title('Age Distribution')
plt.show()
# 3. Cabin Class Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='Cabin', data=train_df)
plt.title('Cabin Class Distribution')
plt.show()
def fill_missing_values(df):
df['HomePlanet'] = df['HomePlanet'].fillna('Earth')
df['CryoSleep'] = df['CryoSleep'].fillna(False).infer_objects(copy=False)
df['Cabin'] = df['Cabin'].fillna('Unknown')
df['Destination'] = df['Destination'].fillna('TRAPPIST-1e')
df['Age'] = df['Age'].fillna(df['Age'].median())
df['VIP'] = df['VIP'].fillna(False).infer_objects(copy=False)
df = df.fillna(0)
return df

# Apply missing value filling
train_df = fill_missing_values(train_df)
test_df = fill_missing_values(test_df)

# Convert columns to string type to ensure uniformity
label_cols = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']
for col in label_cols:
train_df[col] = train_df[col].astype(str)
test_df[col] = test_df[col].astype(str)

# Combine data from both datasets for fitting the encoder
combined_data = pd.concat([train_df[label_cols], test_df[label_cols]], axis=0)

# Label encoding using combined data
label_encoders = {col: LabelEncoder().fit(combined_data[col]) for col in label_cols}

# Apply label encoding to the train and test sets
for col, le in label_encoders.items():
train_df[col] = le.transform(train_df[col])
test_df[col] = le.transform(test_df[col])

# Prepare features and target
X = train_df.drop(['PassengerId', 'Name', 'Transported'], axis=1)
y = train_df['Transported'].astype(int)
X_test = test_df.drop(['PassengerId', 'Name'], axis=1)
# Split the data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
# Train the model
model = xgb.XGBClassifier( n_estimators= 200, learning_rate= 0.1, max_depth = 5)
model.fit(X_train, y_train)
# Predict on validation set
y_pred = model.predict(X_val)
# Calculate accuracy
print('Classification Report:')
print(classification_report(y_val, y_pred))
# Predict on the test set
test_pred = model.predict(X_test)
# Prepare submission file
submission = pd.DataFrame({
'PassengerId': test_df['PassengerId'],
'Transported': test_pred
})
# Convert boolean predictions to string (True/False)
submission['Transported'] = submission['Transported'].map({1: True, 0: False})
# Save the submission file
submission.to_csv('submission.csv', index=False)
print('Submission file saved as submission.csv')
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "/share/dutta/eyao/dataset/kaggle/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import math
from scipy.stats import zscore
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import tensorflow as tf
import warnings
pd.set_option('display.max_columns', None)
warnings.filterwarnings("ignore")
df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv',index_col='Id')
df.head()
df.tail()
df.info()
df.describe()
print("Percentage of Nan in each Column")
for column, percentage in ((df.isna().sum() / df.shape[0]) * 100).items():
count = math.ceil(percentage)
if count > 0:
print(f"{column} : {count} %")
df.drop(columns=['MasVnrType','PoolQC','PoolArea','BsmtHalfBath','KitchenAbvGr','Utilities'],inplace=True)
df['LotFrontage'] = df['LotFrontage'].fillna(0)
df['Alley'] = df['Alley'].fillna("No Alley Acess")
df['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].mean())
df['LotFrontage'] = df['LotFrontage'].fillna(0)
df['BsmtQual'] = df['BsmtQual'].fillna("No Basment")
df['BsmtCond'] = df['BsmtCond'].fillna("No Basment")
df['BsmtExposure'] = df['BsmtExposure'].fillna("No Basment")
df['BsmtFinType1'] = df['BsmtFinType1'].fillna("No Basment")
df['BsmtFinType2'] = df['BsmtFinType2'].fillna("No Basment")
df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])
df['FireplaceQu'] = df['FireplaceQu'].fillna("No Fireplace")
df['GarageType'] = df['GarageType'].fillna("No Garage")
df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)
df['GarageFinish'] = df['GarageFinish'].fillna("No Garage")
df['GarageQual'] = df['GarageQual'].fillna("No Garage")
df['GarageCond'] = df['GarageCond'].fillna("No Garage")
df['Fence'] = df['Fence'].fillna("No Fence")
df['MiscFeature'] = df['MiscFeature'].fillna("No Miscellaneous Feature")
def remove_outlier(df, x):
df[f'{x}_zscore'] = zscore(df[f'{x}'])
df.drop(df[(df[f'{x}_zscore'] >= 3) | (df[f'{x}_zscore'] <= -3)].index, inplace=True)
df.drop(columns=[f'{x}_zscore'], inplace=True)
columns = df.select_dtypes('number').columns.tolist()
for column in columns:
remove_outlier(df, column)
df.info()
df.shape
df.describe()
df.hist(figsize=(20, 20), xlabelsize=10, ylabelsize=10,color='#ffc0cb');
plt.figure(figsize=(30,30))
sns.heatmap(df.select_dtypes('number').corr(),vmax=.8, annot=True,square=True,cmap='brg');
MSSubClass = df.groupby('MSSubClass')['SalePrice'].mean()
MSSubClassMap ={20 : '1-STORY 1946 & NEWER ALL STYLES',
30 : '1-STORY 1945 & OLDER',
40 : '1-STORY W/FINISHED ATTIC ALL AGES',
45 : "1-1/2 STORY - UNFINISHED ALL AGES",
50 : "1-1/2 STORY FINISHED ALL AGES",
60 : "2-STORY 1946 & NEWER",
70 : "2-STORY 1945 & OLDER",
75 : "2-1/2 STORY ALL AGES",
80 : "SPLIT OR MULTI-LEVEL",
85 : "SPLIT FOYER",
90 : "DUPLEX - ALL STYLES AND AGES",
120 : "1-STORY PUD (Planned Unit Development) - 1946 & NEWER",
150 : "1-1/2 STORY PUD - ALL AGES",
160 : "2-STORY PUD - 1946 & NEWER",
180 : "PUD - MULTILEVEL - INCL SPLIT LEV/FOYER",
190 : "2 FAMILY CONVERSION - ALL STYLES AND AGES"}

index_list = MSSubClass.index.tolist()
fig =px.bar(y=MSSubClass.values, x=[MSSubClassMap[i] for i in index_list], labels={'x': 'MSSubClass','y':'Value'},color=MSSubClass.values)
fig.update_layout(title='Mean of SalePrice for each MSSubClass',width=1300,
height=1000 )
fig.show()
MSZoningMap={
"A" : "Agriculture",
"C (all)" : "Commercial",
"FV" : "Floating Village Residential",
"I" : "Industrial",
"RH" : "Residential High Density",
"RL" : "Residential Low Density",
"RP" : "Residential Low Density Park",
"RM" : "Residential Medium Density"
}
index_list = df['MSZoning'].value_counts().index.tolist()

px.pie(values=df['MSZoning'].value_counts().values,names=[MSZoningMap[i] for i in index_list],title='Number of Houses with the Zonning Classification')
def plot_numeric_column_with_price(ax, x):
scatter = ax.scatter(x=df[x], y=df['SalePrice'], c=df['SalePrice'], cmap='viridis', alpha=0.8, s=10)
ax.set(xlabel=x, ylabel='SalePrice', title=f'{x} With Sale Price')
plt.colorbar(scatter, ax=ax, label='SalePrice')

columns = []
for c in df.select_dtypes('number').columns.tolist():
if df[f'{c}'].nunique() > 16:
columns.append(c)

num_rows = math.ceil(len(columns[:-1]) / 2)
fig, axes = plt.subplots(num_rows, 2, figsize=(18, 6 * num_rows))
axes = axes.flatten()
for i, column in enumerate(columns[:-1]):
if i >= len(axes):
break
plot_numeric_column_with_price(axes[i], column)

plt.tight_layout()
plt.show()
def plot_categorical_column_with_price(x, ax):
mean = df.groupby(x)['SalePrice'].mean()
sns.barplot(x=mean.index, y=mean.values, width=0.5, ax=ax,palette='viridis')
ax.set_xlabel(x)
ax.set_ylabel('SalePrice')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
ax.set_title(f'Mean of Sale Price With {x}')

columns = df.select_dtypes(exclude='number').columns.tolist()
for c in df.select_dtypes('number').columns.tolist():
if df[f'{c}'].nunique() <= 16:
columns.append(c)

num_columns = 3
num_rows = (len(columns) + num_columns - 1) // num_columns

fig, axes = plt.subplots(num_rows, num_columns, figsize=(18, 6 * num_rows))

for i, column in enumerate(columns):
row = i // num_columns
col = i % num_columns
if num_rows > 1:
ax = axes[row, col]
else:
ax = axes[col]
plot_categorical_column_with_price(column, ax)

for i in range(len(columns), num_rows * num_columns):
row = i // num_columns
col = i % num_columns
if num_rows > 1:
fig.delaxes(axes[row, col])
else:
fig.delaxes(axes[col])

plt.tight_layout()
plt.show()
def box_plot_categorical_column_with_price(x, ax):
sns.boxplot(x=x, y='SalePrice', data=df, ax=ax, palette='viridis')
ax.set_xlabel(x)
ax.set_ylabel('SalePrice')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
ax.set_title(f'Box Plot of Sale Price by {x}')


columns = df.select_dtypes(exclude='number').columns.tolist()
for c in df.select_dtypes('number').columns.tolist():
if df[f'{c}'].nunique() <= 16:
columns.append(c)

num_columns = 3
num_rows = (len(columns) + num_columns - 1) // num_columns

fig, axes = plt.subplots(num_rows, num_columns, figsize=(18, 6 * num_rows), sharey=True)
axes = axes.flatten()

for i, column in enumerate(columns):
box_plot_categorical_column_with_price(column, axes[i])

for i in range(len(columns), num_rows * num_columns):
fig.delaxes(axes[i])

plt.tight_layout()
plt.show()
df = pd.get_dummies(df,dtype=float,drop_first=True)
df.head()
index_to_exclude = df.columns.get_loc('SalePrice')
X = df.iloc[:, [i for i in range(df.shape[1]) if i != index_to_exclude]].values
y = df.iloc[:,index_to_exclude].values
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=329)
X_train_nstd = X_train.copy()
X_test_nstd = X_test.copy()
y_train_nstd = y_train.copy()

sc_X = StandardScaler()
X_train[:, :34] = sc_X.fit_transform(X_train[:, :34])
X_test[:, :34] = sc_X.transform(X_test[:, :34])
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.reshape(-1,1)).flatten()
multi_lr = LinearRegression()
multi_lr.fit(X_train_nstd, y_train_nstd)
y_pred = multi_lr.predict(X_test_nstd)
y_train_pred = multi_lr.predict(X_train_nstd)
r2_lr_train = r2_score(y_train_nstd, y_train_pred)
r2_lr_test = r2_score(y_test, y_pred.reshape(-1,1))
print("R2 Train Score:", r2_lr_train)
print("R2 Test Score:", r2_lr_test)
mse_lr_train = mean_squared_error(y_train_nstd, y_train_pred)
mse_lr_test = mean_squared_error(y_test, y_pred.reshape(-1,1))
print("Mean Squared Error of Train:", mse_lr_train)
print("Mean Squared Error of Test:", mse_lr_test)
poly_reg = PolynomialFeatures(degree = 2)
X_train_poly = poly_reg.fit_transform(X_train)
X_test_poly = poly_reg.transform(X_test)
poly_lr = LinearRegression()
poly_lr.fit(X_train_poly, y_train)
y_pred = poly_lr.predict(X_test_poly)
y_train_pred = poly_lr.predict(X_train_poly)
r2_poly_train = r2_score(y_train,y_train_pred)
r2_poly_test = r2_score(y_test,sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_poly_train)
print("R2 Test Score:", r2_poly_test)
mse_poly_train = mean_squared_error(y_train, y_train_pred)
mse_poly_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_poly_train)
print("Mean Squared Error of Test:", mse_poly_test)
dec_tree = DecisionTreeRegressor()
dec_tree.fit(X_train, y_train)
y_pred = dec_tree.predict(X_test)
y_train_pred = dec_tree.predict(X_train)
r2_tree_train = r2_score(y_train, y_train_pred)
r2_tree_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_tree_train)
print("R2 Test Score:", r2_tree_test)
mse_tree_train = mean_squared_error(y_train, y_train_pred)
mse_tree_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_tree_train)
print("Mean Squared Error of Test:", mse_tree_test)
rdm_frst = RandomForestRegressor(n_estimators = 100)
rdm_frst.fit(X_train, y_train)
y_pred = rdm_frst.predict(X_test)
y_train_pred = rdm_frst.predict(X_train)
r2_frst_train = r2_score(y_train, y_train_pred)
r2_frst_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_frst_train)
print("R2 Test Score:", r2_frst_test)
mse_frst_train = mean_squared_error(y_train, y_train_pred)
mse_frst_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_frst_train)
print("Mean Squared Error of Test:", mse_frst_test)
svr = SVR(kernel = 'linear')
svr.fit(X_train, y_train)
y_pred = svr.predict(X_test)
y_train_pred = svr.predict(X_train)
r2_svr_lr_train = r2_score(y_train, y_train_pred)
r2_svr_lr_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_svr_lr_train)
print("R2 Test Score:", r2_svr_lr_test)
mse_svr_lr_train = mean_squared_error(y_train, y_train_pred)
mse_svr_lr_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_svr_lr_train)
print("Mean Squared Error of Test:", mse_svr_lr_test)
svr = SVR(kernel = 'rbf')
svr.fit(X_train, y_train)
y_pred = svr.predict(X_test)
y_train_pred = svr.predict(X_train)
r2_svr_train = r2_score(y_train, y_train_pred)
r2_svr_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_svr_train)
print("R2 Test Score:", r2_svr_test)
mse_svr_train = mean_squared_error(y_train, y_train_pred)
mse_svr_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_svr_train)
print("Mean Squared Error of Test:", mse_svr_test)
ridge = Ridge()
ridge.fit(X_train, y_train)
y_pred = ridge.predict(X_test)
y_train_pred = ridge.predict(X_train)
r2_ridge_train = r2_score(y_train, y_train_pred)
r2_ridge_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_ridge_train)
print("R2 Test Score:", r2_ridge_test)
mse_ridge_train = mean_squared_error(y_train, y_train_pred)
mse_ridge_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_ridge_train)
print("Mean Squared Error of Test:", mse_ridge_test)
lasso = Lasso(alpha=0.001)
lasso.fit(X_train, y_train)
y_pred = lasso.predict(X_test)
y_train_pred = lasso.predict(X_train)
r2_lasso_train = r2_score(y_train, y_train_pred)
r2_lasso_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_lasso_train)
print("R2 Test Score:", r2_lasso_test)
mse_lasso_train = mean_squared_error(y_train, y_train_pred)
mse_lasso_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_lasso_train)
print("Mean Squared Error of Test:", mse_lasso_test)
xgb = XGBRegressor(learning_rate=0.2,gamma=0.2)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
y_train_pred = xgb.predict(X_train)
r2_xgb_train = r2_score(y_train, y_train_pred)
r2_xgb_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_xgb_train)
print("R2 Test Score:", r2_xgb_test)
mse_xgb_train = mean_squared_error(y_train, y_train_pred)
mse_xgb_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_xgb_train)
print("Mean Squared Error of Test:", mse_xgb_test)
tf.random.set_seed(329)
np.random.seed(329)

nn_model = tf.keras.Sequential([
tf.keras.layers.Dense(100),
tf.keras.layers.Dense(100),
tf.keras.layers.Dense(100),
tf.keras.layers.Dense(100),
tf.keras.layers.Dense(100),
tf.keras.layers.Dense(1)
])

nn_model.compile(loss = tf.keras.losses.mae,
optimizer=tf.keras.optimizers.Adam(lr=0.1),
metrics=['mae','mse'])

history = nn_model.fit(X_train,y_train,epochs=200,verbose=2)
nn_model.summary()
pd.DataFrame(history.history).plot()
plt.title('Loss Graph')
plt.ylabel('loss')
plt.xlabel('epochs');
y_pred = nn_model.predict(X_test)
y_train_pred = nn_model.predict(X_train)
r2_nn_train = r2_score(y_train, y_train_pred)
r2_nn_test = r2_score(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("R2 Train Score:", r2_nn_train)
print("R2 Test Score:", r2_nn_test)
mse_nn_train = mean_squared_error(y_train, y_train_pred)
mse_nn_test = mean_squared_error(y_test, sc_y.inverse_transform(y_pred.reshape(-1,1)))
print("Mean Squared Error of Train:", mse_nn_train)
print("Mean Squared Error of Test:", mse_nn_test)
models = pd.DataFrame({
'Model': [
'Multiple Linear Regression','Polynomial Regression','Decision Tree',
'Random Forest', 'Support Vector Regression','Linear Support Vector Regression','Ridge','Lasso','XGBoost','Neural Network'
],
'Training R2 Score': [
r2_lr_train,r2_poly_train,r2_tree_train,r2_frst_train,r2_svr_train,r2_svr_lr_train,r2_ridge_train,r2_lasso_train,r2_xgb_train,r2_nn_train
],
'Training Mean Square Error': [
mse_lr_train,mse_poly_train,mse_tree_train,mse_frst_train,mse_svr_train,mse_svr_lr_train,mse_ridge_train,mse_lasso_train,mse_xgb_train,mse_nn_train
],
'Testing R2 Score': [
r2_lr_test,r2_poly_test,r2_tree_test,r2_frst_test,r2_svr_test,r2_svr_lr_test,r2_ridge_test,r2_lasso_test,r2_xgb_test,r2_nn_test
],
'Testing Mean Square Error': [
mse_lr_test,mse_poly_test,mse_tree_test,mse_frst_test,mse_svr_test,mse_svr_lr_test,mse_ridge_test,mse_lasso_test,mse_xgb_test,mse_nn_test
]
})
models.sort_values(by='Testing R2 Score', ascending=False).style.background_gradient(
cmap='Blues')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.exceptions import ConvergenceWarning
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter("ignore", category=ConvergenceWarning)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
train = pd.read_csv("/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv")
test = pd.read_csv("/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv")
df = pd.concat([train, test], ignore_index=True).reset_index(drop=True)
def check_df(dataframe):
print("##################### Shape #####################")
print(dataframe.shape)
print("##################### Types #####################")
print(dataframe.dtypes)
print("##################### Head #####################")
print(dataframe.head(3))
print("##################### Tail #####################")
print(dataframe.tail(3))
print("##################### NA #####################")
print(dataframe.isnull().sum())
print("##################### Quantiles #####################")
numeric_columns = dataframe.select_dtypes(include=['number']).columns
print(dataframe[numeric_columns].quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

check_df(df)
def grab_col_names(dataframe, cat_th=10, car_th=20):
cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes != "O"]
cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtypes == "O"]
cat_cols = cat_cols + num_but_cat
cat_cols = [col for col in cat_cols if col not in cat_but_car]
num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
num_cols = [col for col in num_cols if col not in num_but_cat]
print(f"Observations: {dataframe.shape[0]}")
print(f"Variables: {dataframe.shape[1]}")
print(f'cat_cols: {len(cat_cols)}')
print(f'num_cols: {len(num_cols)}')
print(f'cat_but_car: {len(cat_but_car)}')
print(f'num_but_cat: {len(num_but_cat)}')
return cat_cols, cat_but_car, num_cols

cat_cols, cat_but_car, num_cols = grab_col_names(df)
def cat_summary(dataframe, col_name, plot=False):
print(pd.DataFrame({col_name: dataframe[col_name].value_counts(), "Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)}))
if plot:
sns.countplot(x=dataframe[col_name], data=dataframe)
plt.show()

for col in cat_cols:
cat_summary(df, col)
def num_summary(dataframe, numerical_col, plot=False):
quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
print(dataframe[numerical_col].describe(quantiles).T)
if plot:
dataframe[numerical_col].hist(bins=50)
plt.xlabel(numerical_col)
plt.title(numerical_col)
plt.show()
print("#####################################")
for col in num_cols:
num_summary(df, col, True)
def target_summary_with_cat(dataframe, target, categorical_col):
print(pd.DataFrame({"TARGET_MEAN": dataframe.groupby(categorical_col)[target].mean()}), end="\n\n\n")

for col in cat_cols:
target_summary_with_cat(df, "SalePrice", col)
df["SalePrice"].hist(bins=100)
plt.show()

np.log1p(df['SalePrice']).hist(bins=50)
plt.show()
corr = df[num_cols].corr()
sns.set(rc={'figure.figsize': (12, 12)})
sns.heatmap(corr, cmap="RdBu")
plt.show()
def high_correlated_cols(dataframe, plot=False, corr_th=0.70):
numeric_df = dataframe.select_dtypes(include=[np.number])
corr = numeric_df.corr()
cor_matrix = corr.abs()
upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))
drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]
if plot:
sns.set(rc={'figure.figsize': (15, 15)})
sns.heatmap(corr, cmap="RdBu")
plt.show()
return drop_list

drop_list = high_correlated_cols(df, plot=False)
print(drop_list)

saleprice_corr = corr['SalePrice'].abs().sort_values(ascending=False)
print(saleprice_corr)

top_corr_features = saleprice_corr.index[:10]
print("Columns with highest correlation: ", top_corr_features)
def outlier_thresholds(dataframe, variable, low_quantile=0.10, up_quantile=0.90):
quantile_one = dataframe[variable].quantile(low_quantile)
quantile_three = dataframe[variable].quantile(up_quantile)
interquantile_range = quantile_three - quantile_one
up_limit = quantile_three + 1.5 * interquantile_range
low_limit = quantile_one - 1.5 * interquantile_range
return low_limit, up_limit

def check_outlier(dataframe, col_name):
low_limit, up_limit = outlier_thresholds(dataframe, col_name)
if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
return True
else:
return False
for col in num_cols:
if col != "SalePrice":
print(col, check_outlier(df, col))
def replace_with_thresholds(dataframe, variable):
low_limit, up_limit = outlier_thresholds(dataframe, variable)
dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

for col in num_cols:
if col != "SalePrice":
replace_with_thresholds(df, col)
for col in num_cols:
if col != "SalePrice":
print(col, check_outlier(df, col))
def missing_values_table(dataframe, na_name=False):
na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]
n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)
ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)
missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])
print(missing_df, end="\n")
if na_name:
return na_columns

missing_values_table(df)
no_cols = ["Alley", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "FireplaceQu",
"GarageType", "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "Fence", "MiscFeature"]

for col in no_cols:
df[col].fillna("No", inplace=True)

missing_values_table(df)

def quick_missing_imp(data, num_method="median", cat_length=20, target="SalePrice"):
variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]
temp_target = data[target]
print("# BEFORE")
print(data[variables_with_na].isnull().sum(), "\n\n")
data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == "O" and len(x.unique()) <= cat_length) else x, axis=0)
if num_method == "mean":
data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != "O" else x, axis=0)
elif num_method == "median":
data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != "O" else x, axis=0)
data[target] = temp_target
print("# AFTER \n Imputation method is 'MODE' for categorical variables!")
print(" Imputation method is '" + num_method.upper() + "' for numeric variables! \n")
print(data[variables_with_na].isnull().sum(), "\n\n")
return data

df = quick_missing_imp(df, num_method="median", cat_length=17)
missing_values_table(df)
def rare_analyser(dataframe, target, cat_cols):
for col in cat_cols:
print(col, ":", len(dataframe[col].value_counts()))
print(pd.DataFrame({"COUNT": dataframe[col].value_counts(),
"RATIO": dataframe[col].value_counts() / len(dataframe),
"TARGET_MEAN": dataframe.groupby(col)[target].mean()}), end="\n\n\n")

rare_analyser(df, "SalePrice", cat_cols)

def rare_encoder(dataframe, rare_perc):
temp_df = dataframe.copy()
rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'
and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]
for var in rare_columns:
tmp = temp_df[var].value_counts() / len(temp_df)
rare_labels = tmp[tmp < rare_perc].index
temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])
return temp_df

df = rare_encoder(df, 0.01)
df["NEW_1st*GrLiv"] = df["1stFlrSF"] * df["GrLivArea"]
df["NEW_Garage*GrLiv"] = df["GarageArea"] * df["GrLivArea"]
qual_columns = ["OverallQual", "OverallCond", "ExterQual", "ExterCond", "BsmtCond", "BsmtFinType1",
"BsmtFinType2", "HeatingQC", "KitchenQual", "Functional", "FireplaceQu", "GarageQual", "GarageCond", "Fence"]
for col in qual_columns:
df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
df["TotalQual"] = df[qual_columns].sum(axis=1)
df["NEW_TotalFlrSF"] = df["1stFlrSF"] + df["2ndFlrSF"]
df["NEW_TotalBsmtFin"] = df.BsmtFinSF1 + df.BsmtFinSF2
df["NEW_PorchArea"] = df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch + df["3SsnPorch"] + df.WoodDeckSF
df["NEW_TotalHouseArea"] = df.NEW_TotalFlrSF + df.TotalBsmtSF
df["NEW_TotalSqFeet"] = df.GrLivArea + df.TotalBsmtSF
df["NEW_LotRatio"] = df.GrLivArea / df.LotArea
df["NEW_RatioArea"] = df.NEW_TotalHouseArea / df.LotArea
df["NEW_GarageLotRatio"] = df.GarageArea / df.LotArea
df["NEW_MasVnrRatio"] = df.MasVnrArea / df.NEW_TotalHouseArea
df["NEW_DifArea"] = df.LotArea - df["1stFlrSF"] - df.GarageArea - df.NEW_PorchArea - df.WoodDeckSF
df["NEW_OverallGrade"] = df["OverallQual"] * df["OverallCond"]
df["NEW_Restoration"] = df.YearRemodAdd - df.YearBuilt
df["NEW_HouseAge"] = df.YrSold - df.YearBuilt
df["NEW_RestorationAge"] = df.YrSold - df.YearRemodAdd
df["NEW_GarageAge"] = df.GarageYrBlt - df.YearBuilt
df["NEW_GarageRestorationAge"] = np.abs(df.GarageYrBlt - df.YearRemodAdd)
df["NEW_GarageSold"] = df.YrSold - df.GarageYrBlt
drop_list = ["Street", "Alley", "LandContour", "Utilities", "LandSlope", "Heating", "PoolQC", "MiscFeature", "Neighborhood"]
existing_columns = [col for col in drop_list if col in df.columns]
df.drop(existing_columns, axis=1, inplace=True)
cat_cols, cat_but_car, num_cols = grab_col_names(df)
def label_encoder(dataframe, binary_col):
labelencoder = LabelEncoder()
dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])
return dataframe

binary_cols = [col for col in df.columns if df[col].dtypes == "O" and len(df[col].unique()) == 2]

for col in binary_cols:
label_encoder(df, col)
print(df[binary_cols].head())
df_before_encoding = df.copy()
cat_cols = [col for col in cat_cols if col in df.columns]

def one_hot_encoder(dataframe, categorical_cols, drop_first=False):
dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)
return dataframe

df = one_hot_encoder(df, cat_cols, drop_first=True)

new_columns = set(df.columns) - set(df_before_encoding.columns)
print("Changes due to one-hot encoding:")
print(df[list(new_columns)].head())
train_df = df[df['SalePrice'].notnull()]
test_df = df[df['SalePrice'].isnull()]

y = np.log1p(train_df['SalePrice'])
X = train_df.drop(["Id", "SalePrice"], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)
models = [('LR', LinearRegression()), ('KNN', KNeighborsRegressor()), ('CART', DecisionTreeRegressor()),
('RF', RandomForestRegressor()), ('GBM', GradientBoostingRegressor()),
("XGBoost", XGBRegressor(objective='reg:squarederror')), ("LightGBM", LGBMRegressor(verbose=-1))]
for name, regressor in models:
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
y_pred_exp = np.expm1(y_pred)
y_test_exp = np.expm1(y_test)
rmse = np.sqrt(mean_squared_error(y_test_exp, y_pred_exp))
print(f"RMSE: {round(rmse, 4)} ({name})")
lgbm = LGBMRegressor(verbose=-1).fit(X_train, y_train)
y_pred = lgbm.predict(X_test)
def calculate_metrics(y_true, y_pred):
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_true, y_pred)
return mae, mse, rmse, r2

def evaluate_percentiles(y_true, y_pred):
percentiles = [5, 25, 50, 75, 95, 100]
results = {}
for percentile in percentiles:
threshold = np.percentile(y_true, percentile)
indices = y_true <= threshold
filtered_y_true = y_true[indices]
filtered_y_pred = y_pred[indices]
mae, mse, rmse, r2 = calculate_metrics(filtered_y_true, filtered_y_pred)
results[percentile] = (mae, mse, rmse, r2)
return results
def print_results(results):
for percentile, metrics in results.items():
mae, mse, rmse, r2 = metrics
print(f"Performance for {percentile}th Percentile:")
print(f"  Mean Absolute Error (MAE): {mae}")
print(f"  Mean Squared Error (MSE): {mse}")
print(f"  Root Mean Squared Error (RMSE): {rmse}")
print(f"  R-squared (R): {r2}")
print()
y_pred_exp = np.expm1(y_pred)
y_test_exp = np.expm1(y_test)
results = evaluate_percentiles(y_test_exp, y_pred_exp)
print_results(results)
lgbm_model = LGBMRegressor(random_state=46, verbose=-1)
rmse = np.mean(np.sqrt(-cross_val_score(lgbm_model, X, y, cv=5, scoring="neg_mean_squared_error")))

lgbm_params = {"learning_rate": [0.01, 0.1], "n_estimators": [500, 1500]}
lgbm_gs_best = GridSearchCV(lgbm_model, lgbm_params, cv=3, n_jobs=-1, verbose=True).fit(X, y)

final_model = lgbm_model.set_params(**lgbm_gs_best.best_params_, random_state=46, verbose=-1).fit(X, y)
rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring="neg_mean_squared_error")))
print("Mean RMSE:", rmse)
def plot_importance(model, features, num=len(X), save=False):
feature_imp = pd.DataFrame({"Value": model.feature_importances_, "Feature": features.columns})
plt.figure(figsize=(10, 10))
sns.set(font_scale=1)
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False)[0:num])
plt.title("Features")
plt.tight_layout()
plt.show()
if save:
plt.savefig("importances.png")

model = LGBMRegressor(verbose=-1)
model.fit(X, y)
plot_importance(model, X)
model = LGBMRegressor(verbose=-1)
model.fit(X, y)
predictions = model.predict(test_df.drop(["Id", "SalePrice"], axis=1))
real_predictions = np.exp(predictions)
submission = pd.DataFrame({
"Id": test_df["Id"].astype(int),
"SalePrice": real_predictions
})
submission.to_csv("housePricePredictions.csv", index=False)
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
import warnings
import os

warnings.filterwarnings("ignore")

df_train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
df_test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')

data = pd.concat([df_train, df_test], axis = 0)
data.reset_index(drop = True, inplace = True)
data.head()
data.info()
data.isnull().sum()
display(data.groupby('HomePlanet')['Transported'].mean())
sns.countplot(data, x = 'HomePlanet', hue = 'Transported')
display(data.groupby('CryoSleep')['Transported'].mean())
sns.countplot(data, x = 'CryoSleep', hue = 'Transported')
display(data.groupby('Destination')['Transported'].mean())
sns.countplot(data, x = 'Destination', hue = 'Transported')
display(data.groupby('VIP')['Transported'].mean())
sns.countplot(data, x = 'VIP', hue = 'Transported')
display(data.groupby('HomePlanet')['Transported'].mean())
sns.countplot(data, x = 'HomePlanet', hue = 'Transported')
data['CryoSleep'].fillna(False, inplace = True)

data['CryoSleep'] = data['CryoSleep'].map(lambda x: 1 if x else 0)
data['Transported'] = data['Transported'].map({True: 1, False: 0})

base_train = data[data['Transported'].notnull()]
base_model = RandomForestClassifier(n_estimators = 250, random_state = 0, min_samples_split = 20, oob_score = True)
base_model.fit(base_train[['CryoSleep']], base_train['Transported'])
print(f"oob score: {base_model.oob_score_}")
base_test = data[data['Transported'].isnull()]
base_output = base_model.predict(base_test[['CryoSleep']]).astype(bool)
# base_output = base_output.map({0: False, 1: True})
submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': base_output})
submission.to_csv('Base_Submission.csv', index = False)
data['Age'].fillna(-20, inplace = True)
data['Age'] = data['Age'].astype(int)

mask_transported = data.loc[(data['Transported'] == 1), 'Age']
mask_not_transported = data.loc[(data['Transported'] == 0), 'Age']
fig, ax = plt.subplots(figsize = (13, 5))
ax = sns.histplot(mask_transported, kde = False, label = 'transported', bins = 10)
ax = sns.histplot(mask_not_transported, kde = False, label = 'not_transported', bins = 10)
ax.legend()
data.loc[(data['Age'] == -20), 'Age_code'] = 1
data.loc[(data['Age'] <= 10) & (data['Age'] >= 0), 'Age_code'] = 2
data.loc[(data['Age'] > 10), 'Age_code'] = 0
display(data.groupby('Age_code')['Transported'].mean())
V1_train = data[data['Transported'].notnull()]
model_V1 = RandomForestClassifier(n_estimators = 250, random_state = 0, min_samples_split = 20, oob_score = True)
model_V1.fit(V1_train[['CryoSleep', 'Age_code']], V1_train['Transported'])
print(f"oob score: {model_V1.oob_score_}")
V1_test = data[data['Transported'].isnull()]
V1_output = model_V1.predict(V1_test[['CryoSleep', 'Age_code']]).astype(bool)
submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': V1_output})
submission.to_csv('V1_Submission.csv', index = False)
data['RoomService'].fillna(data['RoomService'].median(), inplace = True)
data['FoodCourt'].fillna(data['FoodCourt'].median(), inplace = True)
data['ShoppingMall'].fillna(data['ShoppingMall'].median(), inplace = True)
data['Spa'].fillna(data['Spa'].median(), inplace = True)
data['VRDeck'].fillna(data['VRDeck'].median(), inplace = True)

data['Total_Cost'] = data['RoomService'] + data['FoodCourt'] + data['ShoppingMall'] + data['Spa'] + data['VRDeck']
V2_train = data[data['Transported'].notnull()]
model_V2 = RandomForestClassifier(n_estimators = 250, random_state = 0, min_samples_split = 20, oob_score = True)
model_V2.fit(V2_train[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']], V2_train['Transported'])
print(f"oob score: {model_V2.oob_score_}")
V2_test = data[data['Transported'].isnull()]
V2_output = model_V2.predict(V2_test[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']]).astype(bool)
submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': V2_output})
submission.to_csv('V2_Submission.csv', index = False)
data['Destination'].fillna('TRAPPIST-1e ', inplace = True)
data['HomePlanet'].fillna('Earth ', inplace = True)
data.groupby(['HomePlanet', 'Destination'])['Transported'].mean()
data['HomePlanet+Destination'] = data['HomePlanet'] + '_' + data['Destination']

le = LabelEncoder()
data['HomePlanet+Destination_code'] = le.fit_transform(data['HomePlanet+Destination'])
V3_train = data[data['Transported'].notnull()]
model_V3 = RandomForestClassifier(n_estimators = 250, random_state = 0, min_samples_split = 20, oob_score = True)
model_V3.fit(V3_train[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet+Destination_code']], V3_train['Transported'])
print(f"oob score: {model_V3.oob_score_}")
V3_test = data[data['Transported'].isnull()]
V3_output = model_V3.predict(V3_test[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet+Destination_code']]).astype(bool)
submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': V3_output})
submission.to_csv('V3_Submission.csv', index = False)
cabin = data['Cabin'].str.split('/', expand = True)
data['Desk'], data['Num'], data['Side'] = cabin[0], cabin[1], cabin[2]
data['Desk'].fillna('F', inplace = True)
data['Side'].fillna('S', inplace = True)
le = LabelEncoder()
data['Desk'] = le.fit_transform(data['Desk'])
data['Side'] = le.fit_transform(data['Side'])
V4_train = data[data['Transported'].notnull()]
model_V4 = RandomForestClassifier(n_estimators = 250, random_state = 0, min_samples_split = 20, oob_score = True)
model_V4.fit(V4_train[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet+Destination_code', 'Desk', 'Side']], V4_train['Transported'])
print(f"oob score: {model_V4.oob_score_}")
V4_test = data[data['Transported'].isnull()]
V4_output = model_V4.predict(V4_test[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet+Destination_code', 'Desk', 'Side']]).astype(bool)
submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': V4_output})
submission.to_csv('V4_Submission.csv', index = False)
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
import warnings
import os

warnings.filterwarnings("ignore")

df_train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
df_test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')

data = pd.concat([df_train, df_test], axis = 0)
data.reset_index(drop = True, inplace = True)

data['CryoSleep'].fillna(False, inplace = True)

data['CryoSleep'] = data['CryoSleep'].map(lambda x: 1 if x else 0)
data['Transported'] = data['Transported'].map({True: 1, False: 0})
data['Age'].fillna(-20, inplace = True)
data['Age'] = data['Age'].astype(int)
data.loc[(data['Age'] == -20), 'Age_code'] = 0
data.loc[(data['Age'] <= 10) & (data['Age'] >= 0), 'Age_code'] = 1
data.loc[(data['Age'] > 10), 'Age_code'] = 2
data['RoomService'].fillna(data['RoomService'].median(), inplace = True)
data['FoodCourt'].fillna(data['FoodCourt'].median(), inplace = True)
data['ShoppingMall'].fillna(data['ShoppingMall'].median(), inplace = True)
data['Spa'].fillna(data['Spa'].median(), inplace = True)
data['VRDeck'].fillna(data['VRDeck'].median(), inplace = True)
data['Surname'] = data['Name'].dropna().str.split(' ', expand = True)[1]
data['Total_Cost'] = data['RoomService'] + data['FoodCourt'] + data['ShoppingMall'] + data['Spa'] + data['VRDeck']
data['Destination'].fillna('TRAPPIST-1e ', inplace = True)
data['HomePlanet'].fillna('Earth ', inplace = True)
data['HomePlanet+Destination'] = data['HomePlanet'] + '_' + data['Destination']
data['Zero_Cost'] = np.where(data['Total_Cost'] == 0, 1, 0)

le = LabelEncoder()
data['HomePlanet+Destination_code'] = le.fit_transform(data['HomePlanet+Destination'])

cabin = data['Cabin'].str.split('/', expand = True)
data['Desk'], data['Num'], data['Side'] = cabin[0], cabin[1], cabin[2]

data['Desk'].fillna('F', inplace = True)
data['Side'].fillna('S', inplace = True)
le = LabelEncoder()
data['Desk'] = le.fit_transform(data['Desk'])
data['Side'] = le.fit_transform(data['Side'])

data.head()
data[['Group_Num', 'Ident']] = data['PassengerId'].str.split('_', expand = True).astype(int)
data['Group_Size'] = data['Group_Num'].apply(lambda x: data['Group_Num'].value_counts()[x])
data['Solo'] = np.where(data['Group_Size'] == 1, 1, 0)
data['Num'].fillna('9999', inplace = True)
data['Num'] = data['Num'].astype(int)
data['Num'] = data['Num'].map(lambda x: np.nan if x == 9999 else x)
data['Num'].value_counts().sort_values(ascending = True)
fig, ax = plt.subplots(figsize = (10, 12))
ax = sns.histplot(data, x = 'Num', hue = 'Transported', binwidth = 20)
ax.vlines(300, ymin=0, ymax=50, color='black')
ax.vlines(600, ymin=0, ymax=50, color='black')
ax.vlines(900, ymin=0, ymax=50, color='black')
ax.vlines(1200, ymin=0, ymax=50, color='black')
ax.vlines(1500, ymin=0, ymax=50, color='black')
ax.vlines(1800, ymin=0, ymax=50, color='black')
ax.set_title('Cabin number')
ax.set_xlim([0,2000])
data['Num_code_1'] = (data['Num']<300).astype(int)   # one-hot encoding
data['Num_code_2'] =((data['Num']>=300) & (data['Num']<600)).astype(int)
data['Num_code_3'] =((data['Num']>=600) & (data['Num']<900)).astype(int)
data['Num_code_4'] =((data['Num']>=900) & (data['Num']<1200)).astype(int)
data['Num_code_5'] =((data['Num']>=1200) & (data['Num']<1500)).astype(int)
data['Num_code_6'] =((data['Num']>=1500) & (data['Num']<1800)).astype(int)
data['Num_code_7'] =(data['Num']>=1800).astype(int)
data.loc[data['Num'].isnull(), 'Num_code_1'] = 1
V5_train = data[data['Transported'].notnull()]
model_V5 = RandomForestClassifier(n_estimators = 250, random_state = 0, min_samples_split = 20, oob_score = True)
model_V5.fit(V5_train[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet+Destination_code', 'Desk', 'Side', 'Num_code_1', 'Num_code_2', 'Num_code_3','Num_code_4','Num_code_5','Num_code_6','Num_code_7']], V5_train['Transported'])
print(f"oob score: {model_V5.oob_score_}")
V5_test = data[data['Transported'].isnull()]
V5_output = model_V5.predict(V5_test[['CryoSleep', 'Age_code', 'Total_Cost', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet+Destination_code', 'Desk', 'Side', 'Num_code_1', 'Num_code_2', 'Num_code_3','Num_code_4','Num_code_5','Num_code_6','Num_code_7']]).astype(bool)
submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': V5_output})
submission.to_csv('V5_Submission.csv', index = False)
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import KNNImputer
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from catboost import CatBoostRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings(action='ignore', category=ConvergenceWarning)
def data_read():
train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv') #Read Train Data
test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv') #Read Test Data
subsample = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/sample_submission.csv') #Read Sample Submission Data
return train, test, subsample

train, test, subsmaple = data_read()
train.head(3) # Let's see train data with head fu
test.head(3) # Let's see test data with head funciton
def sale_plot():
plt.figure(figsize=(18, 6))    # Create figure to analysis graphs.

sns.histplot(train['SalePrice'], bins=100) # Creating Histogram Plot for see crosses in dataset.
plt.xlabel('Selling price ($)', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('Saleprice Rate', fontdict={'fontsize': 11, 'fontweight': 'bold'})

plt.show();

print(train['SalePrice'].describe())

sale_plot()
#Eitim veri seti iin yeni zellikler oluturma
train['Total_Top_Bath'] = train['FullBath'] + (train['HalfBath'] * 0.5)
train['TotalPorchSF'] = train['OpenPorchSF'] + train['EnclosedPorch'] + train['3SsnPorch'] + train['ScreenPorch']
train['GarageAge'] = train['YrSold'] - train['GarageYrBlt']
train['HouseAge'] = train['YrSold'] - train['YearBuilt']
train['TotalSF'] = (train['1stFlrSF'] + train['2ndFlrSF'] + train['TotalBsmtSF'] + train['GarageArea'] +
train['WoodDeckSF'] + train['OpenPorchSF'] + train['EnclosedPorch'] +
train['3SsnPorch'] + train['ScreenPorch'])

train['TotalBath'] = train['FullBath'] + (train['HalfBath'] * 0.5) + train['BsmtFullBath'] + (train['BsmtHalfBath'] * 0.5)
train['Total_Bot_Bath'] = train['BsmtFullBath'] + (train['BsmtHalfBath'] * 0.5)

train['HasGarage'] = train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)


#Test veri seti iin yeni zellikler oluturma
test['Total_Top_Bath'] = test['FullBath'] + (test['HalfBath'] * 0.5)
test['Total_Bot_Bath'] = test['BsmtFullBath'] + (test['BsmtHalfBath'] * 0.5)

test['HasGarage'] = test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
test['TotalSF'] = (test['1stFlrSF'] + test['2ndFlrSF'] + test['TotalBsmtSF'] + test['GarageArea'] +
test['WoodDeckSF'] + test['OpenPorchSF'] + test['EnclosedPorch'] +
test['3SsnPorch'] + test['ScreenPorch'])

test['HouseAge'] = test['YrSold'] - test['YearBuilt']
test['GarageAge'] = test['YrSold'] - test['GarageYrBlt']
test['TotalBath'] = test['BsmtFullBath'] + (test['BsmtHalfBath'] * 0.5) + test['FullBath'] + (test['HalfBath'] * 0.5)
test['TotalPorchSF'] = test['OpenPorchSF'] + test['EnclosedPorch'] + test['3SsnPorch'] + test['ScreenPorch']
columns_to_impute_train = []
columns_to_remove_train = []

columns_to_impute_test = []
columns_to_remove_test = []

for column in train.columns:
null_count = train[column].isnull().sum()
if null_count >= 500:
columns_to_remove_train.append(column)
elif null_count >= 1:
columns_to_impute_train.append(column)
else:
pass

for column in test.columns:
null_count = test[column].isnull().sum()
if null_count >= 500:
columns_to_remove_test.append(column)
elif null_count >= 1:
columns_to_impute_test.append(column)
else:
pass

print("Columns to impute in train ", columns_to_impute_train, '\n')
print("Columns to remove in train: ", columns_to_remove_train, '\n')

print("Columns to impute in test: ", columns_to_impute_test, '\n')
print("Columns to remove in test: ", columns_to_remove_test)
train_clean = train.drop(['Id','PoolQC','PoolArea','Fence', 'MiscFeature'], axis = 1)
test_clean = test.drop(['PoolQC','PoolArea','Fence', 'MiscFeature'], axis = 1)
cat_columns_train = train_clean.select_dtypes(include=['object'])
cat_columns_test = test_clean.select_dtypes(include=['object'])

print(cat_columns_train.columns)
label_encoder = LabelEncoder()

for columna in cat_columns_train.columns:
train_clean[columna] = label_encoder.fit_transform(train_clean[columna])

for columna in cat_columns_test.columns:
test_clean[columna] = label_encoder.fit_transform(test_clean[columna])

train_clean.info()
knn_imputer_train = KNNImputer(n_neighbors=5, metric='nan_euclidean')
knn_imputer_train.fit(train_clean[columns_to_impute_train])

train_clean[columns_to_impute_train] = knn_imputer_train.transform(train_clean[columns_to_impute_train])


knn_imputer_test = KNNImputer(n_neighbors=5, metric='nan_euclidean')
knn_imputer_test.fit(test_clean[columns_to_impute_test])

test_clean[columns_to_impute_test] = knn_imputer_test.transform(test_clean[columns_to_impute_test])


print("No. of nulls in the dataset: ", train_clean.isnull().sum().sum())
correlations = train_clean.corr()
corr = train_clean.corr()
corr_sale = corr['SalePrice'].sort_values(ascending=False)

plt.figure(figsize=(18, 14))

plt.barh(corr_sale.index, corr_sale.values)
plt.xlabel("Correlation", size=12)
plt.ylabel("")
plt.title("Relationship of the variables with  SalePrice", fontdict={'fontsize': 11, 'fontweight': 'bold'})
plt.gca().invert_yaxis()

plt.show()
corr_matrix = train_clean.corr()

saleprice_corr = corr_matrix['SalePrice']

threshold = 0.50
high_corr_vars = saleprice_corr[abs(saleprice_corr) > threshold]


for var, corr_value in zip(high_corr_vars.index, high_corr_vars.values):
print(f"{var} and SalePrice Correlation value: {corr_value:.2f}")
street_count = train['Street'].value_counts()
print(street_count)
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

sns.histplot(data=train_clean, x='OverallQual', ax=axes[0], bins=range(1, 11), kde=True)
axes[0].set_xlabel('Quality level', size=12)
axes[0].set_ylabel('Frecuency', size=12)
axes[0].set_title('Distribution of qualities', size=11, weight='bold')

sns.barplot(data=train_clean, x='OverallQual', y='SalePrice', ax=axes[1])
axes[1].set_xlabel('Quality level', size=12)
axes[1].set_ylabel('Selling price', size=12)
axes[1].set_title('Relationship between quality and sales price', size=11, weight='bold')

plt.tight_layout()
plt.show()
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

sns.histplot(data=train_clean, x='GrLivArea', ax=axes[0], bins=30, kde=True)
axes[0].set_xlabel('Ft2', size=12)
axes[0].set_ylabel('Frecuency', size=12)
axes[0].set_title('Square footage distribution ', size=11, weight='bold')

sns.scatterplot(data=train, x='HouseStyle', y='SalePrice', ax=axes[1])
axes[1].set_xlabel('Ft2', size=12)
axes[1].set_ylabel('Selling price', size=12)
axes[1].set_title('Relationship between square footage and sales price', size=11, weight='bold')

plt.tight_layout()
plt.show()
fig, axes = plt.subplots(1, 3, figsize=(16, 6))

sns.histplot(data=train_clean, x='GarageCars', ax=axes[0], bins=range(6), kde=True)
axes[0].set_xlabel('Parking spaces', size=12)
axes[0].set_ylabel('Frecuency', size=12)
axes[0].set_title('distribution of the number of parking spaces ', size=11, weight='bold')

sns.histplot(data=train_clean, x='GarageArea', ax=axes[1], bins=30, kde=True)
axes[1].set_xlabel('Ft2', size=12)
axes[1].set_ylabel('Frecuency', size=12)
axes[1].set_title('Square footage distribution', size=11, weight='bold')

sns.scatterplot(data=train_clean, x='GarageCars', y='GarageArea', hue='SalePrice', ax=axes[2])
axes[2].set_xlabel('Parking spaces', size=12)
axes[2].set_ylabel('Ft2', size=12)
axes[2].set_title('Relationship between number of seats, size and selling price', size=11, weight='bold')

plt.tight_layout()
plt.show()
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

sns.histplot(data=train_clean, x='TotalBsmtSF', ax=axes[0], bins=30, kde=True)
axes[0].set_xlabel('Ft2', size=12)
axes[0].set_ylabel('Frecuency', size=12)
axes[0].set_title('Square footage distribution', size=11, weight='bold')

sns.scatterplot(data=train_clean, x='TotalBsmtSF', y='SalePrice', ax=axes[1])
axes[1].set_xlabel('Ft2', size=12)
axes[1].set_ylabel('Selling price', size=12)
axes[1].set_title('Selling Price and TotalBstmF', size=11, weight='bold')

plt.tight_layout()
plt.show()
X_train = train_clean[['OverallQual']]
y_train = train_clean['SalePrice']
X_test = test_clean[['OverallQual']]

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

simple_predictions = lr_model.predict(X_test)

rmse_simple = np.sqrt(mean_squared_error(y_train, lr_model.predict(X_train)))
r2_simple = r2_score(y_train, lr_model.predict(X_train))
plt.figure(figsize=(19, 6))
plt.scatter(y_train, lr_model.predict(X_train), alpha=0.6)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)
plt.xlabel('')
plt.ylabel('')
plt.title('Simple Linear Regression')
plt.show()


print(f"Simple Linear Regression RMSE: {rmse_simple}")
print(f"Simple Linear Regression  R-squared: {r2_simple}")
X = train_clean.drop(columns=['SalePrice'])
y = train_clean['SalePrice']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

test_clean_aligned = test_clean[X.columns]
test_clean_scaled = scaler.transform(test_clean_aligned)
test_predictions = model.predict(test_clean_scaled)

rmse_multiple = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred)))
r2_multiple = r2_score(y_test, y_pred)
plt.figure(figsize=(19, 6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('')
plt.ylabel('')
plt.title('Multiple Linear Regression')
plt.show()

print(f"Multiple Linear Regression RMSE: {rmse_multiple}")
print(f"Multiple Linear Regression R-squared: {r2_multiple}")
ridge = Ridge()
ridge_params = {'alpha': [0.01, 0.1, 1, 10,100]}
ridge_grid = GridSearchCV(ridge, ridge_params, cv=10, scoring='neg_mean_squared_error')
ridge_grid.fit(X_train, y_train)
best_ridge = ridge_grid.best_estimator_

y_pred_ridge = best_ridge.predict(X_test)
test_predictions_ridge = best_ridge.predict(test_clean_scaled)

rmse_ridge = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_ridge)))
r2_ridge = r2_score(y_test, y_pred_ridge)

print(f"Ridge Regression RMSE: {rmse_ridge}")
print(f"Ridge Regression R-squared: {r2_ridge}")
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_test)

knn_rmse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_knn)))
knn_r2 = r2_score(y_test, y_pred_knn)

print(f"KNN Regression RMSE: {knn_rmse}")
print(f"KNN Regression R-squared: {knn_r2}")
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
elastic_net.fit(X_train, y_train)


y_pred_en = elastic_net.predict(X_test)


en_rmse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_en)))
en_r2 = r2_score(y_test, y_pred_en)

print(f"ElasticNet RMSE: {en_rmse}")
print(f"ElasticNet R-squared: {en_r2}")
gb = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, random_state=42)
gb.fit(X_train, y_train)


y_pred_gb = gb.predict(X_test)


gb_rmse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_gb)))
gb_r2 = r2_score(y_test, y_pred_gb)

print(f"Gradient Boosting RMSE: {gb_rmse}")
print(f"Gradient Boosting R-squared: {gb_r2}")
catb = CatBoostRegressor()
catb_model = catb.fit(X_train, y_train,
verbose = 0)

y_pred_catb = catb_model.predict(X_test)

catb_rmse_calculator = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_catb)))
catboost_r2_metric = r2_score(y_test, y_pred_catb)

print(f"Category Boosting RMSE Metric: {catb_rmse_calculator}")
print(f"Category Boosting R-squared Metric: {catboost_r2_metric}")
from xgboost import XGBRegressor
xgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,
max_depth=3, min_child_weight=0,
gamma=0, subsample=0.7,
colsample_bytree=0.7,
#objective='reg:linear', nthread=-1,
objective='reg:squarederror', nthread=-1,
scale_pos_weight=1, seed=27,
reg_alpha=0.00005)
xgb_model = xgb.fit(X_train , y_train)

y_pred_xgboost = xgb_model.predict(X_test)

xgboost_rmse_calculator = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred_xgboost)))
xgboost_r2_metric = r2_score(y_test, y_pred_xgboost)

print(f"Extreme Gradient Boosting RMSE Metric: {xgboost_rmse_calculator}")
print(f"Extreme Gradient Boosting R-squared Metric: {xgboost_r2_metric}")
test_pred_xgboost = xgb_model.predict(test_clean_scaled)

submission = pd.DataFrame({
'Id': test['Id'],
'SalePrice': test_pred_xgboost
})

print(submission.head())
submission.to_csv('submission_xg.csv', index=False)
test_pred_xgb = xgb_model.predict(test_clean_scaled)

submission = pd.DataFrame({
'Id': test['Id'],
'SalePrice': test_pred_xgb
})

print(submission.head())
submission.to_csv('submission.csv', index=False)
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "/share/dutta/eyao/dataset/kaggle/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LassoCV, LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv')
df.shape # 1460 Columns, 81 Rows
df.head(5)
df.describe()
df_na = df.isna().sum().to_frame().reset_index()
df_na.columns = ["Column Name", "Null Count"]
df_na["Null Percentage"] = round((df_na["Null Count"] / len(df))*100, 2)
df_na
df_many_na_columns = df_na[df_na["Null Count"] >= 200]["Column Name"].tolist()
df_many_na_columns
df_v1 = df.drop(columns=df_many_na_columns, axis=1)
df_v1.shape # 1460 Rows 74 Columns (81 Columns -> 74 Columns)
df_few_na_columns = df_na[(df_na["Null Count"] < 200) & (df_na["Null Count"] > 0)]["Column Name"].tolist()
df_na[df_na["Column Name"].isin(df_few_na_columns)]
df_v2 = df_v1.dropna()
df_v2.shape # 1338 Rows 74 Columns (1460 Rows -> 1338 Rows)
df_v3 = df_v2.drop("Id", axis=1)
df_v3.shape # 1338 Rows 73 Columns (75 Columns -> 73 Columns)
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_v3, x="Utilities", y="SalePrice")
plt.title("House Prices by Utilities")
plt.show()
df_v4 = df_v3.drop("Utilities", axis = 1)
df_v4.shape # 1338 Rows 72 Columns (74 Columns -> 72 Columns)
print(df_v4.sort_values(by="SalePrice", ascending=False)["SalePrice"].head(10))
print(df_v4.sort_values(by="SalePrice", ascending=True)["SalePrice"].head(10))
expensive_top2 = df_v4.sort_values(by="SalePrice", ascending=False).head(2).index
cheap_top2 = df_v4.sort_values(by="SalePrice", ascending=True).head(2).index
drop_index = expensive_top2.union(cheap_top2)

df_v5 = df_v4.drop(drop_index)
df_v5.shape # 1334 Rows 72 Columns (1338 Rows -> 1334 Rows)
# Numerical Variables
numerical_columns = df_v5.select_dtypes(include=np.number).columns.tolist()
print(numerical_columns)
print(len(numerical_columns)) # 36 Columns
corr_matrix = df_v5[numerical_columns].corr()
# Low Correaltion ( < 0.2)
low_corr = corr_matrix[corr_matrix["SalePrice"].abs() < 0.2].index.tolist()
print(low_corr)
print(len(low_corr)) # 15 Columns
df_v6 = df_v5.drop(columns=low_corr, axis=1)
df_v6.shape # 1334 Rows 57 Columns (73 Columns -> 57 Columns)
# high Correlation ( >= 0.2)
high_corr = corr_matrix[corr_matrix["SalePrice"].abs() >= 0.2].index.tolist()
print(high_corr)
print(len(high_corr)) # 21 Columns
high_correlation = df_v6[high_corr].corr()

plt.figure(figsize=(12, 9))
sns.heatmap(high_correlation, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.title("High Correlation Heatmap")
plt.show()
df_v7 = df_v6.drop(columns=["TotalBsmtSF", "GarageYrBlt", "TotRmsAbvGrd"], axis=1)
df_v7.shape # 1334 Rows 54 Columns (58 columns -> 54 columns)
plt.figure(figsize=(10,6))
sns.histplot(df_v7["SalePrice"], kde=True, bins=50)
plt.title("Distribution of SalePrice")
plt.grid(True)
plt.show()

# Right-Skewed
df_v7["Log_SalePrice"] = np.log(df_v7["SalePrice"])
df_v7.shape # 1334 Rows 55 Columns (54 Columns -> 55 Columns)
# After Log Transformation
plt.figure(figsize=(10,6))
sns.histplot(df_v7["Log_SalePrice"], kde=True, bins=50)
plt.title("Distribution of Log_SalePrice")
plt.grid(True)
plt.show()
# Categorical Variables
categorical_columns = df_v7.select_dtypes(include=['object']).columns.tolist()
print(categorical_columns)
print(len(categorical_columns)) # 36
df_v8 = pd.get_dummies(df_v7, columns=categorical_columns, drop_first=True)
df_v8.shape # 1334 Rows 206 Columns (56 Columns -> 206 Columns)
# Dataset
X = df_v8.drop(columns=["SalePrice", "Log_SalePrice"], axis=1)
y = df_v8["Log_SalePrice"]

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)
# LassoCV
alphas = np.logspace(-4, 4, 50)
lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000, random_state=42)
lasso_cv.fit(X_train_scaled, y_train)
print(f"Optimal Alpha Value: {lasso_cv.alpha_}") # Optimal Alpha Value: 0.004291934260128779
remove_features = np.array(X.columns)[lasso_cv.coef_ == 0]
print(remove_features)
print(len(remove_features)) # 109
df_lasso = df_v8.drop(columns=remove_features, axis=1)
df_lasso.shape # 1334 Rows 97 Columns (206 Columns -> 97 Columns)
# Data
X = df_lasso.drop(columns = ["SalePrice", "Log_SalePrice"])
y = df_lasso["Log_SalePrice"]

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)
# Model
linear_model = LinearRegression()

# Cross Validation
cv_score = cross_val_score(linear_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')

# Training
linear_model.fit(X_train_scaled, y_train)
y_pred = linear_model.predict(X_test_scaled)
linear_mse = mean_squared_error(y_test, y_pred)
linear_r2 = r2_score(y_test, y_pred)
print(linear_mse) # 0.013932131659777207
print(linear_r2) # 0.8852401953026643
# parameters
alphas = [0.001, 0.01, 0.1, 1, 10]
best_alpha = None
best_score = float('inf')

for alpha in alphas:
lasso = Lasso(alpha=alpha)
cv_scores = cross_val_score(lasso, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
mean_cv_scores = -cv_scores.mean()

if mean_cv_scores < best_score:
best_score = mean_cv_scores
best_alpha = alpha
print(best_alpha) # 0.001
print(best_score) # 0.019635386910392665
# Best Lasso Model
lasso_best = Lasso(alpha=best_alpha)
lasso_best.fit(X_train_scaled, y_train)

y_pred = lasso_best.predict(X_test_scaled)
lasso_mse = mean_squared_error(y_test, y_pred)
lasso_r2 = r2_score(y_test, y_pred)
print(lasso_mse) # 0.013429016176088117
print(lasso_r2) # 0.8893843877391352
ridge = Ridge()
parameters = {'alpha': alphas}

ridge_model = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)
ridge_model.fit(X_train_scaled, y_train)
best_ridge = ridge_model.best_estimator_
y_pred = best_ridge.predict(X_test_scaled)
ridge_mse = mean_squared_error(y_test, y_pred)
ridge_r2 = r2_score(y_test, y_pred)
print(ridge_mse) # 0.013824855747062038
print(ridge_r2) # 0.8861238334344703
l1_ratios = [0.2, 0.5, 0.8]
elastic_net = ElasticNet()
parameters = {'alpha':alphas, 'l1_ratio': l1_ratios}

elastic_net_model = GridSearchCV(elastic_net, parameters, scoring='neg_mean_squared_error', cv=5)
elastic_net_model.fit(X_train_scaled, y_train)
best_elastic_net = elastic_net_model.best_estimator_
y_pred = best_elastic_net.predict(X_test_scaled)
elastic_net_mse = mean_squared_error(y_test, y_pred)
elastic_net_r2 = r2_score(y_test, y_pred)
print(elastic_net_mse) # 0.013513780328747907
print(elastic_net_r2) # 0.888686180326076
svr = SVR()
parameters = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}
svr_model = GridSearchCV(svr, parameters, scoring='neg_mean_squared_error', cv=5)
svr_model.fit(X_train_scaled, y_train)
best_svr = svr_model.best_estimator_
y_pred = best_svr.predict(X_test_scaled)
svr_mse = mean_squared_error(y_test, y_pred)
svr_r2 = r2_score(y_test, y_pred)
print(svr_mse) # 0.012982683368125891
print(svr_r2) # 0.8930608578675101
final_results = pd.DataFrame({"Model": ["Linear Regression", "Lasso", "Ridge", "Elastic Net", "SVM"],
"MSE": [linear_mse, lasso_mse, ridge_mse, elastic_net_mse, svr_mse],
"R-Squared": [linear_r2, lasso_r2, ridge_r2, elastic_net_r2, svr_r2]})
final_results
# Visualization
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.bar(final_results["Model"], final_results["MSE"], color='skyblue')
plt.title("MSE Score per Model")
plt.ylabel("MSE")
plt.xticks(rotation=45)
mse_min, mse_max = final_results["MSE"].min(), final_results["MSE"].max()
plt.ylim(mse_min - 0.0001, mse_max + 0.0001)

plt.subplot(1, 2, 2)
plt.bar(final_results["Model"], final_results["R-Squared"], color='salmon')
plt.title("R-Squared Score per Model")
plt.ylabel('R-Squared')
plt.xticks(rotation=45)
r2_min, r2_max = final_results["R-Squared"].min(), final_results["R-Squared"].max()
plt.ylim(r2_min - 0.005, r2_max + 0.005)

plt.tight_layout()
plt.show()
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

plt.style.use("dark_background")
warnings.filterwarnings("ignore")
# ds = pd.read_csv("/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv")

ds = pd.read_csv("spaceship-titanic-data/train.csv")

ds
ts = pd.read_csv("/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv")

ss = pd.read_csv("/share/dutta/eyao/dataset/kaggle/spaceship-titanic/sample_submission.csv")

ts = pd.merge(ts, ss, on='PassengerId')

ts
ds.info()
ds.isnull().sum()
ds.isna().sum()
ds.describe().T
ds.drop(columns=["PassengerId", "Name"], axis=1, inplace=True)
ts.drop(columns=["PassengerId", "Name"], axis=1, inplace=True)
spe = ds
spe.dropna(inplace=True)

numCol = spe.select_dtypes('number').columns

spe
fig, ax = plt.subplots(4, 3, figsize=(20, 15))

for idx, (col, ax) in enumerate(zip([*numCol, *numCol], ax.flatten())):
if len(numCol) <= idx:
sns.boxplot(y=spe[col], ax=ax)
else:
sns.kdeplot(x=spe[col], ax=ax, fill=True)
from sklearn.preprocessing import LabelEncoder

for col in ds.select_dtypes(include=[object, bool]):
ds[col] = LabelEncoder().fit_transform(ds[col])

for col in ts.select_dtypes(include=[object, bool]):
ts[col] = LabelEncoder().fit_transform(ts[col])

ds
ds.isnull().sum()
for col in ds.columns:
ds[col].fillna = np.median(ds[col])

for col in ts.columns:
ts[col].fillna = np.median(ts[col])

ds
ds.Transported.value_counts()
from sklearn.preprocessing import MinMaxScaler

y = ds.Transported.values

ds = pd.DataFrame(MinMaxScaler().fit_transform(ds), columns=ds.columns)

ds.Transported = y

y = ts.Transported.values

ts = pd.DataFrame(MinMaxScaler().fit_transform(ts), columns=ts.columns)

ts.Transported = y

ds
from sklearn.model_selection import train_test_split

x_train, x_val, y_train, y_val = train_test_split(ds.iloc[:, :-1], ds.iloc[:, -1], train_size=.8, random_state=10)

x_train.shape, x_val.shape, y_train.shape, y_val.shape
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

model = Sequential([
Dense(x_train.shape[1], activation="relu"),
Dense(24, activation="relu"),
Dense(12, activation="relu"),
Dense(10, activation="relu"),
Dense(1, activation="sigmoid")
])

model.compile(loss="binary_crossentropy", optimizer=Adam(.03), metrics=["Accuracy"])

history = model.fit(x_train, y_train, epochs=50, validation_batch_size=64, validation_data=(x_val, y_val))
loss = history.history['loss']
acc = history.history['Accuracy']

fig, ax = plt.subplots(1, 2, figsize=(20, 5))

ax[0].plot(acc)
ax[0].set_xlabel("Accuracy")

ax[1].plot(loss)
ax[1].set_xlabel("Loss")
loss = history.history['val_loss']
acc = history.history['val_Accuracy']

fig, ax = plt.subplots(1, 2, figsize=(20, 5))

ax[0].plot(acc)
ax[0].set_xlabel("Accuracy")

ax[1].plot(loss)
ax[1].set_xlabel("Loss")
yPred = model.predict(ts.iloc[:, :-1])

yPred = pd.Series(np.ravel(yPred) < .5)

yPred.value_counts()
ss["Transported"] = yPred

ss
ss.to_csv('submission.csv', index = False)
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "/share/dutta/eyao/dataset/kaggle/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import LabelEncoder


# Load the training data
train_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')

# Data Preprocessing
train_data.fillna(0, inplace=True)  # Replace NaN values with 0 for simplicity

# Convert categorical variables to numerical representations
le = LabelEncoder()
train_data['CryoSleep'] = le.fit_transform(train_data['CryoSleep'])
# Repeat the above step for other categorical variables if needed

# Feature selection
features = ['CryoSleep', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']
X = train_data[features]
y = train_data['Transported']

# Split the data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)

# Build a small neural network model
model = keras.Sequential([
layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
layers.Dense(8, activation='relu'),
layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_valid, y_valid))

# Model Evaluation on Validation Set
y_pred_prob = model.predict(X_valid)
y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions
accuracy = accuracy_score(y_valid, y_pred)
print(f"Validation Accuracy: {accuracy}")

# Predictions on Test Set
test_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
# Perform the same data preprocessing steps on the test data
test_data.fillna(0, inplace=True)
test_data['CryoSleep'] = le.transform(test_data['CryoSleep'])
# Repeat the above steps for other categorical variables if needed

X_test = test_data[features]
X_test = scaler.transform(X_test)

# Make predictions
test_predictions_prob = model.predict(X_test)
test_predictions = (test_predictions_prob > 0.5).astype(int)  # Convert probabilities to binary predictions

# Prepare submission file
submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Transported': test_predictions.flatten()})
submission.to_csv('submission_small_neural_network.csv', index=False)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
train = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
train
test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
test
imputer = SimpleImputer(strategy='median')
train[['Age']] = imputer.fit_transform(train[['Age']])
train['Age'].fillna(train['Age'].median())
train['HomePlanet'].fillna('Unknown')
train['CryoSleep'].fillna(False)
categorical_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']
train = pd.get_dummies(train, columns=categorical_cols)
train['Total_Billed'] = train[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)
X = train.drop(['PassengerId', 'Name', 'Cabin', 'Transported'], axis=1)
y = train['Transported'].astype(int)
X.fillna(0, inplace=True)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_val)
print(f"Validation Accuracy: {accuracy_score(y_val, y_pred)}")
test[['Age']] = imputer.transform(test[['Age']])
test['HomePlanet'].fillna('Unknown')
test['CryoSleep'].fillna(False)
test['VIP'].fillna(False)
test = pd.get_dummies(test, columns=categorical_cols)
test['Total_Billed'] = test[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)
X_test = test.drop(['PassengerId', 'Name', 'Cabin'], axis=1)
X_test = X_test.reindex(columns=X.columns, fill_value=0)

# Check for any remaining NaN values in X_test and handle them
X_test.fillna(0, inplace=True)
test_preds = model.predict(X_test)
submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Transported': test_preds})
submission.to_csv('submission.csv', index=False)

pip install lazypredict
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder , FunctionTransformer
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression , SGDClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from lazypredict.Supervised import LazyClassifier
from lightgbm import LGBMClassifier
import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))
data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
test_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
data.head()
data.info()
data.describe()
data.isna().sum()
testing= test_data
data[['deck', 'num' , 'side']] = data['Cabin'].str.split('/', expand=True)[[0 , 1,  2]]
testing[['deck', 'num' , 'side']] = data['Cabin'].str.split('/', expand=True)[[0 , 1,  2]]
data['Group'] = data['PassengerId'].str.split('_' , expand = True)[0].astype(int)
test_data['Group'] = test_data['PassengerId'].str.split('_' , expand = True)[0].astype(int)

def wow (x):
return x.astype(int)

fun = FunctionTransformer(wow)
X= data.drop(columns= ['Transported'])
y = data['Transported']

X_train , X_test , y_train , y_test =  train_test_split(X , y , test_size=0.15, random_state=42)


numeric_transformer = Pipeline(steps=[
('imputer', SimpleImputer(strategy='mean')),
('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
('imputer', SimpleImputer(strategy='most_frequent')),
('onehot', OneHotEncoder(handle_unknown='ignore'))])

boolean_transformer = Pipeline(steps=[
('imputer', SimpleImputer(strategy='most_frequent')) ,
("bool_to_int" ,fun)
])

numeric_features = ["Spa", "Group" , "FoodCourt" , "VRDeck" , "RoomService" , "Age" , "ShoppingMall" , 'num']
categorical_features = ["deck" ,"side" , "HomePlanet" , "Destination"]
boolean_features = ["CryoSleep" , "VIP"]
preprocessor = ColumnTransformer(
transformers=[
('num', numeric_transformer, numeric_features),
('cat', categorical_transformer, categorical_features),
('bool', boolean_transformer, boolean_features)
])


preprocessor.fit(X_train)


X_train_preprocessed = preprocessor.transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)


preprocessor.fit(testing)
X_test2_preprocessed = preprocessor.transform(testing)

ohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)


feature_names = list(numeric_features) + list(boolean_features)  + list(ohe_feature_names)


X_train = pd.DataFrame(X_train_preprocessed, columns=feature_names)
X_test = pd.DataFrame(X_test_preprocessed, columns=feature_names)
testing = pd.DataFrame(X_test2_preprocessed, columns=feature_names)

X_train['num'] = X_train['num'].astype(int)
X_test['num'] =X_test['num'].astype(int)

testing['num']=testing['num'].astype(int)


X_train.head()
X_train

type(X_train['num'][10])
z = X_train
z['Transported'] = y_train.to_list()
X_train = X_train.drop(columns=['Transported'])
z.isna().sum()
hm = z.corr()
plt.figure(figsize=(16 , 9))
sb.heatmap(hm , annot=True , fmt=".2f"  )
plt.show()
bst = XGBClassifier(n_estimators=40, max_depth=6, learning_rate=0.1,min_child_weight= 5,subsample = 1,colsample_bytree= 0.75)
bst.fit(X_train , y_train)
y_pred = bst.score(X_test,y_test)
y_pred
models = {
'LogisticRegression': {
'model': LogisticRegression(),
'params': {
'C': [0.1, 1, 10, 100]
}
},
'KNeighborsClassifier': {
'model': KNeighborsClassifier(),
'params': {
'n_neighbors': [3, 5, 7, 9]
}
},
'SVC': {
'model': SVC(),
'params': {
'C': [0.1, 1, 10, 100],
'gamma': [0.1, 0.01, 0.001, 0.0001]
}
},
'XGBClassifier': {
'model': XGBClassifier(),
'params': {
'max_depth': [6, 8, 10],
'learning_rate': [0.01, 0.05, 0.1],
'n_estimators': [100, 200, 300]
}
},
'RandomForestClassifier': {
'model': RandomForestClassifier(),
'params': {
'n_estimators': [100, 200, 300],
'max_depth': [None, 5, 10, 15],
'min_samples_split': [2, 5, 10]
}
},
'GaussianNB': {
'model': GaussianNB(),
'params': {}
},
'GradientBoostingClassifier': {
'model': GradientBoostingClassifier(),
'params': {
'learning_rate': [0.01, 0.1, 1],
'n_estimators': [100, 200, 300],
'max_depth': [3, 5, 8]
}
},
'SGDClassifier': {
'model': SGDClassifier(),
'params': {
'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],
'penalty': ['l2', 'l1', 'elasticnet']
}
},
'DecisionTreeClassifier': {
'model': DecisionTreeClassifier(),
'params': {
'criterion': ['gini', 'entropy'],
'max_depth': [None, 5, 10, 15]
}
},
'LGBMClassifier': {
'model': LGBMClassifier(),
'params': {
'learning_rate': [0.01, 0.1, 1],
'n_estimators': [20, 40, 60, 80, 100],
'num_leaves': [31, 60, 90, 120]
}
}
}

best_params_scores = {}

for model_name, model_info in models.items():

grid_clf = GridSearchCV(model_info['model'], model_info['params'], cv=5)
grid_clf.fit(X_train, y_train)


best_params_scores[model_name] = {
'best_params': grid_clf.best_params_,
'best_score': grid_clf.best_score_
}

for model_name, params_score in best_params_scores.items():
print(f"Model: {model_name}")
print(f"Best parameters: {params_score['best_params']}")
print(f"Best score: {params_score['best_score']}\n")
lgb = {
'model': LGBMClassifier(),
'params': {
'learning_rate': [0.01, 0.1, 0.05, 1],
'n_estimators': [20, 40, 60, 80, 100],
'num_leaves': [31, 60, 90, 120],
'max_depth': [4, 6],
'colsample_bytree': [0.7, 0.8, 0.9],
'subsample': [0.7, 0.8, 0.9],
'min_child_samples': [1, 5, 10]
}
}

model = GridSearchCV(lgb['model'], lgb['params'], cv=5 ,
n_jobs=-1,
scoring='neg_root_mean_squared_error')
model.fit(X_train, y_train)

best_params = model.best_estimator_
print(best_params)
# lgb = LGBMClassifier(colsample_bytree=0.7, learning_rate=0.01, max_depth=4,
#                min_child_samples=1, n_estimators=20, subsample=0.7)


# lgbc.fit(X_train  , y_train)
# lgbc.score(X_test , y_test)

# y_pred = lgbc.predict(testing)
# test_data['Transported'] =y_pred
# final_test = test_data[['PassengerId' , 'Transported']]
# final_test.head()
# final_test.to_csv("submission.csv" , index=False)
clf = LazyClassifier(verbose = 0 , ignore_warnings = True)
models , predictions = clf.fit(X_train, X_test , y_train , y_test)
models
models = {
'lgbm' : LGBMClassifier(colsample_bytree=0.7, learning_rate=0.01, max_depth=4,
min_child_samples=1, n_estimators=20, subsample=0.7),
'random_forest' : RandomForestClassifier(max_depth =15 , min_samples_split=5 , n_estimators=300),
'svc' : SVC(C=10 , gamma=0.1),
'xgb' : XGBClassifier(learning_rate = 0.1 , max_depth=6 , n_estimators= 100)
}

for name , model in models.items() :
mod= model
mod.fit(X_train , y_train)
score=mod.score(X_test, y_test)
print(score)
# y_pred = mod.predict(testing)
# test_data['Transported'] = y_pred.astype(bool)
# final_test = test_data[['PassengerId' , 'Transported']]

# final_test.head()
# final_test.to_csv(f"{name}.csv" , index=False)
#  Problemi

# Her bir eve ait zelliklerin ve ev fiyatlarnn bulunduu veriseti kullanlarak,
# farkl tipteki evlerin fiyatlarna ilikin bir makine renmesi projesi
# gerekletirilmek istenmektedir.

# Business Problem

# Using a dataset of properties and house prices for each house,
# a machine learning project on the prices of different types of houses
# is intended to be realized.
# Veri Seti Hikayesi

# Ames, Lowadaki konut evlerinden oluan bu veri seti ierisinde 79 aklayc deiken bulunduruyor. Kaggle zerinde bir yarmas
# da bulunan projenin veri seti ve yarma sayfasna aadaki linkten ulaabilirsiniz. Veri seti bir kaggle yarmasna ait
# olduundan dolay train ve test olmak zere iki farkl csv dosyas vardr. Test veri setinde ev fiyatlar bo braklm olup, bu
# deerleri sizin tahmin etmeniz beklenmektedir


# Dataset Story

# This dataset of residential homes in Ames, Iowa contains 79 explanatory variables. A contest on Kaggle
# You can access the dataset and the competition page of the project from the link below. The dataset belongs to a kaggle competition
# Therefore, there are two different csv files, train and test. In the test dataset, house prices are left blank and this
# you are expected to estimate the values
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.exceptions import ConvergenceWarning
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter("ignore", category=ConvergenceWarning)


pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
# Adm 1: Train ve Test veri setlerini okutup birletiriniz. Birletirdiiniz veri zerinden ilerleyiniz

test_df = pd.read_csv("/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv")
train_df = pd.read_csv("/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv")

df = pd.concat([train_df, test_df], ignore_index=True)

# reset_index() kullanarak indeksi sfrlayn
df.reset_index(drop=True, inplace=True)

df.head()
def check_df(dataframe, head=5):
print("##################### Shape #####################")
print(dataframe.shape)
print("##################### Types #####################")
print(dataframe.dtypes)
print("##################### Head #####################")
print(dataframe.head(head))
print("##################### Tail #####################")
print(dataframe.tail(head))
print("##################### NA #####################")
print(dataframe.isnull().sum())
print("##################### Quantiles #####################")
print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

# check_df(df)
# Adm 2: Numerik ve kategorik deikenleri yakalaynz
def grab_col_names(dataframe, cat_th=10, car_th=20):
"""
grab_col_names for given dataframe

:param dataframe:
:param cat_th:
:param car_th:
:return:
"""

cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]

num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
dataframe[col].dtypes != "O"]

cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
dataframe[col].dtypes == "O"]

cat_cols = cat_cols + num_but_cat
cat_cols = [col for col in cat_cols if col not in cat_but_car]

num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
num_cols = [col for col in num_cols if col not in num_but_cat]

print(f"Observations: {dataframe.shape[0]}")
print(f"Variables: {dataframe.shape[1]}")
print(f'cat_cols: {len(cat_cols)}')
print(f'num_cols: {len(num_cols)}')
print(f'cat_but_car: {len(cat_but_car)}')
print(f'num_but_cat: {len(num_but_cat)}')

# cat_cols + num_cols + cat_but_car = deiken says.
# num_but_cat cat_cols'un ierisinde zaten.
# dolaysyla tm u 3 liste ile tm deikenler seilmi olacaktr: cat_cols + num_cols + cat_but_car
# num_but_cat sadece raporlama iin verilmitir.

return cat_cols, cat_but_car, num_cols

cat_cols, cat_but_car, num_cols = grab_col_names(df)
# Adm 3: Gerekli dzenlemeleri yapnz. (Tip hatas olan deikenler gibi)

cat_cols += ["Neighborhood"]
# Adm 4: Numerik ve kategorik deikenlerin veri iindeki dalmn gzlemleyiniz.

def cat_summary(dataframe, col_name, plot=False):
print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),
"Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)}))

if plot:
sns.countplot(x=dataframe[col_name], data=dataframe)
plt.show()


for col in cat_cols:
cat_summary(df, col)
def num_summary(dataframe, numerical_col, plot=False):
quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
print(dataframe[numerical_col].describe(quantiles).T)

if plot:
dataframe[numerical_col].hist(bins=50)
plt.xlabel(numerical_col)
plt.title(numerical_col)
plt.show()

print("#####################################")


for col in num_cols:
num_summary(df, col, True)
# Adm 5: Kategorik deikenler ile hedef deiken incelemesini yapnz.

def target_summary_with_cat(dataframe, target, categorical_col):
print(pd.DataFrame({"TARGET_MEAN": dataframe.groupby(categorical_col)[target].mean()}), end="\n\n\n")


for col in cat_cols:
target_summary_with_cat(df,"SalePrice",col)
# Baml deikenin incelenmesi
df["SalePrice"].hist(bins=100)
plt.show()
# Baml deikenin logaritmasnn incelenmesi
np.log1p(df['SalePrice']).hist(bins=50)
plt.show()
corr = df[num_cols].corr()
corr

# Korelasyonlarn gsterilmesi
sns.set(rc={'figure.figsize': (12, 12)})
sns.heatmap(corr, cmap="RdBu")
plt.show()
# Adm 1: Eksik ve aykr gzlemler iin gerekli ilemleri yapnz.


# Aykr deerlerin basklanmas

def outlier_thresholds(dataframe, variable, low_quantile=0.10, up_quantile=0.90):
quantile_one = dataframe[variable].quantile(low_quantile)
quantile_three = dataframe[variable].quantile(up_quantile)
interquantile_range = quantile_three - quantile_one
up_limit = quantile_three + 1.5 * interquantile_range
low_limit = quantile_one - 1.5 * interquantile_range
return low_limit, up_limit

# Aykr deer kontrol
def check_outlier(dataframe, col_name):
low_limit, up_limit = outlier_thresholds(dataframe, col_name)
if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
return True
else:
return False


for col in num_cols:
if col != "SalePrice":
print(col, check_outlier(df, col))


# Aykr deerlerin basklanmas
def replace_with_thresholds(dataframe, variable):
low_limit, up_limit = outlier_thresholds(dataframe, variable)
dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit


for col in num_cols:
if col != "SalePrice":
replace_with_thresholds(df,col)
def missing_values_table(dataframe, na_name=False):
na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]

n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)

ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)

missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])

print(missing_df, end="\n")

if na_name:
return na_columns

missing_values_table(df)


df["Alley"].value_counts()
df["BsmtQual"].value_counts()


# Baz deikenlerdeki bo deerler evin o zellie sahip olmadn ifade etmektedir
no_cols = ["Alley","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","FireplaceQu",
"GarageType","GarageFinish","GarageQual","GarageCond","PoolQC","Fence","MiscFeature"]

# Kolonlardaki boluklarn "No" ifadesi ile doldurulmas
for col in no_cols:
df[col].fillna("No",inplace=True)

missing_values_table(df)



# Bu fonsksiyon eksik deerlerin median veya mean ile doldurulmasn salar

def quick_missing_imp(data, num_method="median", cat_length=20, target="SalePrice"):
variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]  # Eksik deere sahip olan deikenler listelenir

temp_target = data[target]

print("# BEFORE")
print(data[variables_with_na].isnull().sum(), "\n\n")  # Uygulama ncesi deikenlerin eksik deerlerinin says

# deiken object ve snf says cat_lengthe eit veya altndaysa bo deerleri mode ile doldur
data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == "O" and len(x.unique()) <= cat_length) else x, axis=0)

# num_method mean ise tipi object olmayan deikenlerin bo deerleri ortalama ile dolduruluyor
if num_method == "mean":
data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != "O" else x, axis=0)
# num_method median ise tipi object olmayan deikenlerin bo deerleri ortalama ile dolduruluyor
elif num_method == "median":
data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != "O" else x, axis=0)

data[target] = temp_target

print("# AFTER \n Imputation method is 'MODE' for categorical variables!")
print(" Imputation method is '" + num_method.upper() + "' for numeric variables! \n")
print(data[variables_with_na].isnull().sum(), "\n\n")

return data


df = quick_missing_imp(df, num_method="median", cat_length=17)
# Adm 2: Rare Encoder uygulaynz.


# Kategorik kolonlarn dalmnn incelenmesi

def rare_analyser(dataframe, target, cat_cols):
for col in cat_cols:
print(col, ":", len(dataframe[col].value_counts()))
print(pd.DataFrame({"COUNT": dataframe[col].value_counts(),
"RATIO": dataframe[col].value_counts() / len(dataframe),
"TARGET_MEAN": dataframe.groupby(col)[target].mean()}), end="\n\n\n")

rare_analyser(df, "SalePrice", cat_cols)
# Nadir snflarn tespit edilmesi
def rare_encoder(dataframe, rare_perc):
temp_df = dataframe.copy()

rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'
and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]

for var in rare_columns:
tmp = temp_df[var].value_counts() / len(temp_df)
rare_labels = tmp[tmp < rare_perc].index
temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])

return temp_df


rare_encoder(df,0.01)

df.head()
# Adm 3: Yeni deikenler oluturunuz.

df["NEW_1st*GrLiv"] = df["1stFlrSF"] * df["GrLivArea"]

df["NEW_Garage*GrLiv"] = (df["GarageArea"] * df["GrLivArea"])

# df["TotalQual"] = df[["OverallQual", "OverallCond", "ExterQual", "ExterCond", "BsmtCond", "BsmtFinType1",
# "BsmtFinType2", "HeatingQC", "KitchenQual", "Functional", "FireplaceQu", "GarageQual", "GarageCond", "Fence"]].sum(axis = 1) # 42

# Total Floor
df["NEW_TotalFlrSF"] = df["1stFlrSF"] + df["2ndFlrSF"] # 32

# Total Finished Basement Area
df["NEW_TotalBsmtFin"] = df.BsmtFinSF1 + df.BsmtFinSF2 # 56

# Porch Area
df["NEW_PorchArea"] = df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch + df["3SsnPorch"] + df.WoodDeckSF # 93

# Total House Area
df["NEW_TotalHouseArea"] = df.NEW_TotalFlrSF + df.TotalBsmtSF # 156

df["NEW_TotalSqFeet"] = df.GrLivArea + df.TotalBsmtSF # 35


# Lot Ratio
df["NEW_LotRatio"] = df.GrLivArea / df.LotArea # 64

df["NEW_RatioArea"] = df.NEW_TotalHouseArea / df.LotArea # 57

df["NEW_GarageLotRatio"] = df.GarageArea / df.LotArea # 69

# MasVnrArea
df["NEW_MasVnrRatio"] = df.MasVnrArea / df.NEW_TotalHouseArea # 36

# Dif Area
df["NEW_DifArea"] = (df.LotArea - df["1stFlrSF"] - df.GarageArea - df.NEW_PorchArea - df.WoodDeckSF) # 73


df["NEW_OverallGrade"] = df["OverallQual"] * df["OverallCond"] # 61


df["NEW_Restoration"] = df.YearRemodAdd - df.YearBuilt # 31

df["NEW_HouseAge"] = df.YrSold - df.YearBuilt # 73

df["NEW_RestorationAge"] = df.YrSold - df.YearRemodAdd # 40

df["NEW_GarageAge"] = df.GarageYrBlt - df.YearBuilt # 17

df["NEW_GarageRestorationAge"] = np.abs(df.GarageYrBlt - df.YearRemodAdd) # 30

df["NEW_GarageSold"] = df.YrSold - df.GarageYrBlt # 48



drop_list = ["Street", "Alley", "LandContour", "Utilities", "LandSlope","Heating", "PoolQC", "MiscFeature","Neighborhood"]

# drop_list'teki deikenlerin drlmesi
df.drop(drop_list, axis=1, inplace=True)

# df["NEW_MSZoning_LotShape"] = df.MSZoning + "_" + df["LotShape"]

# df["NEW_BldgType_LotConfig"] = df["BldgType"] + "_" + df["LotConfig"]

# df["NEW_MSZoning_Condition1"] = df["MSZoning"] + "_" + df["Condition1"]

# df["NEW_MSZoning_Condition2"] = df["MSZoning"] + "_" + df["Condition2"]

# df["NEW_MSZoning_BldgType"] = df["MSZoning"] + "_" + df.BldgType

# df["NEW_MSZoning_HouseStyle"] = df["MSZoning"] + "_" + df["HouseStyle"]

# df["NEW_MSZoning_RoofStyle"] = df["MSZoning"] + "_" + df["RoofStyle"]

# df["NEW_MSZoning_RoofMatl"] = df["MSZoning"] + "_" + df["RoofMatl"]

# df["NEW_MasVnrArea/YearBuilt*OverallQual"] = df["MasVnrArea"] / df["YearBuilt"] * df["OverallQual"]

# df["NEW_OverallQual*OverallCond-YearBuilt"] = df["OverallQual"] * df["OverallCond"] - df["YearBuilt"]

# df["NEW_LotFrontage+LotArea"] = df["LotFrontage"] + df["LotArea"]

# df["NEW_TotalBath"] = df["BsmtFullBath"] + df["BsmtHalfBath"] + df["FullBath"] + df["HalfBath"]

# df["NEW_Age"] = df["YrSold"] - df["YearBuilt"]

# df["NEW_Pool_Garage"] = df["GarageCars"] + df["PoolArea"]

# df["NEW_BldgType_HouseStyle"] = df["BldgType"] + "_" + df["HouseStyle"]

# df["NEW_RoofStyle_RoofMatl"] = df["RoofStyle"] + "_" + df["RoofMatl"]

# df["NEW_BedroomAbvGr_KitchenAbvGr"] = df["BedroomAbvGr"] + df["KitchenAbvGr"]

# df["NEW_GarageType_Finish"] = df["GarageType"] + "_" + df["GarageFinish"]

# df["NEW_GarageCars_GarageArea_GarageYrBlt"] = df["GarageYrBlt"] / df["GarageCars"] * df["GarageArea"]

# df["NEW_BsmtFinType1_BsmtFinType2"] = df["BsmtFinType1"] + "_" + df["BsmtFinType2"]

# df["NEW_PavedDrive_GarageQual"] = df["GarageQual"] + "_" + df["PavedDrive"]

# df["NEW_RoofStyle_HouseStyle"] = df["HouseStyle"] + "_" + df["RoofStyle"]

# df["NEW_Exterior1st_Exterior2nd"] = df["Exterior1st"] + "_" + df["Exterior2nd"]

# df["NEW_Fence_PavedDrive"] = df["PavedDrive"] + "_" + df["Fence"]
df.head()
# Adm 4: Encoding ilemlerini gerekletiriniz.

cat_cols, cat_but_car, num_cols = grab_col_names(df)
def label_encoder(dataframe, binary_col):
labelencoder = LabelEncoder()
dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])
return dataframe
binary_cols = [col for col in df.columns if df[col].dtypes == "O" and len(df[col].unique()) == 2]
for col in binary_cols:
label_encoder(df, col)
def one_hot_encoder(dataframe, categorical_cols, drop_first=False):
dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)
return dataframe

df = one_hot_encoder(df, cat_cols, drop_first=True)
for col in num_cols:
if col != "SalePrice":
print(col, check_outlier(df, col))

for col in num_cols:
if col != ["SalePrice", "Id"]:
replace_with_thresholds(df,col)

for col in num_cols:
if col != "SalePrice":
print(col, check_outlier(df, col))
"""def minmax_scaler(dataframe, num_cols):
minmaxscaler = MinMaxScaler()
dataframe[num_cols] = minmaxscaler.fit_transform(dataframe[num_cols].values.reshape(-1, 1))
return dataframe

num_cols = [col for col in df[num_cols] if col != "Id"]

for col in num_cols:
minmax_scaler(df, col)"""
# df["NEW_MasVnrRatio"] = df["NEW_MasVnrRatio"].fillna(df["NEW_MasVnrRatio"].median())
# Adm 1: Train ve Test verisini ayrnz. (SalePrice deikeni bo olan deerler test verisidir.
train_df = df[df['SalePrice'].notnull()]
test_df = df[df['SalePrice'].isnull()]
y = train_df['SalePrice'] # np.log1p(df['SalePrice'])
X = train_df.drop(["Id", "SalePrice"], axis=1)
# Adm 2: Train verisi ile model kurup, model baarsn deerlendiriniz

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)

lr = LinearRegression()

rmse = np.mean(np.sqrt(-cross_val_score(lr, X, y, cv=5, scoring="neg_mean_squared_error")))
rmse
knn = KNeighborsRegressor()
rmse = np.mean(np.sqrt(-cross_val_score(knn, X, y, cv=5, scoring="neg_mean_squared_error")))
rmse
CART = DecisionTreeRegressor()
rmse = np.mean(np.sqrt(-cross_val_score(CART, X, y, cv=5, scoring="neg_mean_squared_error")))
rmse
GBM = GradientBoostingRegressor()
rmse = np.mean(np.sqrt(-cross_val_score(GBM, X, y, cv=5, scoring="neg_mean_squared_error")))
rmse
LightGBM = LGBMRegressor()
rmse = np.mean(np.sqrt(-cross_val_score(LightGBM, X, y, cv=5, scoring="neg_mean_squared_error")))
rmse
"""
RMSE: 26139.034 (LR)
RMSE: 45084.477 (KNN)
RMSE: 37916.060 (CART)
RMSE: 25101.333 (GBM)
RMSE: 25188.703 (LightGBM)

"""
df["SalePrice"].mean()
# 180450.736
df["SalePrice"].std()
# 76826.747

##################
# BONUS : Log dnm yaparak model kurunuz ve rmse sonularn gzlemleyiniz.
# Not: Log'un tersini (inverse) almay unutmaynz.
##################

# Log dnmnn gerekletirilmesi


train_df = df[df["SalePrice"].notnull()]
test_df = df[df["SalePrice"].isnull()]

y = np.log1p(train_df["SalePrice"])
X = train_df.drop(["Id", "SalePrice"], axis=1)

# Verinin eitim ve tet verisi olarak blnmesi
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)

# lgbm_tuned = LGBMRegressor(**lgbm_gs_best.best_params_).fit(X_train, y_train)

lgbm = LGBMRegressor().fit(X_train, y_train)
y_pred = lgbm.predict(X_test)

y_pred
# Yaplan LOG dnmnn tersinin (inverse'nin) alnmas
new_y = np.expm1(y_pred)
new_y
new_y_test = np.expm1(y_test)
new_y_test

np.sqrt(mean_squared_error(new_y_test, new_y))

# RMSE: 25188.703 (LightGBM)
# RMSE: 24509.170 (LightGBM)
gbm = GradientBoostingRegressor().fit(X_train, y_train)
y_pred = gbm.predict(X_test)

y_pred
# Yaplan LOG dnmnn tersinin (inverse'nin) alnmas
new_y = np.expm1(y_pred)
new_y
new_y_test = np.expm1(y_test)
new_y_test

np.sqrt(mean_squared_error(new_y_test, new_y))
# RMSE: 25101.333 (GBM)
# RMSE: 24947.151 (GBM)
# hiperparametre optimizasyonlarn gerekletiriniz.

lgbm_model = LGBMRegressor(random_state=46)

rmse = np.mean(np.sqrt(-cross_val_score(lgbm_model, X, y, cv=5, scoring="neg_mean_squared_error")))
print(rmse)

lgbm_params = {"learning_rate": [0.01, 0.1],
"n_estimators": [500, 1500]
#"colsample_bytree": [0.5, 0.7, 1]
}

lgbm_gs_best = GridSearchCV(lgbm_model,
lgbm_params,
cv=3,
n_jobs=-1,
verbose=True).fit(X_train, y_train)
final_model = lgbm_model.set_params(**lgbm_gs_best.best_params_).fit(X, y)

rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring="neg_mean_squared_error")))
print(rmse)
# Deikenlerin nem dzeyini belirten feature_importance fonksiyonunu kullanarak zelliklerin sralamasn izdiriniz.

# feature importance
def plot_importance(model, features, num=len(X), save=False):

feature_imp = pd.DataFrame({"Value": model.feature_importances_, "Feature": features.columns})
plt.figure(figsize=(40, 40))
sns.set(font_scale=1)  # Yaz boyutunu drdk
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False)[0:num])
plt.title("Features")
plt.tight_layout()
plt.show()
if save:
plt.savefig("importances.png")

model = LGBMRegressor()
model.fit(X, y)

plot_importance(model, X)
test_df['Id'] = test_df['Id'].astype(int)
test_df['Id'].dtype
# test dataframeindeki bo olan salePrice deikenlerini tahminleyiniz ve
# Kaggle sayfasna submit etmeye uygun halde bir dataframe oluturunuz. (Id, SalePrice)

model = LGBMRegressor()
model.fit(X, y)
predictions = model.predict(test_df.drop(["Id", "SalePrice"], axis=1))

dictionary = {"Id": test_df['Id'], "SalePrice": predictions}
dfSubmission = pd.DataFrame(dictionary)
dfSubmission.to_csv("housePricePredictions.csv", index=False)
test_df.tail()
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "/share/dutta/eyao/dataset/kaggle/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import xgboost as xgb
from lightgbm import LGBMClassifier
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, log_loss
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

from sklearn.preprocessing import LabelEncoder
train_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')
test_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')
sample_submission_data = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/sample_submission.csv')
train_data.head()
test_data.head()
train_data.info()
train_data.isnull().sum()
train_data.shape
train_data.drop_duplicates(inplace = True)
train_data.shape
train_data.describe()
train_data['Age'].hist(figsize=(5,3), color='g')
train_data['HomePlanet'].value_counts()
sns.countplot(x = 'HomePlanet', hue = 'Transported' ,data = train_data)
train_data['CryoSleep'].value_counts()
sns.countplot(x='CryoSleep', hue='Transported',data=train_data)
train_data['Age'].mode()[0]
train_data['Age'].median()
for col_name in train_data.columns:
if train_data[col_name].dtypes=='object':
train_data[col_name] = train_data[col_name].fillna(train_data[col_name].mode()[0])
else:
train_data[col_name] = train_data[col_name].fillna(train_data[col_name].median())
print(train_data.shape)
for col_name in test_data.columns:
if test_data[col_name].dtypes=='object':
test_data[col_name] = test_data[col_name].fillna(test_data[col_name].mode()[0])
else:
test_data[col_name] = test_data[col_name].fillna(test_data[col_name].median())
print(test_data.shape)
encoder = LabelEncoder()
for col_name in train_data.columns:
if train_data[col_name].dtypes == 'object':
train_data[col_name] = encoder.fit_transform(train_data[col_name])
object_columns = test_data.select_dtypes(include='object').columns.difference(['PassengerId'])
encoder = LabelEncoder()
for col_name in object_columns:
if test_data[col_name].dtype == 'object':
test_data[col_name] = encoder.fit_transform(test_data[col_name])
plt.figure(figsize=(18,12))
sns.heatmap(train_data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
X_train = train_data.drop(['Transported','PassengerId','Name','ShoppingMall'], axis=1)
y_train = train_data['Transported']
X_test = test_data.drop(['PassengerId','Name','ShoppingMall'], axis=1)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
model_XGB = XGBClassifier(learning_rate = 0.1 , max_depth = 4, n_estimators = 100)
model_XGB.fit(X_train, y_train)
XGB_pred = model_XGB.predict(X_test)
# param_grid = {
#     'learning_rate': [0.01, 0.1, 0.2],
#     'max_depth': [3, 4, 5, 7, 8, 10],
#     'n_estimators': [50, 100, 150, 200]
# }
# grid_search = GridSearchCV(estimator = model_XGB,param_grid = param_grid, cv = 5, scoring = 'roc_auc' )
# grid_search.fit(X_train, y_train)
# best_params = grid_search.best_params_
# print("Best Hyperparameters:", best_params)
submission = pd.DataFrame({
'PassengerId': test_data.PassengerId,
'Transported': XGB_pred
})
submission['Transported'] = submission['Transported'].astype(bool)
print(submission.head())
submission.to_csv('submission.csv', index = False)
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "/share/dutta/eyao/dataset/kaggle/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):
for filename in filenames:
print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
train_data = pd.read_csv("/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/train.csv")
test_data = pd.read_csv("/share/dutta/eyao/dataset/kaggle/house-prices-advanced-regression-techniques/test.csv")
test_ids = test_data['Id']
print("Shape:", train_data.shape)
print("Duplicated data :", train_data.duplicated().sum())
fig, ax = plt.subplots(figsize=(25,10))
sns.heatmap(data=train_data.isnull(), yticklabels=False, ax=ax)
fig, ax = plt.subplots(figsize=(25,10))
sns.countplot(x=train_data['SaleCondition'])
sns.histplot(x=train_data['SaleType'], kde=True, ax=ax)
sns.violinplot(x=train_data['HouseStyle'], y=train_data['SalePrice'],ax=ax)
sns.scatterplot(x=train_data["Foundation"], y=train_data["SalePrice"], palette='deep', ax=ax)
plt.grid()
train_data['FireplaceQu'].fillna("No", inplace=True)
train_data['BsmtQual'].fillna("No", inplace=True)
train_data['BsmtCond'].fillna("No", inplace=True)
train_data['BsmtFinType1'].fillna("No", inplace=True)
train_data['BsmtFinType2'].fillna("No", inplace=True)
train_data['BsmtFinType2'].fillna("None", inplace=True)

def fill_all_missing_values(data):
for col in data.columns:
if((data[col].dtype == 'float64') or (data[col].dtype == 'int64')):
data[col].fillna(data[col].mean(), inplace=True)
else:
data[col].fillna(data[col].mode()[0], inplace=True)


fill_all_missing_values(train_data)
fill_all_missing_values(test_data)
drop_col = ['Id', 'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'MoSold', 'YrSold', 'MSSubClass',
'GarageType', 'GarageArea', 'GarageYrBlt', 'GarageFinish', 'YearRemodAdd', 'LandSlope',
'BsmtUnfSF', 'BsmtExposure', '2ndFlrSF', 'LowQualFinSF', 'Condition1', 'Condition2', 'Heating',
'Exterior1st', 'Exterior2nd', 'HouseStyle', 'LotShape', 'LandContour', 'LotConfig', 'Functional',
'BsmtFinSF1', 'BsmtFinSF2', 'FireplaceQu', 'WoodDeckSF', 'GarageQual', 'GarageCond', 'OverallCond'
]

train_data.drop(drop_col, axis=1, inplace=True)
test_data.drop(drop_col, axis=1, inplace=True)
from sklearn.preprocessing import OrdinalEncoder

ordinal_col = ['GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'ExterQual', 'ExterCond', 'KitchenQual', 'FireplaceQu',
'PavedDrive', 'Functional', 'Electrical', 'Heating', 'BsmtFinType1', 'BsmtFinType2', 'Utilities']

OE = OrdinalEncoder(categories=[['No', 'Po', 'Fa', 'TA', 'Gd', 'Ex']])
train_data['BsmtQual'] = OE.fit_transform(train_data[['BsmtQual']])
test_data['BsmtQual'] = OE.transform(test_data[['BsmtQual']])


OE = OrdinalEncoder(categories=[['No', 'Po', 'Fa', 'TA', 'Gd', 'Ex']])
train_data['BsmtCond'] = OE.fit_transform(train_data[['BsmtCond']])
test_data['BsmtCond'] = OE.transform(test_data[['BsmtCond']])


OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])
train_data['ExterQual'] = OE.fit_transform(train_data[['ExterQual']])
test_data['ExterQual'] = OE.transform(test_data[['ExterQual']])


OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])
train_data['ExterCond'] = OE.fit_transform(train_data[['ExterCond']])
test_data['ExterCond'] = OE.transform(test_data[['ExterCond']])


OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])
train_data['KitchenQual'] = OE.fit_transform(train_data[['KitchenQual']])
test_data['KitchenQual'] = OE.transform(test_data[['KitchenQual']])
OE = OrdinalEncoder(categories=[['N', 'P', 'Y']])
train_data['PavedDrive'] = OE.fit_transform(train_data[['PavedDrive']])
test_data['PavedDrive'] = OE.transform(test_data[['PavedDrive']])


OE = OrdinalEncoder(categories=[['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr']])
train_data['Electrical'] = OE.fit_transform(train_data[['Electrical']])
test_data['Electrical'] = OE.transform(test_data[['Electrical']])

OE = OrdinalEncoder(categories=[['No', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']])
train_data['BsmtFinType1'] = OE.fit_transform(train_data[['BsmtFinType1']])
test_data['BsmtFinType1'] = OE.transform(test_data[['BsmtFinType1']])
OE = OrdinalEncoder(categories=[['No', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']])
train_data['BsmtFinType2'] = OE.fit_transform(train_data[['BsmtFinType2']])
test_data['BsmtFinType2'] = OE.transform(test_data[['BsmtFinType2']])


OE = OrdinalEncoder(categories=[['ELO', 'NoSeWa', 'NoSewr', 'AllPub']])
train_data['Utilities'] = OE.fit_transform(train_data[['Utilities']])
test_data['Utilities'] = OE.transform(test_data[['Utilities']])

OE = OrdinalEncoder(categories=[['C (all)', 'RH', 'RM', 'RL', 'FV']])
train_data['MSZoning'] = OE.fit_transform(train_data[['MSZoning']])
test_data['MSZoning'] = OE.transform(test_data[['MSZoning']])

OE = OrdinalEncoder(categories=[['Slab', 'BrkTil', 'Stone', 'CBlock', 'Wood', 'PConc']])
train_data['Foundation'] = OE.fit_transform(train_data[['Foundation']])
test_data['Foundation'] = OE.transform(test_data[['Foundation']])

OE = OrdinalEncoder(categories=[['MeadowV', 'IDOTRR', 'BrDale', 'Edwards', 'BrkSide', 'OldTown', 'NAmes', 'Sawyer', 'Mitchel', 'NPkVill', 'SWISU', 'Blueste', 'SawyerW', 'NWAmes', 'Gilbert', 'Blmngtn', 'ClearCr', 'Crawfor', 'CollgCr', 'Veenker', 'Timber', 'Somerst', 'NoRidge', 'StoneBr', 'NridgHt']])
train_data['Neighborhood'] = OE.fit_transform(train_data[['Neighborhood']])
test_data['Neighborhood'] = OE.transform(test_data[['Neighborhood']])

OE = OrdinalEncoder(categories=[['None', 'BrkCmn', 'BrkFace', 'Stone']])
train_data['MasVnrType'] = OE.fit_transform(train_data[['MasVnrType']])
test_data['MasVnrType'] = OE.transform(test_data[['MasVnrType']])
OE = OrdinalEncoder(categories=[['AdjLand', 'Abnorml','Alloca', 'Family', 'Normal', 'Partial']])
train_data['SaleCondition'] = OE.fit_transform(train_data[['SaleCondition']])
test_data['SaleCondition'] = OE.transform(test_data[['SaleCondition']])

OE = OrdinalEncoder(categories=[['Gambrel', 'Gable','Hip', 'Mansard', 'Flat', 'Shed']])
train_data['RoofStyle'] = OE.fit_transform(train_data[['RoofStyle']])
test_data['RoofStyle'] = OE.transform(test_data[['RoofStyle']])

OE = OrdinalEncoder(categories=[['ClyTile', 'CompShg', 'Roll','Metal', 'Tar&Grv','Membran', 'WdShake', 'WdShngl']])
train_data['RoofMatl'] = OE.fit_transform(train_data[['RoofMatl']])
test_data['RoofMatl'] = OE.transform(test_data[['RoofMatl']])
Level_col = ['Street' ,'BldgType', 'SaleType', 'CentralAir']

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
def encode_catagorical_columns(train, test):
for col in Level_col:
train[col] = encoder.fit_transform(train[col])
test[col]  = encoder.transform(test[col])
encode_catagorical_columns(train_data, test_data)
train_data['BsmtRating'] = train_data['BsmtCond'] * train_data['BsmtQual']
train_data['ExterRating'] = train_data['ExterCond'] * train_data['ExterQual']
train_data['BsmtFinTypeRating'] = train_data['BsmtFinType1'] * train_data['BsmtFinType2']

train_data['BsmtBath'] = train_data['BsmtFullBath'] + train_data['BsmtHalfBath']
train_data['Bath'] = train_data['FullBath'] + train_data['HalfBath']
train_data['PorchArea'] = train_data['OpenPorchSF'] + train_data['EnclosedPorch'] + train_data['3SsnPorch'] + train_data['ScreenPorch']

test_data['BsmtRating'] = test_data['BsmtCond'] * test_data['BsmtQual']
test_data['ExterRating'] = test_data['ExterCond'] * test_data['ExterQual']
test_data['BsmtFinTypeRating'] = test_data['BsmtFinType1'] * test_data['BsmtFinType2']

test_data['BsmtBath'] = test_data['BsmtFullBath'] + test_data['BsmtHalfBath']
test_data['Bath'] = test_data['FullBath'] + test_data['HalfBath']
test_data['PorchArea'] = test_data['OpenPorchSF'] + test_data['EnclosedPorch'] + test_data['3SsnPorch'] + test_data['ScreenPorch']
drop_col = ['OverallQual',
'ExterCond', 'ExterQual',
'BsmtCond', 'BsmtQual',
'BsmtFinType1', 'BsmtFinType2',
'HeatingQC',
'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',
'BsmtFullBath', 'BsmtHalfBath',
'FullBath', 'HalfBath',
]

train_data.drop(drop_col, axis=1, inplace=True)
test_data.drop(drop_col, axis=1, inplace=True)

print(train_data.shape)
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import PowerTransformer
ft = FunctionTransformer(func=np.log1p)
pt = PowerTransformer()

train_data['LotArea'] = ft.fit_transform(train_data['LotArea'])
test_data['LotArea'] = ft.transform(test_data['LotArea'])
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


y = train_data['SalePrice']
X = train_data.drop(['SalePrice'], axis=1)

candidate_max_leaf_nodes = [250]
#model = LinearRegression()

for node in candidate_max_leaf_nodes:
model = RandomForestRegressor(max_leaf_nodes=node,)
model.fit(X, y)
score = cross_val_score(model, X, y, cv=10)
print(score.mean())
price = model.predict(test_data)
submission = pd.DataFrame({
"Id": test_ids,
"SalePrice": price
})

submission.to_csv("submission.csv", index=False)
submission.sample(10)