{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-17T00:39:59.604213Z",
     "iopub.status.busy": "2024-04-17T00:39:59.603809Z",
     "iopub.status.idle": "2024-04-17T00:40:05.937209Z",
     "shell.execute_reply": "2024-04-17T00:40:05.935869Z",
     "shell.execute_reply.started": "2024-04-17T00:39:59.604182Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_log_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Input data files are available in the read-only \"/share/dutta/eyao/dataset/kaggle/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/share/dutta/eyao/dataset/kaggle'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory () that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spaceship Titanic\n",
    "## Introduction \n",
    "- In this notebook we will be building a model for the kaggle competition Spaceship Titanic. This notebook will go through the steps necessary to enrich your dataset and make a good classification model. In this iteration we will be focusing on using Random Forest Classifier however you can experiment with others (e.g. Gradient boosting, Logistic Regression). This notebook will include some commented out code which will assist you in implementing the other aforementioned models.\n",
    "\n",
    "## Table of Contents\n",
    "- [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis)\n",
    "- [Data Preprocessing](#Data-Preprocessing)\n",
    "    - [Feature Engineering](#Feature-Engineering)\n",
    "    - [Encoding](#Encoding)\n",
    "- [Model Building](#Model-Building)\n",
    "    - [Submission](#Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "- In this section we will be taking a quick look into our dataset to get a feel for it. While looking we will devise stratedgies that we will carry out during the Data preprocessing phase.\n",
    "\n",
    "[üîù](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:05.94047Z",
     "iopub.status.busy": "2024-04-17T00:40:05.939692Z",
     "iopub.status.idle": "2024-04-17T00:40:06.036422Z",
     "shell.execute_reply": "2024-04-17T00:40:06.035359Z",
     "shell.execute_reply.started": "2024-04-17T00:40:05.940422Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/train.csv')\n",
    "df_test = pd.read_csv('/share/dutta/eyao/dataset/kaggle/spaceship-titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:06.044066Z",
     "iopub.status.busy": "2024-04-17T00:40:06.043539Z",
     "iopub.status.idle": "2024-04-17T00:40:06.080909Z",
     "shell.execute_reply": "2024-04-17T00:40:06.079607Z",
     "shell.execute_reply.started": "2024-04-17T00:40:06.044021Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preview above we will notice we have categorical and numerical features so we should think about encoding stratedgies. We have a few columns that contains purchases which we could use for feature engineering. Within the categorical columns we can see a lot of them are parts connected by some characters like ' ', '/', '-'. It would be easy to seperate these columns to gain better results while encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:06.08261Z",
     "iopub.status.busy": "2024-04-17T00:40:06.082246Z",
     "iopub.status.idle": "2024-04-17T00:40:06.120619Z",
     "shell.execute_reply": "2024-04-17T00:40:06.119301Z",
     "shell.execute_reply.started": "2024-04-17T00:40:06.082579Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the shape of our dataset is (8693, 14) which is a nice size for models like Logistic Regression, Gradient Boosting and Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:06.122983Z",
     "iopub.status.busy": "2024-04-17T00:40:06.122558Z",
     "iopub.status.idle": "2024-04-17T00:40:06.399408Z",
     "shell.execute_reply": "2024-04-17T00:40:06.398447Z",
     "shell.execute_reply.started": "2024-04-17T00:40:06.12295Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = ['Not Transported', 'Transported']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df['Transported'].value_counts().plot(kind='pie', ax=ax, labels=labels, autopct='%1.1f%%', colors=['coral', 'teal'])\n",
    "\n",
    "ax.set_xlabel('Passenger Fate')\n",
    "ax.set_ylabel('') \n",
    "ax.legend(title='Fates')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:06.402756Z",
     "iopub.status.busy": "2024-04-17T00:40:06.401683Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "colors = [\"coral\", \"white\", \"teal\"] \n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_coral_teal\", colors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))  \n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=cmap,\n",
    "            xticklabels=corr.columns, yticklabels=corr.columns,\n",
    "            cbar_kws={'label': 'Correlation coefficient'})\n",
    "\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.yticks(rotation=45)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have quite a few missing values in almost every column so we should be thinking of imputation stratedgies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'HomePlanet: {len(df.HomePlanet.unique())} \\nCabin: {len(df.Cabin.unique())}\\nDestination:{len(df.Destination.unique())}\\nName:{len(df.Name.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too investigate the categorical columns we can immediately see `Name` and `Cabin` would greatly benefit from being split. It isn't necessary to reduce the dimensionality of the other columns and will encode them as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- In this stage we will be doing some feature engineering as discussed previously. We will be leaving some steps out here as we will cover them in our machine learning pipeline later. This allows us to tune some of the preprocessors like the imputer.\n",
    "\n",
    "[üîù](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "The feature engineering steps will help you build the best model possible. The steps we will be doing are the following:\n",
    "- Splitting Cabin into 3 columns composing of the parts\n",
    "- Split Name into First Name and Last Name to correlate families\n",
    "- Calculate a total column that comprises total spent on services\n",
    "\n",
    "[üîù](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Cabin1', 'Cabin2', 'Cabin3']] = df['Cabin'].str.split('/', expand=True)\n",
    "df[['FirstName', 'LastName']] = df['Name'].str.split(' ', expand=True)\n",
    "df['total'] = df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
    "df['AgeBin'] = pd.qcut(df['Age'].fillna(df['Age'].mode()[0]), q=5, labels=False)\n",
    "\n",
    "\n",
    "df.drop(['Cabin', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-04-17T00:40:07.092222Z",
     "shell.execute_reply": "2024-04-17T00:40:07.091099Z",
     "shell.execute_reply.started": "2024-04-17T00:40:07.076481Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Cabin1: {len(df.Cabin1.unique())} \\nCabin2: {len(df.Cabin2.unique())}\\nCabin3: {len(df.Cabin3.unique())}\\nFirstName: {len(df.FirstName.unique())}\\nLastName: {len(df.LastName.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we reduced the dimensionality pretty significantly for all of the columns. We can see cabin split very well with the high cardinality being a numerical features. We see First Name and Last Name are still high cardinality so we will need to take that into account when choosing an encoding stratedgy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:07.094528Z",
     "iopub.status.busy": "2024-04-17T00:40:07.093984Z",
     "iopub.status.idle": "2024-04-17T00:40:07.10714Z",
     "shell.execute_reply": "2024-04-17T00:40:07.10571Z",
     "shell.execute_reply.started": "2024-04-17T00:40:07.09449Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['Transported'].astype(int)\n",
    "df.drop(['Transported'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are dropping the transported column before encoding so we can transform our submission dataframe to it later with dimension issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "- Here we are going to encode our features with the BinaryEncoder. For low cardinality features it will be pretty similar to OneHotEncoding however it won't blow up our column size with high cardinality features. There are a lot of encoders out there so I would reccomend experimenting with some for potential improvements.\n",
    "\n",
    "[üîù](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:07.108581Z",
     "iopub.status.busy": "2024-04-17T00:40:07.108243Z",
     "iopub.status.idle": "2024-04-17T00:40:07.401293Z",
     "shell.execute_reply": "2024-04-17T00:40:07.400155Z",
     "shell.execute_reply.started": "2024-04-17T00:40:07.108553Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = BinaryEncoder(cols=['FirstName', 'LastName', 'Cabin1', 'Cabin3', 'Destination', 'VIP', 'HomePlanet'], return_df=True)\n",
    "df = encoder.fit_transform(df)\n",
    "\n",
    "df.drop(['VIP_1'], axis=1,inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "- In this section we will be building a machine learning pipeline where we perform imputing and scaling before passing into our model. This will be paired with a GridSearch to find the most optimal parameters. I have commented out the parametwers I've tried with other models for future use. \n",
    "\n",
    "[üîù](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:07.403624Z",
     "iopub.status.busy": "2024-04-17T00:40:07.403231Z",
     "iopub.status.idle": "2024-04-17T00:40:07.418371Z",
     "shell.execute_reply": "2024-04-17T00:40:07.417069Z",
     "shell.execute_reply.started": "2024-04-17T00:40:07.403593Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaled_cols = ['Age', 'RoomService', 'FoodCourt']\n",
    "\n",
    "X = df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:07.429322Z",
     "iopub.status.busy": "2024-04-17T00:40:07.428886Z",
     "iopub.status.idle": "2024-04-17T00:40:22.625551Z",
     "shell.execute_reply": "2024-04-17T00:40:22.624343Z",
     "shell.execute_reply.started": "2024-04-17T00:40:07.429276Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params = {'n_estimators': 248, 'learning_rate': 0.08276477030425759, 'max_depth': 4, 'reg_lambda': 9.144307734410582, 'subsample': 0.9761017636523421}\n",
    "rf_params = {'n_estimators': 829, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('impute', KNNImputer(weights='distance', n_neighbors=3)),\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('xgb', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('xgb', xgb_model),\n",
    "            ('rf', rf_model)\n",
    "        ]\n",
    "    )),\n",
    "])\n",
    "\n",
    "best_model = pipeline.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# roc_auc = roc_auc_score(Y_test, Y_proba)\n",
    "# print(f\"ROC-AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "- Here we are applying all of the preprocessing steps that we did with the training data. It's important that you perform these steps in the same order. It's also important that you using transform with the encoder and not fit. We also need to add a 0 column `LastName_11` since the encoding dimensionality of the test set is lower and needs to be fit to the pipeline.\n",
    "\n",
    "[üîù](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:22.627822Z",
     "iopub.status.busy": "2024-04-17T00:40:22.627116Z",
     "iopub.status.idle": "2024-04-17T00:40:24.370348Z",
     "shell.execute_reply": "2024-04-17T00:40:24.369355Z",
     "shell.execute_reply.started": "2024-04-17T00:40:22.627759Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test[['Cabin1', 'Cabin2', 'Cabin3']] = df_test['Cabin'].str.split('/', expand=True)\n",
    "df_test[['FirstName', 'LastName']] = df_test['Name'].str.split(' ', expand=True)\n",
    "df_test['total'] = df_test[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
    "df_test['AgeBin'] = pd.qcut(df_test['Age'].fillna(df_test['Age'].mode()[0]), q=5, labels=False)\n",
    "df_test.drop(['Cabin', 'Name'], axis=1, inplace=True)\n",
    "df_test = encoder.transform(df_test)\n",
    "df_test.drop(['VIP_1'], axis=1,inplace=True)\n",
    "df_test['LastName_11'] = 0\n",
    "\n",
    "\n",
    "preds = best_model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T00:40:24.372556Z",
     "iopub.status.busy": "2024-04-17T00:40:24.371849Z",
     "iopub.status.idle": "2024-04-17T00:40:24.390687Z",
     "shell.execute_reply": "2024-04-17T00:40:24.389243Z",
     "shell.execute_reply.started": "2024-04-17T00:40:24.372512Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = preds.astype(bool)\n",
    "df_test['Transported'] = preds\n",
    "submission_df = df_test[['PassengerId', 'Transported']]\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3220602,
     "sourceId": 34377,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
